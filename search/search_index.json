{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Tools for Reproducible Research","text":""},{"location":"#course-overview","title":"Course overview","text":"<p> GitHub repository      14 - 16 Juin, 2023</p> <p>One of the key principles of proper scientific procedure is the act of repeating an experiment or analysis and being able to reach similar conclusions. Published research based on computational analysis (e.g. bioinformatics or computational biology) have often suffered from incomplete method descriptions (e.g. list of used software versions); unavailable raw data; and incomplete, undocumented and/or unavailable code. This essentially prevents any possibility of reproducing the results of such studies. The term \u201creproducible research\u201d has been used to describe the idea that a scientific publication should be distributed along with all the raw data and metadata used in the study, all the code and/or computational notebooks needed to produce results from the raw data, and the computational environment or a complete description thereof.</p> <p>Reproducible research not only leads to proper scientific conduct, but also enables other researchers to build upon previous work. Most importantly, the person who organizes their work with reproducibility in mind will quickly realize the immediate personal benefits: an organized and structured way of working. The person that most often has to reproduce your own analysis is your future self!</p>"},{"location":"#course-content-and-learning-outcomes","title":"Course content and learning outcomes","text":"<p>The following topics and tools are covered in the course:</p> <ul> <li>Data management</li> <li>Project organisation</li> <li>Git</li> <li>Conda</li> <li>Snakemake</li> <li>Nextflow</li> <li>R Markdown</li> <li>Jupyter</li> <li>Docker</li> <li>Singularity</li> </ul> <p>At the end of the course, students should be able to:</p> <ul> <li>Use good practices for data analysis and management</li> <li>Clearly organise their bioinformatic projects</li> <li>Use the version control system Git to track and collaborate on code</li> <li>Use the package and environment manager Conda</li> <li>Use and develop workflows with Snakemake and Nextflow</li> <li>Use R Markdown and Jupyter Notebooks to document and generate automated   reports for their analyses</li> <li>Use Docker and Singularity to distribute containerized computational   environments</li> </ul>"},{"location":"#application","title":"Application","text":"<p>This is an SouthGreen course. The course is open for PhD students, postdocs, group leaders and core facility staff related to the SouthGreen Platform (IRD, CIRAD, INRAE and the Alliance Bioversity international-CIAT).</p> <p>The only entry requirements for this course is a basic knowledge of Unix systems (i.e. being able to work on the command line) as well as at least a basic knowledge of either R or Python.</p> <p>Due to limited space the course can accommodate maximum of 20 participants. If we receive more applications, participants will be selected based on several criteria. Selection criteria include correct entry requirements, motivation to attend the course. We also take in consideration a balance between the different institute and department.</p> <p>Please note that SouthGreen training events do not provide any formal university credits. The training content is estimated to correspond to a certain number of credits, however the estimated credits are just guidelines. If formal credits are crucial, the student needs to confer with the home department before submitting a course application in order to establish whether the course is valid for formal credits or not.</p> <p>By accepting to participate in the course, you agree to follow the Code of Conduct.</p>"},{"location":"#schedule","title":"Schedule","text":"<p>You can find the course schedule at this page.</p>"},{"location":"#location","title":"Location","text":"<p>This course round is given on site. It will take place in the Badiane room at Agropolis.</p>"},{"location":"#course-material","title":"Course material","text":"<p>The pre-course setup page lists all the information you need before the course starts. The most important part is the installation and setup of all the tools used in the course, so make sure you've gone through it all for the course start.</p>"},{"location":"#teachers","title":"Teachers","text":"<ul> <li>Jacques Dainat (course responsible) </li> <li>Thomas Denecker (teacher) </li> <li>Aurore Comte (teacher) </li> <li>Gautier Sarah (teacher) </li> <li>Julie Orjuela (teacher) </li> <li>Nicolas Fernandez (teacher) </li> <li>S\u00e9bastien Ravel (Helper) </li> </ul>"},{"location":"#contact","title":"Contact","text":"<p>To contact us, please send a mail using the contact form available here.</p>"},{"location":"#acknowledgement","title":"Acknowledgement","text":"<p>This work is based on the NBIS / ELIXIR course Tools for Reproducible Research that can be found here. We extend our gratitude to the creators and providers of the training material used in this course.</p>"},{"location":"lectures/","title":"Lectures","text":""},{"location":"lectures/#requirements","title":"Requirements","text":"<p>Install the required packages using:</p> <pre><code>conda env create -f environment.yml\n</code></pre>"},{"location":"lectures/#rendering-in-html","title":"Rendering in html","text":"<p>Lectures in Rmarkdown format can be rendered using the following from the command line:</p> <pre><code>Rscript -e 'rmarkdown::render(\"&lt;Rmd-file&gt;\", \"xaringan::moon_reader\")'\n</code></pre> <p>Lectures in Jupyter notebook format can be rendered using:</p> <pre><code>jupyter nbconvert &lt;.ipynb-file&gt; --to slides --debug --allow-chromium-download\n</code></pre>"},{"location":"lectures/#rendering-in-pdf","title":"Rendering in pdf","text":"<p>Lectures in Rmarkdown format can be rendered using the following from the command line:</p> <pre><code>Rscript -e 'library(webshot); webshot(\"&lt;html-file&gt;\", \"&lt;pdf-file-output&gt;\")'\n</code></pre> <p>Lectures in Jupyter notebook format can be rendered using:</p> <pre><code>jupyter nbconvert &lt;.ipynb-file&gt; --to webpdf --debug --allow-chromium-download </code></pre>"},{"location":"lectures/#render-everything","title":"Render everything","text":"<p>To render all lectures you can use <code>snakemake</code>:</p> <pre><code>snakemake -j 1\n</code></pre>"},{"location":"pages/final_word/","title":"Final word","text":""},{"location":"pages/ice-breaker/","title":"Ice breaker session","text":"<p>We will devide in groups and discuss the topics below. In each group you will have to choose a person who will take notes, and make a quick report to everyone at the end of this session.</p> <p>Topics for discussion in breakout rooms: </p> <ul> <li>Do you organize your work in distinct projects?</li> <li>How do you organize your files in this context?</li> <li>Are you happy with the way you work today?</li> <li>Does your group have a data management plan in place?</li> <li>Do you know \"your\" repositories and how to submit data to them?</li> </ul>"},{"location":"pages/slido/","title":"Ask a question","text":"<p>We have set up a slido workplace where you can ask questions and vote for others' questions at any time. We will try to take the time to answer these questions along the training.</p> <p>The slido for the training is also accessible here : https://app.sli.do/event/qD5C3sKAG2Q2bN2JMXsZZA/live/questions</p>"},{"location":"pages/wms/","title":"Workflow kesako?","text":""},{"location":"pages/cheat_sheet/bash/bash/","title":"level 1 - Basic commands","text":""},{"location":"pages/cheat_sheet/bash/bash/#level-2-advanced-commands","title":"level 2 - Advanced commands","text":""},{"location":"pages/cheat_sheet/bash/bash/#level-3-programming","title":"level 3 - Programming","text":""},{"location":"pages/cheat_sheet/conda/conda/","title":"Conda","text":""},{"location":"pages/cheat_sheet/conda/conda/#conda-cheat-sheet","title":"Conda cheat sheet","text":""},{"location":"pages/cheat_sheet/conda/conda/#interesting-ressources","title":"Interesting ressources","text":"<p>If you are interested in learning more about Conda in general, here are some reading tips for you:</p> <ul> <li>Official conda introduction</li> <li>Conda course - NBIS</li> <li>An introduction to Conda - Happy Belly Bioinformatics</li> </ul>"},{"location":"pages/cheat_sheet/git/git/","title":"The GIT version-control system","text":"<p>Most of the documention find here is inspired or copied from the excellent ressources provided by the CodeRefinery project. CodeRefinery is a project within the Nordic e-Infrastructure Collaboration (NeIC).</p>"},{"location":"pages/cheat_sheet/git/git/#basics","title":"Basics","text":"Command Comment git init initialize new repository git add add files or stage file(s) git commit commit staged file(s) git status see what is going on git log see history git log --oneline to get a better log of your commits history git diff show unstaged/uncommitted modifications git show show the change for a specific commit git mv move tracked files git rm remove tracked files"},{"location":"pages/cheat_sheet/git/git/#undoing-things","title":"Undoing things","text":"Command Comment git revert f960dd3 This creates a new commit that does the opposite of the reverted commit. The old commit remains in the history git commit --amend Amend to the last commit. This can also be used to modify the last commit message. Note that this will change the commit hash. This command modifies the history. This means that we never use this command on commits that we have shared with others. git checkout &lt;filename&gt; Undo unstaged/uncommitted changes."},{"location":"pages/cheat_sheet/git/git/#the-staging-area","title":"The staging area","text":"<p>The staging area helps us to create well-defined commits.</p> <p></p> Command Comment git diff show unstaged/uncommitted modifications git diff --staged see staged changes git reset unstages staged changes git checkout &lt;path&gt; check out the latest staged version ( or committed version if file has not been staged ) git add -u stage all modification"},{"location":"pages/cheat_sheet/git/git/#aliases","title":"Aliases","text":"Command Comment git config --global alias.graph \"log --all --graph --decorate --oneline\" the command \"log --all --graph --decorate --oneline\" can now be called using git graph"},{"location":"pages/cheat_sheet/git/git/#branching-and-merging","title":"Branching and merging","text":"Command Comment git branch see where we are git branch &lt;name&gt; create branch &lt;name&gt; git checkout &lt;name&gt; switch to branch &lt;name&gt; git merge &lt;name&gt; merge branch &lt;name&gt; (to current branch) git branch -d &lt;name&gt; delete branch &lt;name&gt; git branch -D &lt;name&gt; delete unmerged branch git checkout -b &lt;name&gt; create branch &lt;name&gt; and switch to it git reset --hard &lt;branch/hash&gt; rewind current branch to &lt;branch/hash&gt; and throw away all later code changes git reset --soft &lt;branch/hash&gt; rewind current branch to &lt;branch/hash&gt; but keep all later code changes and stage them git rebase &lt;branch/hash&gt; cut current branch off and transplant it on top of &lt;branch/hash&gt; git reflog show me a log of past hashes I have visited git checkout -b &lt;branch/hash&gt; create a branch pointing to &lt;bran <ul> <li>Typical workflows</li> </ul> <p>With this there are two typical workflows:</p> <pre><code>    $ git checkout -b new-feature  # create branch, switch to it\n    $ git commit                   # work, work, work, ...\n                       # test\n                       # feature is ready\n    $ git checkout master          # switch to master\n    $ git merge new-feature        # merge work to master\n    $ git branch -d new-feature    # remove branch\n</code></pre> <p>Sometimes you have a wild idea which does not work. Or you want some throw-away branch for debugging:</p> <pre><code>    $ git checkout -b wild-idea\n                       # work, work, work, ...\n                       # realize it was a bad idea\n    $ git checkout master\n    $ git branch -D wild-idea      # it is gone, off to a new idea\n                       # -D because we never merged back\n</code></pre> <p>No problem: we worked on a branch, branch is deleted, master is clean.</p> <p>Rebase vs. merge Git rebase and commit squashing</p>"},{"location":"pages/cheat_sheet/git/git/#tagging","title":"Tagging","text":"Command Comment \u00a0git tag -a v1.0 -m \"message\" To record particular states or milestones of a project at a given point in time, like for instance versions. \u00a0git push origin &lt;tagname&gt; To push a tag to remote servers. \u00a0git push --delete origin &lt;tagname&gt; To delete the remote tag. \u00a0git tag --delete &lt;tagname&gt; To delete the local tag."},{"location":"pages/cheat_sheet/git/git/#conflict-resolution","title":"Conflict resolution","text":"<p>See the link for manual resolution.</p> Command Comment git merge -s recursive -Xours branch-name merge and in doubt take the changes from current branch git merge -s recursive -Xtheirs branch-name merge and in doubt take the changes from less-avocados branch git merge --abort Abort a conflicting merge"},{"location":"pages/cheat_sheet/git/git/#interrupted-work","title":"Interrupted work","text":"<p>You are in a middle of a development and a colleague wants to fix/commit something right now. How to do ?   * Stashing   The stash is the first and easiest place to temporarily \u201cstash\u201d things. The stashes form a stack, so you can stash several batches of modifications.</p> Command Comment git stash will put working directory and staging area changes away. Your code will be same as last commit. git stash pop will return to the state you were before. Can give it a list. git stash list will list the current stashes. \u00a0git stash save NAME is like the first, but will give it a name. Useful if it might last a while. git stash save [-p] [filename] will stash certain files files and/or by patches. git stash drop will drop the most recent stash (or whichever stash you give). git stash apply reapply the work from the most recent stash git stash apply stash@{2} reapply the work from a specific stash <ul> <li>Create branches<pre><code>git checkout -b temporary  # create a branch and switch to it\ngit add &lt;paths&gt;            # stage changes\ngit commit                 # commit them\ngit checkout master        # back to master\n                 # do your work...\ngit checkout temporary     # continue where you left off\n</code></pre> </li> </ul>"},{"location":"pages/cheat_sheet/git/git/#working-with-remotes","title":"Working with remotes","text":"Command Comment git clone https://host.com/user/project.git project cloning a repository git push origin master push the change to the upstream repository git pull origin master Pull updates from the upstream repository (It is equivalent to <code>git fetch origin</code> + <code>git merge origin/master</code>) git pull --rebase origin master alternative to avoid merge commits git push origin -u branchName \u00a0Push your change as a new branch branchName git push origin somefeature push to the remote branch somefeature git pull origin somefeature pull to the remote branch somefeature git push origin --delete somefeature delete the remote branch somefeature"},{"location":"pages/cheat_sheet/git/git/#git-archaeology","title":"Git archaeology","text":"Command Comment git diff HEAD^^ HEAD &lt;filename&gt; to see the difference for a file &lt;filename&gt; between now and two commits back git diff 61a86561a1edb438963f5f22ec9e0773a0c4aacf HEAD &lt;filename&gt; to see the difference for a file &lt;filename&gt; between now and a specific commit git show e83c51633 Inspecting commit e83c51633 git grep -i term Greps entire repository below current directory git blame &lt;filename&gt; Show what revision and author last modified each line of a file. git log --oneline --grep \"term\" grepping commit messages git log -S 'term' &lt;filename&gt; Finding removed code (term) from file &lt;filename&gt; git checkout -b &lt;name&gt; &lt;hash&gt; Branch from arbitrary (earlier) hash. Recommended mechanism to inspect old code. remove the git shortlog --summary --numbered To show all users and the number of commits <ul> <li>bisect<pre><code>$ git bisect start\n$ git bisect good 89578ed  # this is a commit that worked\n$ git bisect bad HEAD      # last commit is broken\n  # now compile and/or run\n  # after that decide whether\n$ git bisect good\n  # or\n$ git bisect bad\n  # now compile and/or run\n  # after that decide whether\n$ git bisect good\n  # or\n$ git bisect bad\n  # iterate until commit is found\n</code></pre> </li> </ul> <p>This can even be automatized with <code>git bisect run &lt;script&gt;</code>. For this you write a script that returns zero/non-zero (success/failure).</p>"},{"location":"pages/cheat_sheet/git/git/#cherry-picking","title":"Cherry Picking","text":"<p>When? Let\u2019s say you are working in an project where you are making changes in a branch called <code>new-features</code>. You have already made a few commits but want to move just one of them into the master branch.</p> <p>How?   * Checkout the branch where you want to cherry pick the specific commits (<code>git checkout branchName</code>)   * Cherry pick from <code>new-features branch</code> (<code>git cherry-pick __commit-id__</code>)</p> Command Comment git cherry-pick commit-hash cherry pick the commit with hash commit-hash git cherry-pick commit-hash1 commit-hash2 cherry pick several commits git cherry-pick --continue When the cherry picking stoped by a conflicts and you have resolved it, then launch this command git cherry-pick --abort Cancel the operation and return to the pre-sequence state git cherry-pick -m 1 &lt;hash&gt; herry pick a merge instead of a commit <p>/!\\ Cherry picking is commonly discouraged in developer community. The main reason is because it creates a duplicate commit with the same changes and you lose the ability to track the history of the original commit. If you can merge, then you should use that instead of cherry picking. Use it with caution!</p>"},{"location":"pages/cheat_sheet/git/git/#extra","title":"Extra","text":"<p>Git branch design lesson</p>"},{"location":"pages/cheat_sheet/git/git_official/","title":"Git cheat sheet official","text":""},{"location":"pages/cheat_sheet/notebook/notebook/","title":"Notebook cheat-sheets","text":""},{"location":"pages/cheat_sheet/notebook/notebook/#markdown","title":"Markdown","text":""},{"location":"pages/cheat_sheet/notebook/notebook/#rmarkdown","title":"Rmarkdown","text":""},{"location":"pages/cheat_sheet/notebook/notebook/#jupyter","title":"Jupyter","text":""},{"location":"pages/cheat_sheet/notebook/notebook/#quarto","title":"Quarto","text":""},{"location":"pages/conda/conda-1-introduction/","title":"Introduction","text":"<p>Conda is a package and environment manager. As a package manager it enables you to install a wide range of software and tools using one simple command: <code>conda install</code>. As an environment manager it allows you to create and manage multiple different environments, each with their own set of packages. What are the benefits of using an environment manager? Some examples include the ability to easily run different versions of the same package, have different cross-package dependencies that are otherwise incompatible with each other and, last but not least, easy installation of all the software needed for an analysis.</p> <p>Environments are of particular relevance when making bioinformatics projects reproducible. Full reproducibility requires the ability to recreate the system that was originally used to generate the results. This can, to a large extent, be accomplished by using Conda to make a project environment with specific versions of the packages that are needed in the project. You can read more about Conda here.</p> <p>A Conda package is a compressed tarball (system-level libraries, Python or other modules, executable programs or other components). Conda keeps track of the dependencies between packages and platforms - this means that when installing a given package, all necessary dependencies will also be installed.</p> <p>Conda packages are typically hosted and downloaded from remote so-called channels. Some widely used channels for general-purpose and bioinformatics packages are conda-forge and Bioconda, respectively. Both of these are community-driven projects, so if you're missing some package you can contribute to the channel by adding the package to it. When installing a Conda package you specify the package name, version (optional) and channel to download from.</p> <p>A Conda environment is essentially a directory that is added to your PATH and that contains a specific collection of packages that you have installed. Packages are symlinked between environments to avoid unnecessary duplication.</p>"},{"location":"pages/conda/conda-2-the-basics/","title":"The basics","text":"<p>Let's assume that you are just about to start a new exciting research project called Project A.</p>"},{"location":"pages/conda/conda-2-the-basics/#creating-conda-environments","title":"Creating Conda environments","text":"<ul> <li>Let's make our first Conda environment:</li> </ul> <pre><code>conda create -n project_a -c bioconda fastqc\n</code></pre> <p>This will create an environment called <code>project_a</code>, containing FastQC from the Bioconda channel. Conda will list the packages that will be installed and ask for your confirmation.</p> <ul> <li>Once it is done, you can activate the environment:</li> </ul> <pre><code>conda activate project_a\n</code></pre> <p>By default, Conda will add information to your prompt telling you which environment that is active.</p> <ul> <li>To see all your environments you can run:</li> </ul> <pre><code>conda info --envs # you can also use \"conda env list\" command\n</code></pre> <p>The active environment will be marked with an asterisk.</p> <ul> <li>To see the installed packages and their versions in the active environment,   run:</li> </ul> <pre><code>conda list\n</code></pre> <ul> <li>To save the installed packages to a file, run:</li> </ul> <pre><code>conda env export --from-history &gt; environment.yml\n</code></pre> <p>where <code>--from-history</code> only reports the packages requested to be installed and not additional dependancies. A caveat is that if no version was originally specified, then it is not included in the export file either. Check the <code>environment.yml</code>file content.</p> <ul> <li>Now, deactivate the environment by running <code>conda deactivate</code>.</li> <li>List all environments again. Which environment is now marked as active?</li> <li>Try to run FastQC:</li> </ul> <pre><code>fastqc --version\n</code></pre> <ul> <li>Did it work? Activate your <code>project_a</code> environment and run the <code>fastqc   --version</code> command again. Does it work now?</li> </ul> <p>Hopefully the FastQC software was not found in your base environment (unless you had installed it previously), but worked once your environment was activated.</p>"},{"location":"pages/conda/conda-2-the-basics/#adding-more-packages","title":"Adding more packages","text":"<ul> <li>Now, let's add another package (SRA-Tools) to our environment using <code>conda   install</code>. Make sure that <code>project_a</code> is the active environment first.</li> </ul> <pre><code>conda install -c bioconda sra-tools\n</code></pre> <ul> <li>If we don't specify the package version, the latest available version will be   installed. What version of SRA-Tools got installed?</li> <li>Run the following to see what versions are available:</li> </ul> <pre><code>conda search -c bioconda sra-tools\n</code></pre> <ul> <li>Now try to install a different version of SRA-Tools, e.g.:</li> </ul> <pre><code>conda install -c bioconda sra-tools=2.7.0\n</code></pre> <p>Read the information that Conda displays in the terminal. It probably asks if you want to downgrade the initial SRA-Tools installation to the one specified here (<code>2.7.0</code> in the example). You can only have one version of a given package in a given environment.</p> <p>Let's assume that you will have sequencing data in your Project A, and want to use the latest Bowtie2 software to align your reads.</p> <ul> <li>Find out what versions of Bowtie2 are available in the Bioconda channel using   <code>conda search -c bioconda</code>.</li> <li>Now install the latest available version of Bowtie2 in your <code>project_a</code>   environment.</li> </ul> <p>Let's further assume that you have an old project (called Project Old) where you know you used Bowtie2 <code>2.2.5</code>. You just got back reviewer comments and they want you to include some alignment statistics. Unfortunately, you haven't saved that information so you will have to rerun the alignment. Now, it is essential that you use the same version of Bowtie that your results are based on, otherwise the alignment statistics will be misleading. Using Conda environments this becomes simple. You can just have a separate environment for your old project where you have an old version of Bowtie2 without interfering with your new Project A where you want the latest version.</p> <ul> <li>Make a new environment for your old project:</li> </ul> <pre><code>conda create -n project_old -c bioconda bowtie2=2.2.5\n</code></pre> <ul> <li>List your environments (do you remember the command?).</li> <li>Activate <code>project_old</code> and check the Bowtie2 version (<code>bowtie2 --version</code>).</li> <li>Activate <code>project_a</code> again and check the Bowtie2 version.</li> </ul>"},{"location":"pages/conda/conda-2-the-basics/#removing-packages","title":"Removing packages","text":"<p>Now let's try to remove an installed package from the active environment:</p> <pre><code>conda remove sra-tools\n</code></pre> <ul> <li>Run <code>conda deactivate</code> to exit your active environment.</li> <li>Now, let's remove an environment:</li> </ul> <pre><code>conda env remove -n project_old\n</code></pre> <p>After making a few different environments and installing a bunch of packages, Conda can take up some disk space. You can remove unnecessary files with the command:</p> <pre><code>conda clean -a\n</code></pre> <p>This will remove package tar-balls that are left from package installations, unused packages (i.e. those not present in any environments), and cached data.</p> <p>Quick recap</p> <p>In this section we've learned:</p> <ul> <li>How to use <code>conda install</code> for installing packages on the fly.</li> <li>How to create, activate and change between environments.</li> <li>How to remove packages or environments and clean up.</li> </ul>"},{"location":"pages/conda/conda-3-projects/","title":"Environments in projects","text":"<p>We have up until now specified which Conda packages to install directly on the command line using the <code>conda create</code> and <code>conda install</code> commands. For working in projects this is not the recommended way. Instead, for increased control and reproducibility, it is better to use an environment file (in YAML format) that specifies the packages, versions and channels needed to create the environment for a project.</p> <p>Throughout these tutorials we will use a case study where we analyze an RNA-seq experiment with the multiresistant bacteria MRSA (see intro). You will now start to make a Conda yml file for this MRSA project. The file will contain a list of the software and versions needed to execute the analysis code.</p> <p>In this Conda tutorial, all code for the analysis is available in the script <code>run_qc.sh</code>. This code will download the raw FASTQ-files and subsequently run quality control on these using the FastQC software.</p> <p>We will start by making a Conda yml-file that contains the required packages to perform these two steps. Later in the course, you will update the Conda yml-file with more packages, as the analysis workflow is expanded.</p> <ul> <li>First be sure to be located in the tutorial dedicated folder:</li> </ul> <pre><code>cd ~/training-reproducible-research-area/conda\n</code></pre> <ul> <li>Let's get going! Make a YAML file called <code>environment.yml</code> looking like   this, and save it in the current directory:</li> </ul> <pre><code>channels:\n- conda-forge\n- bioconda\ndependencies:\n- fastqc=0.11.9\n- sra-tools=2.11.0\n</code></pre> <ul> <li>Now, make a new Conda environment from the YAML file (note that here the   command is <code>conda env create</code> as opposed to <code>conda create</code> that we used   above):</li> </ul> <pre><code>conda env create -n project_mrsa -f environment.yml\n</code></pre> <p>Tip</p> <p>You can also specify exactly which channel a package should come from inside the environment file, using the <code>channel::package=version</code> syntax.</p> <p>Tip</p> <p>Instead of the <code>-n</code> flag you can use the <code>-p</code> flag to set the full path to where the Conda environment should be installed. In that way you can contain the Conda environment inside the project directory, which does make sense from a reproducibility perspective, and makes it easier to keep track of what environment belongs to what project. If you don't specify <code>-p</code> the environment will be installed in the default <code>miniconda3/envs/</code> directory.</p> <ul> <li> <p>Activate the environment!</p> </li> <li> <p>Now we can run the code for the MRSA project found in <code>run_qc.sh</code>,   either by running <code>bash run_qc.sh</code> or by opening the <code>run_qc.sh</code> file   and executing each line in the terminal one by one. Do this!</p> </li> </ul> <p>Warning</p> <p>On some computers we've found that the package <code>sra-tools</code> which is used in the course is not working properly. The error seems to be related to some certificate used to communicate with remote read archives and may affect all environments with <code>sra-tools</code> on the dependency list.</p> <p>If you run into errors with the program <code>fastq-dump</code> from the <code>sra-tools</code> package try the following:</p> <ol> <li>Remove <code>sra-tools</code> from the relevant environment: <code>conda remove sra-tools</code></li> <li>Download the most recent binaries for your operating system from here (example shown for Mac OSX): <code>curl --output sratoolkit.tar.gz https://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/current/sratoolkit.current-mac64.tar.gz</code></li> <li>Create a temporary directory for the installation: <code>mkdir tmp_out</code></li> <li>Extract the binary files: <code>tar -C tmp_out -zxvf sratoolkit.tar.gz */bin/*</code></li> <li>Copy binary files into the conda environment: <code>cp -r tmp_out/*/bin/* $CONDA_PREFIX/bin/</code></li> <li>Remove the downloaded files: <code>rm -r sratoolkit.tar.gz tmp_out/</code></li> </ol> <p>This should download the project FASTQ files and run FastQC on them (as mentioned above).</p> <ul> <li>Check your directory contents (<code>ls -Rlh</code>, or in your file browser). It should   now have the following structure:</li> </ul> <pre><code>   conda/\n    |\n    |- code/\n    |   |- run_qc.sh\n    |\n    |- data/\n    |   |- raw_internal/\n    |       |- SRR935090.fastq.gz\n    |       |- SRR935091.fastq.gz\n    |       |- SRR935092.fastq.gz\n    |\n    |- intermediate/\n    |   |- fastqc/\n    |       |- SRR935090_fastqc.zip\n    |       |- SRR935091_fastqc.zip\n    |       |- SRR935092_fastqc.zip\n    |\n    |- results/\n    |   |- fastqc/\n    |       |- SRR935090_fastqc.html\n    |       |- SRR935091_fastqc.html\n    |       |- SRR935092_fastqc.html\n    |\n    |- environment.yml\n</code></pre> <p>Note that all that was needed to carry out the analysis and generate these files and results was <code>environment.yml</code> (that we used to create a Conda environment with the required packages) and the analysis code in <code>run_qc.sh</code>.</p>"},{"location":"pages/conda/conda-3-projects/#keeping-track-of-dependencies","title":"Keeping track of dependencies","text":"<p>Projects can often be quite large and require lots of dependencies; it can feel daunting to try to capture all of that in a single Conda environment, especially when you consider potential incompatibilities that may arise. It can therefore be a good idea to start new projects with an environment file with each package you know that you will need to use, but without specifying exact versions (except for those packages where you know you need a specific version). Conda will then try to get the latest compatible versions of all the specified software, making the start-up and installation part of new projects easier. You can then add the versions that were installed to your environment file afterwards, ensuring future reproducibility.</p> <p>There is one command that can make this easier: <code>conda env export</code>. This allows you to export a list of the packages you've already installed, including their specific versions, meaning you can easily add them after the fact to your environment file. If you use the <code>--no-builds</code> flag, you'll get a list of the packages minus their OS-specific build specifications, which is more useful for making the environment portable across systems. This way, you can start with an environment file with just the packages you need (without version), allow Conda to solve the dependency tree and install the most up-to-date version possible, and then add the resulting version back in to the environment file using the <code>export</code> command!</p>"},{"location":"pages/conda/conda-3-projects/#optimising-for-speed","title":"Optimising for speed","text":"<p>One of the greatest strengths of Conda is, unfortunately, also its greatest weakness in its current implementation: the availability of a frankly enormous number of packages and versions. This means that the search space for the dependency hierarchy of any given Conda environment can become equally enormous, leading to a (at times) ridiculous execution time for the dependency solver. It is not uncommon to find yourself waiting for minutes for Conda to solve a dependency hierarchy, sometimes even into the double digits. How can this be circumvented?</p> <p>Firstly, it is useful to specify as many of the <code>major.minor.patch</code> version numbers as possible when defining your environment: this drastically reduces the search space that Conda needs to go through. This is not always possible, though. For example, we mentioned in the end of the Environments in projects section that you might want to start out new projects without version specifications for most packages, which means that the search space is going to be large. Here is where another software comes into play: Mamba.</p> <p>The Mamba package manager is built on-top of Conda with some changes and additions that greatly speed up the execution time. First of all, core parts of Mamba are written in C++ instead of Python, like the original Conda. Secondly, it uses a different dependency solver algorithm which is much faster than the one Conda uses. Lastly, it allows for parallel downloading of repository data and package files with multi-threading. All in all, these changes mean that Mamba is (currently) simply a better version of Conda. Hopefully these changes will be incorporated into the Conda core at some point in the future!</p> <p>So, how do you get Mamba? Funnily enough, the easiest way to install it is (of course) using Conda! Just run <code>conda install -n base -c conda-forge mamba</code>, which will install Mamba in your <code>base</code> Conda environment. Mamba works almost exactly the same as Conda, meaning that all you need to do is to stop using <code>conda command</code> and instead use <code>mamba command</code> - simple! Be aware though that in order to use <code>mamba activate</code> and <code>mamba deactivate</code> you first need to run <code>mamba init</code>. So transitioning into using Mamba is actually quite easy - enjoy your shorter execution times!</p> <p>Quick recap</p> <p>In this section we've learned:</p> <ul> <li>How to define our Conda environment using a YAML-file.</li> <li>How to use <code>conda env create</code> to make a new environment from a YAML-file.</li> <li>How to use <code>conda env export</code> to get a list of installed packages.</li> <li>How to work with Conda in a project-like setting.</li> <li>How to optimise Conda for speed.</li> </ul>"},{"location":"pages/conda/conda-4-extra-material/","title":"Extra material","text":"<p>The following extra material contains some more advanced things you can do with Conda and the command line in general, which is not part of the main course materials. All the essential skills of Conda are covered by the previous section: the material here should be considered tips and tricks from people who use Conda as part of their daily work. You thus don't need to use these things unless you want to, and you can even skip this part of the lesson if you like!</p>"},{"location":"pages/conda/conda-4-extra-material/#configuring-conda","title":"Configuring Conda","text":"<p>The behaviour of your Conda installation can be changed using an optional configuration file <code>.condarc</code>. On a fresh Conda install no such file is included but it's created in your home directory as <code>~/.condarc</code> the first time you run <code>conda config</code>.</p> <p>You can edit the <code>.condarc</code> file either using a text editor or by way of the <code>conda config</code> command. To list all config parameters and their settings run:</p> <pre><code>conda config --show\n</code></pre> <p>Similar to Conda environment files, the configuration file is in YAML syntax. This means that the config file is structured in the form of <code>key:value</code> pairs where the <code>key</code> is the name of the config parameter (e.g. <code>auto_update_conda</code>) and the <code>value</code> is the parameter setting (e.g. <code>True</code>).</p> <p>Adding the name of a config parameter to <code>conda config --show</code> will show only that parameter, e.g. <code>conda config --show channels</code>.</p> <p>You can change parameters with the <code>--set</code>, <code>--add</code>, <code>--append</code> and <code>--remove</code> flags to <code>conda config</code>.</p> <p>If you for example want to enable the 'Always yes' behaviour which makes Conda automatically choose the <code>yes</code> option, such as when installing, you can run:</p> <pre><code>conda config --set always_yes True\n</code></pre> <p>To see details about a config parameter you can run <code>conda config --describe parameter</code>. Try running it on the <code>channels</code> parameter:</p> <pre><code>conda config --describe channels\n</code></pre> <p>In the beginning of this tutorial we added Conda channels to the <code>.condarc</code> file using <code>conda config --add channels</code>. To remove one of the channels from the configuration file you can run:</p> <pre><code>conda config --remove channels conda-forge\n</code></pre> <p>Check your <code>.condarc</code> file to see the change. To add the conda-forge channel back to the top of the <code>channels</code> simply run:</p> <pre><code>conda config --add channels conda-forge\n</code></pre> <p>To completely remove a parameter and all its values run:</p> <pre><code>conda config --remove-key parameter\n</code></pre> <p>For a list of Conda configuration parameters see the Conda configuration page.</p>"},{"location":"pages/conda/conda-4-extra-material/#managing-python-versions","title":"Managing Python versions","text":"<p>With Conda it's possible to keep several different versions of Python on your computer at the same time, and switching between these versions is very easy. However, a single Conda environment can only contain one version of Python.</p>"},{"location":"pages/conda/conda-4-extra-material/#your-current-python-installation","title":"Your current Python installation","text":"<p>The Conda base environment has its own version of Python installed. When you open a terminal (after having installed Conda on your system) this base environment is activated by default (as evidenced by <code>(base)</code> prepended to your prompt). You can check what Python version is installed in this environment by running <code>python --version</code>. To see the exact path to the Python executable type <code>which python</code>.</p> <p>In addition to this your computer may already have Python installed in a separate (system-wide) location outside of the Conda installation. To see if that is the case type <code>conda deactivate</code> until your prompt is not prepended with a Conda environment name. Then type <code>which python</code>. If a path was printed to the terminal (e.g. <code>/usr/bin/python</code>) that means some Python version is already installed in that location. Check what version it is by typing <code>python --version</code>.</p> <p>Now activate the base Conda environment again by typing <code>conda activate</code> (or the equivalent <code>conda activate base</code>) then check the Python installation path and version using <code>which</code> and <code>python --version</code> as above. See the difference? When you activate a Conda environment your <code>$PATH</code> variable is updated so that when you call <code>python</code> (or any other program) the system first searches the directory of the currently active environment.</p>"},{"location":"pages/conda/conda-4-extra-material/#different-python-versions","title":"Different Python versions","text":"<p>When you create a new Conda environment you can choose to install a specific version of Python in that environment as well. As an example, create an environment containing Python version <code>3.5</code> by running:</p> <pre><code>conda create -n py35 python=3.5\n</code></pre> <p>Here we name the environment <code>py35</code> but you can choose whatever name you want.</p> <p>To activate the environment run:</p> <pre><code>conda activate py35\n</code></pre> <p>You now have a completely separate environment with its own Python version.</p> <p>Let's say you instead want an environment with Python version <code>2.7</code> installed. You may for instance want to run scripts or packages that were written for Python 2.x and are thus incompatible with Python 3.x. Simply create the new Conda environment with:</p> <pre><code>conda create -n py27 python=2.7\n</code></pre> <p>Activate this environment with:</p> <pre><code>conda activate py27\n</code></pre> <p>Now, switching between Python versions is as easy as typing <code>conda activate py35</code> / <code>conda activate py27</code>.</p> <p>Note</p> <p>If you create an environment where none of the packages require Python, and you don't explicitly install the <code>python</code> package then that new environment will use the Python version installed in your base Conda environment.</p>"},{"location":"pages/conda/conda-4-extra-material/#decorating-your-prompt","title":"Decorating your prompt","text":"<p>By default, Conda adds the name of the currently activated environment to the end of your command line prompt. This is a good thing, as it makes it easier to keep track of what environment and packages you have access to. The way this is done in the default implementation becomes an issue when using absolute paths for environments (specifying <code>conda env create -p path/to/environment</code>, though, as the entire path will be added to the prompt. This can take up a lot of unnecessary space, but can be solved in a number of ways.</p> <p>The most straightforward way to solve this is to change the Conda configuration file, specifically the settings of the <code>env_prompt</code> configuration value which determines how Conda modifies your command line prompt. For more information about this setting you can run <code>conda config --describe env_prompt</code> and to see your current setting you can run <code>conda config --show env_prompt</code>.</p> <p>By default <code>env_prompt</code> is set to <code>({default_env})</code> which modifies your prompt with the active environment name if it was installed using the <code>-n</code> flag or if the environment folder has a parent folder named <code>envs/</code>. Otherwise the full environment path (i.e. the 'prefix') is displayed.</p> <p>If you instead set env_prompt to <code>({name})</code> Conda will modify your prompt with the folder name of the active environment. You can change the setting by running <code>conda config --set env_prompt '({name}) '</code></p> <p>If you wish to keep the <code>({default_env})</code> behaviour, or just don't want to change your Conda config, an alternative is to keep Conda environment folders within a parent folder called <code>envs/</code>. This will make Conda only add the folder name of the Conda environment to your prompt when you activate it.</p> <p>As an example, say you have a project called project_a with the project path <code>~/myprojects/project_a</code>. You could then install the environment for project_a into a folder <code>~/myprojects/project_a/envs/project_a_environment</code>. Activating the environment by pointing Conda to it (e.g. <code>conda activate ~/myprojects/project_a/envs/project_a_environment</code>) will only cause your prompt to be modified with project_a_environment.</p>"},{"location":"pages/conda/conda-4-extra-material/#bash-aliases-for-conda","title":"Bash aliases for conda","text":"<p>Some programmers like to have aliases (i.e. shortcuts) for common commands. Two aliases that might be usefol for you are <code>alias coac='conda activate'</code> and <code>alias code='conda deactivate'</code>. Don't forget to add them to your <code>~/.bash_profile</code> if you want to use them!</p>"},{"location":"pages/conda/conda-4-extra-material/#rolling-back-to-an-earlier-version-of-the-environment","title":"Rolling back to an earlier version of the environment","text":"<p>Conda keeps a history of the changes to an environment. You can see revisions to an environment by using:</p> <pre><code>conda list --revisions\n</code></pre> <p>which shows each revision (numbered) and what's installed.</p> <p>You can revert back to particular revision using:</p> <pre><code>conda install --revision 5\n</code></pre>"},{"location":"pages/conda/conda-installation/","title":"Setup Conda tutorial","text":""},{"location":"pages/conda/conda-installation/#setup-course-material","title":"Setup course material","text":"Follow the instructions below only if you start the course at this stage! Otherwise skip this step! <pre><code>This tutorial depends on files from the course GitHub repo. Please follow these instructions \non how to set it up if you haven't done so already.  \nLet's create a directory and clone the course GitHub repo.\n```bash\nmkdir -p  ~/training-reproducible-research-area\ncd ~/training-reproducible-research-area\ngit clone https://github.com/SouthGreenPlatform/training_reproducible_research\n```\n</code></pre>"},{"location":"pages/conda/conda-installation/#setup-environment","title":"Setup environment","text":"<p>First let's create a dedicated folder for this tutorial:</p> <pre><code>mkdir -p  ~/training-reproducible-research-area/conda\ncd ~/training-reproducible-research-area/conda\ncp -r ~/training-reproducible-research-area/training_reproducible_research/tutorials/conda/* . </code></pre>"},{"location":"pages/conda/conda-installation/#installing-conda","title":"Installing Conda","text":"<p>Conda is installed by downloading and executing an installer from the Conda website, but which version you need depends on your operating system. Choose the  appropriate installer from the list you can found here:  https://docs.conda.io/en/latest/miniconda.html.  Then copy the link and download the installer on your computer, see example below: </p> <pre><code># Install Miniconda3 for 64-bit Linux\ncurl -L https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O\n</code></pre> <p>Warning</p> <p>If you already have installed Conda but want to update, you should be able to simply run <code>conda update conda</code> and subsequently <code>conda init</code>, and skip the installation instructions below.</p> <p>Now you can execute the installer:</p> <pre><code>bash Miniconda3-latest-Linux-x86_64.sh\n</code></pre> <p>The installer will ask you questions during the installation:</p> <ul> <li>Do you accept the license terms? <ul> <li>Yes</li> </ul> </li> <li>Do you accept the installation path or do you want to choose a different one?<ul> <li>Yes</li> </ul> </li> <li>Do you want to run <code>conda init</code> to setup Conda on your system?<ul> <li>Yes</li> </ul> </li> </ul> <p>Restart your shell so that the settings in <code>~/.bashrc</code> or <code>~/.bash_profile</code> can take effect. You can verify that the installation worked by running:</p> <pre><code>conda --version\n</code></pre> <p>You can now get rid of the installer, you don't need it anymore</p> <pre><code>rm Miniconda3-latest-Linux-x86_64.sh\n</code></pre> <p>By default conda will be activated to every new terminal you will open (in the <code>base</code> environment). If you  whish to deactivate this behaviour run:</p> <pre><code>conda config --set auto_activate_base false\n</code></pre> <p>Different Condas</p> <p>There are three Conda-related things you may have encountered: the first is Conda, the package and environment manager we've been talking about so far. Second is Miniconda, which is the installer for Conda. The third is Anaconda, which is a distribution of not only Conda, but also over 150 scientific Python packages. It's generally better to stick with only Conda, i.e. installing with Miniconda, rather than installing 3 GB worth of packages you may not even use.</p> <p>Lastly, we will setup the default channels (from where packages will be searched for and downloaded if no channel is specified).</p> <pre><code>conda config --add channels defaults\nconda config --add channels bioconda\nconda config --add channels conda-forge\nconda config --set channel_priority strict\n</code></pre>"},{"location":"pages/containers/","title":"Index","text":"<p>Originally we have a docker course docker.md and a singularity course singularity.md (see others) that go into details of each technology. Now both are covered, focusing primarily on docker and introducing to singularity allowing to optimize the time needed to cover both.</p>"},{"location":"pages/containers/containers-1-introduction/","title":"Introduction","text":"<p>Container-based technologies are designed to make it easier to create, deploy, and run applications by isolating them in self-contained software units (hence their name). The idea is to package software and/or code together with everything it needs (other packages it depends, various environment settings, etc.) into one unit, i.e. a container. This way we can ensure that the software or code functions in exactly the same way regardless of where it's executed. Containers are in many ways similar to virtual machines but more lightweight. Rather than starting up a whole new operating system, containers can use the same kernel (usually Linux) as the system that they're running on. This makes them much faster and smaller compared to virtual machines. While this might sound a bit technical, actually using containers is quite smooth and very powerful.</p> <p>Containers have also proven to be a very good solution for packaging, running and distributing scientific data analyses. Some applications of containers relevant for reproducible research are:</p> <ul> <li>When publishing, package your analyses in a container image and let it   accompany the article. This way interested readers can reproduce your analysis   at the push of a button.</li> <li>Packaging your analysis in a container enables you to develop on e.g. your   laptop and seamlessly move to cluster or cloud to run the actual analysis.</li> <li>Say that you are collaborating on a project and you are using Mac while your   collaborator is using Windows. You can then set up a container image specific   for your project to ensure that you are working in an identical environment.</li> </ul> <p>One of the largest and most widely used container-based technologies is Docker. Just as with Git, Docker was designed for software development but is rapidly becoming widely used in scientific research. Another container-based technology is Singularity, which was developed to work well in computer cluster environments. We will cover both Docker and Singularity in this course, but the focus will be on the former (since that is the most widely used and runs on all three operating systems).</p> <p>Warning</p> <p>Docker images tend to take up quite a lot of space. In order to do all the exercises in this tutorial you need to have ~10 GB available.</p>"},{"location":"pages/containers/containers-2-the-basics/","title":"The basics","text":"<p>We're almost ready to start, just one last note on nomenclature. You might have noticed that we sometimes refer to \"Docker images\" and sometimes to \"Docker containers\". A container is simply an instance of an image. To use a programming metaphor, if an image is a class, then a container is an instance of that class \u2014 a runtime object. You can have an image containing, say, a certain Linux distribution, and then start multiple containers running that same OS.</p> <p>Warning</p> <p>If you don't have root privileges you have to prepend all Docker commands with <code>sudo</code>.</p>"},{"location":"pages/containers/containers-2-the-basics/#downloading-containers","title":"Downloading containers","text":"<p>Docker containers typically run Linux, so let's start by downloading an image containing Ubuntu (a popular Linux distribution that is based on only open-source tools) through the command line.</p> <pre><code>docker pull ubuntu:latest\n</code></pre> <p>You will notice that it downloads different layers with weird hashes as names. This represents a very fundamental property of Docker images that we'll get back to in just a little while. The process should end with something along the lines of:</p> <pre><code>Status: Downloaded newer image for ubuntu:latest\ndocker.io/library/ubuntu:latest\n</code></pre> <p>Let's take a look at our new and growing collection of Docker images:</p> <pre><code>docker image ls\n</code></pre> <p>The Ubuntu image show show up in this list, with something looking like this:</p> <pre><code>REPOSITORY       TAG              IMAGE ID            CREATED             SIZE\nubuntu           latest           d70eaf7277ea        3 weeks ago         72.9MB\n</code></pre>"},{"location":"pages/containers/containers-2-the-basics/#running-containers","title":"Running containers","text":"<p>We can now start a container running our image. We can refer to the image either by \"REPOSITORY:TAG\" (\"latest\" is the default so we can omit it) or \"IMAGE ID\". The syntax for <code>docker run</code> is <code>docker run [OPTIONS] IMAGE [COMMAND] [ARG...]</code>. Let's run the command <code>uname -a</code> to get some info about the operating system. First run on your own system (use <code>systeminfo</code> if you are on Windows):</p> <pre><code>uname -a\n</code></pre> <p>This should print something like this to your command line:</p> <pre><code>Darwin liv433l.lan 15.6.0 Darwin Kernel Version 15.6.0: Mon Oct  2 22:20:08 PDT 2017; root:xnu-3248.71.4~1/RELEASE_X86_64 x86_64\n</code></pre> <p>Seems like I'm running the Darwin version of macOS. Then run it in the Ubuntu Docker container:</p> <pre><code>docker run ubuntu uname -a\n</code></pre> <p>Here I get the following result:</p> <pre><code>Linux 24d063b5d877 5.4.39-linuxkit #1 SMP Fri May 8 23:03:06 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux\n</code></pre> <p>And now I'm running on Linux! Try the same thing with <code>whoami</code>.</p>"},{"location":"pages/containers/containers-2-the-basics/#running-interactively","title":"Running interactively","text":"<p>So, seems we can execute arbitrary commands on Linux. Seems useful, but maybe a bit limited. We can also get an interactive terminal with the flags <code>-it</code>.</p> <pre><code>docker run -it ubuntu\n</code></pre> <p>This should put at a terminal prompt inside a container running Ubuntu. Your prompt should now look similar to:</p> <pre><code>root@1f339e929fa9:/#\n</code></pre> <p>Here you can do whatever; install, run, remove stuff. It will still be within the container and never affect your host system. Now exit the container with <code>exit</code>.</p>"},{"location":"pages/containers/containers-2-the-basics/#containers-inside-scripts","title":"Containers inside scripts","text":"<p>Okay, so Docker lets us work in any OS in a quite convenient way. That would probably be useful on its own, but Docker is much more powerful than that. For example, let's look at the <code>shell</code> part of the <code>index_genome</code> rule in the Snakemake workflow for the MRSA case study:</p> <pre><code>shell:\n\"\"\"\n    bowtie2-build tempfile intermediate/{wildcards.genome_id} &gt; {log}\n    \"\"\"\n</code></pre> <p>You may have seen that one can use containers through both Snakemake and Nextflow if you've gone through their tutorial's extra material, but we can also use containers directly inside scripts in a very simple way. Let's imagine we want to run the above command using containers instead. How would that look? It's quite simple, really: first we find a container image that has <code>bowtie2</code> installed, and then prepend the command with <code>docker run &lt;image&gt;</code>.</p> <p>First of all we need to download the genome to index though, so run: <pre><code>mkdir -p $PWD/analysis\ncd analysis\ncurl -o NCTC8325.fa.gz ftp://ftp.ensemblgenomes.org/pub/bacteria/release-37/fasta/bacteria_18_collection/staphylococcus_aureus_subsp_aureus_nctc_8325/dna//Staphylococcus_aureus_subsp_aureus_nctc_8325.ASM1342v1.dna_rm.toplevel.fa.gz\ngunzip -c NCTC8325.fa.gz &gt; tempfile\n</code></pre></p> <p>to download and prepare the input for bowtie2.</p> <p>Now try running the following Bash code, create an <code>analysis</code> directory:</p> <pre><code>docker run -v $(pwd)/analysis:/home quay.io/biocontainers/bowtie2:2.5.0--py310h8d7afc0_0 bowtie2-build /home/tempfile /home/NCTC832\n</code></pre> <p>Docker will automatically download the container image and subsequently run the command! Here we're using <code>-v HOST:PATH_INTO_DOCKER</code> to mount the required directory inside the container in order to make the <code>tempfile</code> input available to bowtie2. More on these so-called \"Bind mounts\" in Section bind mount of this tutorial.</p> <p>Quick recap</p> <p>In this section we've learned:</p> <ul> <li>How to use <code>docker pull</code> for downloading images from a central registry.</li> <li>How to use <code>docker image ls</code> for getting information about the images we have on our system.</li> <li>How to use <code>docker run</code> for starting a container from an image.</li> <li>How to use the <code>-it</code> flag for running in interactive mode.</li> <li>How to use Docker inside scripts.</li> </ul>"},{"location":"pages/containers/containers-3-building-images/","title":"Building a Docker image","text":"<p>In the previous section we downloaded a Docker image of Ubuntu and noticed that it was based on layers, each with a unique hash as id. An image in Docker is based on a number of read-only layers, where each layer contains the differences to the previous layers. If you've done the Git tutorial this might remind you of how a Git commit contains the difference to the previous commit. The great thing about this is that we can start from one base layer, say containing an operating system and some utility programs, and then generate many new images based on this, say 10 different project-specific images. </p> <p>Docker provides a convenient way to describe how to go from a base image to the image we want by using a \"Dockerfile\". This is a simple text file containing the instructions for how to generate each layer. Docker images are typically quite large, often several GBs, while Dockerfiles are small and serve as blueprints for the images. It is therefore good practice to have your Dockerfile in your project Git repository, since it allows other users to exactly replicate your project environment.</p> <p>We will be looking at a Dockerfile called <code>Dockerfile_slim</code> that is located in your <code>containers</code> directory (where you should hopefully be standing already). We will now go through that file and discuss the different steps and what they do. After that we'll build the image and test it out. Lastly, we'll start from that image and make a new one to reproduce the results from the Conda tutorial.</p>"},{"location":"pages/containers/containers-3-building-images/#understanding-dockerfiles","title":"Understanding Dockerfiles","text":"<p>Here are the first few lines of <code>Dockerfile_slim</code>. Each line in the Dockerfile will typically result in one layer in the resulting image. The format for Dockerfiles is <code>INSTRUCTION arguments</code>. A full specification of the format, together with best practices, can be found here.</p> <pre><code>FROM ubuntu:16.04\n\nLABEL description = \"Minimal image for the NBIS reproducible research course.\"\nMAINTAINER \"John Sundh\" john.sundh@scilifelab.se\n</code></pre> <p>Here we use the instructions <code>FROM</code>, <code>LABEL</code> and <code>MAINTAINER</code>. The important one is <code>FROM</code>, which specifies the base image our image should start from. In this case we want it to be Ubuntu 16.04, which is one of the official repositories. There are many roads to Rome when it comes to choosing the best image to start from. Say you want to run RStudio in a Conda environment through a Jupyter notebook. You could then start from one of the rocker images for R, a Miniconda image, or a Jupyter image. Or you just start from one of the low-level official images and set up everything from scratch. <code>LABEL</code> and <code>MAINTAINER</code> is just meta-data that can be used for organizing your various Docker components.</p> <p>Let's take a look at the next section of the Dockerfile.</p> <pre><code># Use bash as shell\nSHELL [\"/bin/bash\", \"-c\"]\n\n# Set workdir\nWORKDIR /course\n</code></pre> <p><code>SHELL</code> simply sets which shell to use. <code>WORKDIR</code> determines the directory the container should start in. The next few lines introduce the important <code>RUN</code> instruction, which is used for executing shell commands:</p> <pre><code># Install necessary tools\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y --no-install-recommends bzip2 \\\n                                               ca-certificates \\\n                                               curl \\\n                                               fontconfig \\\n                                               git \\\n                                               language-pack-en \\\n                                               tzdata \\\n                                               vim \\\n                                               unzip \\\n                                               wget \\\n    &amp;&amp; apt-get clean\n\n# Install Miniconda and add to PATH\nRUN curl -L https://repo.continuum.io/miniconda/Miniconda3-4.7.12.1-Linux-x86_64.sh -O &amp;&amp; \\\n    bash Miniconda3-4.7.12.1-Linux-x86_64.sh -bf -p /usr/miniconda3/ &amp;&amp; \\\n    rm Miniconda3-4.7.12.1-Linux-x86_64.sh &amp;&amp; \\\n    /usr/miniconda3/bin/conda clean -tipsy &amp;&amp; \\\n    ln -s /usr/miniconda3/etc/profile.d/conda.sh /etc/profile.d/conda.sh &amp;&amp; \\\n    echo \". /usr/miniconda3/etc/profile.d/conda.sh\" &gt;&gt; ~/.bashrc &amp;&amp; \\\n    echo \"conda activate base\" &gt;&gt; ~/.bashrc\n</code></pre> <p>As a general rule, you want each layer in an image to be a \"logical unit\". For example, if you want to install a program the <code>RUN</code> command should both retrieve the program, install it and perform any necessary clean up. This is due to how layers work and how Docker decides what needs to be rerun between builds. The first command uses Ubuntu's package manager APT to install some packages (similar to how we've previously used Conda). Say that the first command was split into two instead:</p> <pre><code># Update apt-get\nRUN apt-get update\n\n# Install packages\nRUN apt-get install -y --no-install-recommends bzip2 \\\n                                               ca-certificates \\\n                                               curl \\\n                                               fontconfig \\\n                                               git \\\n                                               language-pack-en \\\n                                               tzdata \\\n                                               vim \\\n                                               unzip \\\n                                               wget\n\n# Clear the local repository of retrieved package files\nRUN apt-get clean\n</code></pre> <p>The first command will update the apt-get package lists and the second will install the packages <code>bzip2</code>, <code>ca-certificates</code>, <code>curl</code>, <code>fontconfig</code>, <code>git</code>, <code>language-pack-en</code>, <code>tzdata</code>, <code>vim</code>, <code>unzip</code> and <code>wget</code>. Say that you build this image now, and then in a month's time you realize that you would have liked a French language pack instead of an English. You change to <code>language-pack-fr</code> and rebuild the image. Docker detects that there is no layer with the new list of packages and reruns the second <code>RUN</code> command. However, there is no way for Docker to know that it should also update the apt-get package lists. You therefore risk to end up with old versions of packages and, even worse, the versions would depend on when the previous version of the image was first built.</p> <p>The next <code>RUN</code> command retrieves and installs Miniconda3. Let's see what would happen if we had that as separate commands instead.</p> <pre><code># Download Miniconda3\nRUN curl -L https://repo.continuum.io/miniconda/Miniconda3-4.7.12.1-Linux-x86_64.sh -O\n\n# Install it\nRUN bash Miniconda3-4.7.12.1-Linux-x86_64.sh -bf -p /usr/miniconda3/\n\n# Remove the downloaded installation file\nRUN rm Miniconda3-4.7.12.1-Linux-x86_64.sh\n\n# Remove unused packages and caches\nRUN /usr/miniconda3/bin/conda clean -tipsy\n\n# Permanently enable the Conda command\nRUN ln -s /usr/miniconda3/etc/profile.d/conda.sh /etc/profile.d/conda.sh\nRUN echo \". /usr/miniconda3/etc/profile.d/conda.sh\" &gt;&gt; ~/.bashrc\n\n# Add the base environment permanently to PATH\nRUN echo \"conda activate base\" &gt;&gt; ~/.bashrc\n</code></pre> <p>Remember that each layer contains the difference compared to the previous layer? What will happen here is that the first command adds the installation file and the second will unpack the file and install the software. The third layer will say \"the installation file should no longer exist on the file system\". However, the file will still remain in the image since the image is constructed layer-by-layer bottom-up. This results in unnecessarily many layers and bloated images. Line four is cleaning up conda to free up space, and the next two lines are there to make the Conda command available in the shell. The last command adds a code snippet to the bash startup file which automatically activates the Conda base environment in the container.</p> <pre><code># Add conda to PATH and set locale\nENV PATH=\"/usr/miniconda3/bin:${PATH}\"\nENV LC_ALL en_US.UTF-8\nENV LC_LANG en_US.UTF-8\n</code></pre> <p>Here we use the new instruction <code>ENV</code>. The first command adds <code>conda</code> to the path, so we can write <code>conda install</code> instead of <code>/usr/miniconda3/bin/conda install</code>. The next two commands set a UTF-8 character encoding so that we can use weird characters (and a bunch of other things).</p> <pre><code># Configure Conda channels and install Mamba\nRUN conda config --add channels bioconda \\\n    &amp;&amp; conda config --add channels conda-forge \\\n    &amp;&amp; conda config --set channel_priority strict \\\n    &amp;&amp; conda install mamba \\\n    &amp;&amp; mamba clean --all\n</code></pre> <p>Here we just configure Conda and install Mamba, for quicker installations of any subsequent Conda packages we might want to do.</p> <pre><code># Open port for running Jupyter Notebook\nEXPOSE 8888\n\n# Start Bash shell by default\nCMD /bin/bash\n</code></pre> <p><code>EXPOSE</code> opens up the port 8888, so that we can later run a Jupyter Notebook server on that port. <code>CMD</code> is an interesting instruction. It sets what a container should run when nothing else is specified. It can be used for example for printing some information on how to use the image or, as here, start a shell for the user. If the purpose of your image is to accompany a publication then <code>CMD</code> could be to run the workflow that generates the paper figures from raw data.</p>"},{"location":"pages/containers/containers-3-building-images/#tp-building-from-dockerfiles","title":"TP : Building from Dockerfiles","text":"<p>Ok, so now we understand how a Dockerfile works. Constructing the image from the Dockerfile is really simple. Try it out now:</p> <pre><code>docker build -f Dockerfile_slim -t my_docker_image .\n</code></pre> <p>This should result in something similar to this:</p> <pre><code>=&gt; [internal] load build definition from Dockerfile_slim\n=&gt; =&gt; transferring dockerfile: 1.88kB\n=&gt; [internal] load .dockerignore\n=&gt; =&gt; transferring context: 2B\n=&gt; [internal] load metadata for docker.io/library/ubuntu:16.04\n=&gt; [auth] library/ubuntu:pull token for registry-1.docker.io\n=&gt; [1/5] FROM docker.io/library/ubuntu:16.04@sha256:bb84bbf2ff36d46acaf0bb0c6bcb33dae64cd93cba8652d74c9aaf438fada438\n=&gt; CACHED [2/5] WORKDIR /course\n=&gt; CACHED [3/5] RUN apt-get update &amp;&amp;     apt-get install -y --no-install-recommends bzip2                                                ca-certificates\n=&gt; CACHED [4/5] RUN curl -L https://repo.continuum.io/miniconda/Miniconda3-4.7.12.1-Linux-x86_64.sh -O &amp;&amp;   h Miniconda3-4.7.12.1-Linux-x86_64.sh -bf -p /usr/miniconda3\n=&gt; CACHED [5/5] RUN conda config --add channels bioconda     &amp;&amp; conda config --add channels conda-forge     &amp;&amp; conda config --set channel_priority strict     &amp;&amp; conda instal\n=&gt; exporting to image\n=&gt; =&gt; exporting layers\n=&gt; =&gt; writing image sha256:d14301f829d4554816df54ace927ec0aaad4a994e028371455f7a18a370f6af9\n=&gt; =&gt; naming to docker.io/library/my_docker_image\n</code></pre> <p>Exactly how the output looks depends on which version of Docker you are using. The <code>-f</code> flag sets which Dockerfile to use and <code>-t</code> tags the image with a name. This name is how you will refer to the image later. Lastly, the <code>.</code> is the path to where the image should be build (<code>.</code> means the current directory). This had no real impact in this case, but matters if you want to import files. Validate with <code>docker image ls</code> that you can see your new image.</p>"},{"location":"pages/containers/containers-3-building-images/#tp-creating-your-own-dockerfile","title":"TP : Creating your own Dockerfile","text":"<p>Now it's time to make our own Dockerfile to reproduce the results from the Conda tutorial. If you haven't done the tutorial, it boils down to creating a Conda environment file, setting up that environment, downloading three RNA-seq data files, and running FastQC on those files. We will later package and run the whole RNA-seq workflow in a Docker container, but for now we keep it simple to reduce the size and time required.</p> <p>The Conda tutorial uses a shell script, <code>run_qc.sh</code>, for downloading and running the analysis. A copy of this file should also be available in your current directory. If we want to use the same script we need to include it in the image. So, this is what we need to do:</p> <ol> <li> <p>Create the file <code>Dockerfile_conda</code>.</p> </li> <li> <p>Set <code>FROM</code> to the image we just built.</p> </li> <li> <p>Install the required packages with Conda. We could do this by adding    <code>environment.yml</code> from the Conda tutorial. Chanels needed for conda are already included into the <code>Dockerfile_slim</code>, we can directly install <code>fastqc=0.11.9</code> and    <code>sra-tools=2.11.0</code> with <code>mamba install</code>. The packages will be installed to    the default environment named <code>base</code> inside the container.</p> </li> <li> <p>Add <code>run_qc.sh</code> to the image by using the <code>COPY</code> instruction. The syntax is    <code>COPY source target</code>, so in our case simply <code>COPY run_qc.sh .</code> to copy to    the work directory in the image.</p> </li> <li> <p>Set the default command for the image to <code>bash run_qc.sh</code>, which will    execute the shell script.</p> </li> </ol> <p>Try to add required lines to <code>Dockerfile_conda</code>. If it seems overwhelming you can take a look at an example below:</p> Click to show the solution <pre><code>FROM my_docker_image:latest\nRUN mamba install -n base fastqc=0.11.9 \nRUN mamba install -n base sra-tools=2.11.0\nCOPY run_qc.sh .\nCMD bash run_qc.sh\n</code></pre> <p>Build the image and tag it <code>my_docker_conda</code>:</p> <pre><code>docker build -t my_docker_conda -f Dockerfile_conda .\n</code></pre> <p>Verify that the image was built using <code>docker image ls</code>.</p> <p>Quick recap</p> <p>In this section we've learned:</p> <ul> <li>How the keywords <code>FROM</code>, <code>LABEL</code>, <code>MAINTAINER</code>, <code>RUN</code>, <code>ENV</code>, <code>SHELL</code>, <code>WORKDIR</code>, and <code>CMD</code> can be used when writing a Dockerfile.</li> <li>The importance of letting each layer in the Dockerfile be a \"logical unit\".</li> <li>How to use <code>docker build</code> to construct and tag an image from a Dockerfile.</li> <li>How to create your own Dockerfile.</li> </ul>"},{"location":"pages/containers/containers-4-managing-containers/","title":"Managing containers","text":"<p>When you start a container with <code>docker run</code> it is given an unique id that you can use for interacting with the container. Let's try to run a container from the image we just created:</p> <pre><code>docker run my_docker_conda\n</code></pre> <p>If everything worked <code>run_qc.sh</code> is executed and will first download and then analyse the three samples. Once it's finished you can list all containers, including those that have exited.</p> <pre><code>docker container ls --all\n</code></pre> <p>This should show information about the container that we just ran. Similar to:</p> <pre><code>CONTAINER ID        IMAGE                    COMMAND                  CREATED             STATUS                      PORTS               NAMES\n39548f30ce45        my_docker_conda     \"/bin/bash -c 'bas...\"    3 minutes ago       Exited (0) 3 minutes ago                             el\n</code></pre> <p>If we run <code>docker run</code> without any flags, your local terminal is attached to the container. This enables you to see the output of <code>run_qc.sh</code>, but also disables you from doing anything else in the meantime. We can start a container in detached mode with the <code>-d</code> flag. Try this out and run <code>docker container ls</code> to validate that the container is running.</p> <p>By default, Docker keeps containers after they have exited. This can be convenient for debugging or if you want to look at logs, but it also consumes huge amounts of disk space. It's therefore a good idea to always run with <code>--rm</code>, which will remove the container once it has exited.</p> <p>If we want to enter a running container, there are two related commands we can use, <code>docker attach</code> and <code>docker exec</code>. <code>docker attach</code> will attach local standard input, output, and error streams to a running container. This can be useful if your terminal closed down for some reason or if you started a terminal in detached mode and changed your mind. <code>docker exec</code> can be used to execute any command in a running container. It's typically used to peak in at what is happening by opening up a new shell. Here we start the container in detached mode and then start a new interactive shell so that we can see what happens. If you use <code>ls</code> inside the container you can see how the script generates file in the <code>data</code>, <code>intermediate</code> and <code>results</code> directories. Note that you will be thrown out when the container exits, so you have to be quick.</p> <pre><code>docker run -d --rm --name my_container my_docker_conda\ndocker exec -it my_container /bin/bash\n</code></pre>"},{"location":"pages/containers/containers-4-managing-containers/#bind-mounts","title":"Bind mounts","text":"<p>There are obviously some advantages to isolating and running your data analysis in containers, but at some point you need to be able to interact with the host system to actually deliver the results. This is done via bind mounts. When you use a bind mount, a file or directory on the host machine is mounted into a container. That way, when the container generates a file in such a directory it will appear in the mounted directory on your host system.</p> <p>Tip</p> <p>Docker also has a more advanced way of data storage called volumes. Volumes provide added flexibility and are independent of the host machine's filesystem having a specific directory structure available. They are particularly useful when you want to share data between containers.</p> <p>Say that we are interested in getting the resulting html reports from FastQC in our container. We can do this by mounting a directory called, say, <code>fastqc_results</code> in your current directory to the <code>/course/results/fastqc</code> directory in the container. Try this out by running:</p> <pre><code>docker run --rm -v $(pwd)/fastqc_results:/course/results/fastqc my_docker_conda\n</code></pre> <p>Here the <code>-v</code> flag to docker run specifies the bind mount in the form of <code>directory/on/your/computer:/directory/inside/container</code>. <code>$(pwd)</code> simply evaluates to the working directory on your computer.</p> <p>Once the container finishes validate that it worked by opening one of the html reports under <code>fastqc_results/</code>.</p> <p>We can also use bind mounts for getting files into the container rather than out. We've mainly been discussing Docker in the context of packaging an analysis pipeline to allow someone else to reproduce its outcome. Another application is as a kind of very powerful environment manager, similarly to how we've used Conda before. If you've organized your work into projects, then you can mount the whole project directory in a container and use the container as the terminal for running stuff while still using your normal OS for editing files and so on. Let's try this out by mounting our current directory and start an interactive terminal. Note that this will override the <code>CMD</code> command, so we won't start the analysis automatically when we start the container.</p> <pre><code>docker run -it --rm -v $(pwd):/course/ my_docker_conda /bin/bash\n</code></pre> <p>If you run <code>ls</code> you will see that all the files in the <code>docker</code> directory are there. Now edit <code>run_qc.sh</code> on your host system to download, say, 12000 reads instead of 15000. Then rerun the analysis with <code>bash run_qc.sh</code>. Tada! Validate that the resulting html reports look fine and then exit the container with <code>exit</code>.</p> <p>Quick recap</p> <p>In this section we've learned:</p> <ul> <li>How to use <code>docker run</code> for starting a container and how the flags <code>-d</code> and <code>--rm</code> work.</li> <li>How to use <code>docker container ls</code> for displaying information about the containers.</li> <li>How to use <code>docker attach</code> and <code>docker exec</code> to interact with running containers.</li> <li>How to use bind mounts to share data between the container and the host system.</li> </ul>"},{"location":"pages/containers/containers-5-sharing-images/","title":"Distributing your images","text":"<p>There would be little point in going through all the trouble of making your analyses reproducible if you can't distribute them to others. Luckily, sharing Docker containers is extremely easy, and can be done in several ways. One of the more common ways to share Docker images is through container registries and repositories.</p> <p>For example, a Docker registry is a service that stores Docker images, which could be hosted by a third party, publicly or privately. One of the most common registries is Docker Hub, which is a registry hosted by Docker itself. A repository, on the other hand, is a collection of container images with the same name but different tags, i.e. versions. For example, <code>ubuntu:latest</code> or <code>ubuntu:20.04</code>. Repositories are stored in registries.</p> <p>Note</p> <p>Remember that we now have some clashing nomenclature between Git repositories (which we covered in the Git tutorial) and container repositories, so be aware of which one you're talking about!</p> <p>There are many registries out there, but here are some that might be of interest to you who are taking this course:</p> <ul> <li>Docker Hub</li> <li>Quay</li> <li>Biocontainers</li> <li>Rocker</li> <li>Jupyter containers</li> </ul> <p>The most common registry is probably Docker Hub, which lets you host unlimited public images and one private image for free (after which they charge a small fee). Let's see how it's done!</p> <ol> <li> <p>Register for an account on Docker Hub.</p> </li> <li> <p>Use <code>docker login -u your_dockerhub_id</code> to login to the Docker Hub registry.</p> </li> <li> <p>When you build an image, tag it with <code>-t your_dockerhub_id/image_name</code>,    rather than just <code>image_name</code>.</p> </li> <li> <p>Once the image has been built, upload it to Docker Hub with <code>docker push    your_dockerhub_id/image_name</code>.</p> </li> <li> <p>If another user runs <code>docker run your_dockerhub_id/image_name</code> the image    will automatically be retrieved from Docker Hub. You can use <code>docker pull</code>    for downloading without running.</p> </li> </ol> <p>If you want to refer to a Docker image in for example a publication, it's very important that it's the correct version of the image. You can do this by adding a tag to the name like this <code>docker build -t your_dockerhub_id/image_name:tag_name</code>.</p> <p>Tip</p> <p>On Docker Hub it is also possible to link to your Bitbucket or GitHub account and select repositories from which you want to automatically build and distribute Docker images. The Docker Hub servers will then build an image from the Dockerfile in your Git repository and make it available for download using <code>docker pull</code>. That way, you don't have to bother manually building and pushing using <code>docker push</code>. The GitHub repository for this course is linked to Docker Hub and the Docker images are built automatically from <code>Dockerfile</code> and <code>Dockerfile_slim</code>, triggered by changes made to the GitHub repository. You can take a look at the course on Docker Hub here.</p> <p>Quick recap</p> <p>In this section we've learned:</p> <ul> <li>How container registries and repositories work</li> <li>How to use Docker Hub to share Docker images</li> </ul>"},{"location":"pages/containers/containers-6-packaging-the-case-study/","title":"Packaging the case study","text":"<p>During these tutorials we have been working on a case study about the multiresistant bacteria MRSA. Here we will build and run a Docker container that contains all the work we've done so far.</p> <ul> <li>We've set up a GitHub repository for version control   and for hosting our project.</li> <li>We've defined a Conda environment that specifies the   packages we're depending on in the project.</li> <li>We've constructed a Snakemake workflow   that performs the data analysis and keeps track of files and parameters.</li> <li>We've written a R Markdown document that   takes the results from the Snakemake workflow and summarizes them in a report.</li> </ul> <p>The <code>workshop-reproducible-research/tutorials/containers</code> directory contains the final versions of all the files we've generated in the other tutorials: <code>environment.yml</code>, <code>Snakefile</code>, <code>config.yml</code>, <code>code/header.tex</code>, and <code>code/supplementary_material.Rmd</code>. The only difference compared to the other tutorials is that we have also included the rendering of the Supplementary Material HTML file into the Snakemake workflow as the rule <code>make_supplementary</code>. Running all of these steps will take some time to execute (around 20 minutes or so), in particular if you're on a slow internet connection.</p> <p>Now take a look at <code>Dockerfile</code>. Everything should look quite familiar to you, since it's basically the same steps as in the image we constructed in the previous section, although some sections have been moved around. The main difference is that we add the project files needed for executing the workflow (mentioned in the previous paragraph), and install the conda packages listed in <code>environment.yml</code>. If you look at the <code>CMD</code> command you can see that it will run the whole Snakemake workflow by default.</p> <p>Now run <code>docker build</code> as before, tag the image with <code>my_docker_project</code>:</p> <pre><code>docker build -t my_docker_project -f Dockerfile .\n</code></pre> <p>Go get a coffee while the image builds (or you could use <code>docker pull nbisweden/workshop-reproducible-research</code> which will download the same image).</p> <p>Validate with <code>docker image ls</code>. Now all that remains is to run the whole thing with <code>docker run</code>. We just want to get the results, so mount the directory <code>/course/results/</code> to, say, <code>mrsa_results</code> in your current directory.</p> <p>Well done! You now have an image that allows anyone to exactly reproduce your analysis workflow (if you first <code>docker push</code> to Dockerhub that is).</p>"},{"location":"pages/containers/containers-7-singularity/","title":"Singularity","text":""},{"location":"pages/containers/containers-7-singularity/#singularity-as-an-alternative-container-tool","title":"Singularity as an alternative container tool","text":"<p>Singularity is a container software alternative to Docker. It was originally developed by researchers at Lawrence Berkeley National Laboratory with focus on security, scientific software, and HPC clusters. One of the ways in which Singularity is more suitable for HPC is that it very actively restricts permissions so that you do not gain access to additional resources while inside the container.</p> <p>Here we give a brief introduction to Singularity and specifically how it can be used on HPC clusters.</p> <p>If you want to read more, here are some additional resources:</p> <ul> <li>Singularity docs</li> <li>HPC Singularity user guide</li> </ul> <p>Singularity and Apptainer</p> <p>Singularity has very recently been renamed to Apptainer, but we have opted to stick with the original name in the material for now, while the change is still being adopted by the community and various documentation online.</p>"},{"location":"pages/containers/containers-7-singularity/#more-useful-commands-in-singularity","title":"More useful commands in Singularity","text":"<p>Build a container</p> <p><code>singularity build Singularity.sif Singularity.def</code></p> <p>Run a command from a container</p> <p><code>singularity run Singularity.sif echo toto</code></p> <p>Executing a script inside the container</p> <p><code>singularity exec Singularity.sif</code> </p> <p>Use a container interactively</p> <p><code>singularity run Singularity.sif</code> </p> <p>you can also define bind points if you need access outside the container.  <code>$HOME</code>, <code>/tmp</code>, <code>/proc</code>, <code>/sys</code>, <code>/dev</code> are mount by default but they can configurated.</p> <p>Many of the Singularity commands such as run, exec , and shell take the <code>--bind</code>  command-line option to specify bind paths, in addition to the SINGULARITY_BINDPATH environment variable.</p> <pre><code>## export SINGULARITY_BINDPATH=\"/opt,/data:/mnt\n## OR -B /opt,/data:/mnt \nsingularity run Singularity.sif --bind $DIR\n</code></pre>"},{"location":"pages/containers/containers-7-singularity/#converting-docker-images-to-singularity-files","title":"Converting Docker images to Singularity files","text":"<p>Singularity, unlike Docker, stores images as single files. A Singularity image file is self-contained (no shared layers) and can be moved around and shared like any other file.</p> <p>While it is possible to define and build Singularity images from scratch, in a manner similar to what you've already learned for Docker, this is not something we will cover here (but feel free to read more about this in e.g. the Singularity docs).</p> <p>Instead, we will take advantage of the fact that Singularity can convert Docker images to the Singularity Image Format (SIF). This is great if there's a Docker image that you want to use on an HPC cluster where you cannot use Docker.</p> <p>Let's try to convert the Docker image for this course directly from DockerHub using <code>singularity pull</code>:</p> <pre><code>singularity pull mrsa_proj.sif docker://nbisweden/workshop-reproducible-research\n</code></pre> <p>This should result in a file called <code>mrsa_proj.sif</code>.</p>"},{"location":"pages/containers/containers-7-singularity/#running-a-singularity-image","title":"Running a singularity image","text":"<p>In the Docker image we included the code needed for the workflow in the <code>/course</code> directory of the image. These files are of course also available in the Singularity image. However, a Singularity image is read-only (unless using the sandbox feature). This will be a problem if we try to run the workflow within the <code>/course</code> directory, since the workflow will produce files and Snakemake will create a <code>.snakemake</code> directory.  Instead, we need to provide the files externally from our host system and simply use the Singularity image as the environment to execute the workflow in (i.e. all the software and dependencies).</p> <p>In your current working directory (<code>workshop-reproducible-research/tutorials/containers/</code>) the vital MRSA project files are already available (<code>Snakefile</code>, <code>config.yml</code>, <code>code/header.tex</code> and <code>code/supplementary_material.Rmd</code>).</p> <p>Since Singularity bind mounts the current working directory we can simply execute the workflow and generate the output files using:</p> <pre><code>singularity run mrsa_proj.sif\n</code></pre> <p>This executes the default run command, which is <code>snakemake -rp -c 1 --configfile config.yml</code> (as defined in the original <code>Dockerfile</code>).</p> <p>The previous step in this tutorial included running the <code>run_qc.sh</code> script, so that part of the workflow has already been run and Snakemake will continue from that automatically without redoing anything. Once completed you should see a bunch of directories and files generated in your current working directory, including the <code>results/</code> directory containing the final HTML report.</p>"},{"location":"pages/containers/containers-7-singularity/#containers-at-different-platforms","title":"Containers at different platforms","text":"<p>A common problem with Singularity is that you can only create local builds if you are working on a Linux system, as local builds for MacOS and Windows are currently not supported. This means that you might favour using Docker instead of Singularity, but what happens when you need to use a HPC cluster such as HPC? Docker won't work there, as it requires root privileges, so Singularity is the only solution. </p> <p>Quick recap</p> <p>In this section we've learned:</p> <ul> <li>How to convert Docker images to Singularity images.</li> <li>How to use <code>singularity run</code> for starting a container from an image.</li> </ul>"},{"location":"pages/containers/containers-8-extra-material/","title":"Extra material","text":"<p>Containers can be large and complicated, but once you start using them regularly you'll find that you start understand these complexities. There are lots of different things you can do with images and containers in general, especially when it comes to optimising build time or final image size. Here is some small tips and tricks that you can be inspired from!</p> <p>If you want to read more about containers in general you can check out these resources:</p> <ul> <li>A \"Get started with Docker\" at docker.com.</li> <li>An early paper on the subject of using   Docker for reproducible research.</li> </ul>"},{"location":"pages/containers/containers-8-extra-material/#a-base-image-with-conda","title":"A base image with Conda","text":"<p>We've used Conda throughout this container tutorial, and we did it by installing Conda inside the image when we built it. Wouldn't it be nice if we didn't have to do this particular step? After all, installing Conda is just busy-work, compared to installing the actual environment that we want to use for the analyses. Luckily, there are already container images out there that have Conda (and Mamba) installed, such as the ones over at <code>condaforge/mambaforge</code>! What follows is a Dockerfile that you could use instead of the ones described above to install things using a Conda <code>environment.yml</code> file, without having to install Conda in the Docker image when building it!</p> <pre><code>FROM condaforge/mambaforge:4.10.1-0\nLABEL description = \"Image description\"\nMAINTAINER \"Firstname Lastname\" firstname.lastname@gmail.se\n\n# Use bash as shell\nSHELL [\"/bin/bash\", \"-c\"]\n\n# Set working directory\nWORKDIR /project\n\n# Copy and install the Conda environment\nCOPY environment.yml ./\nRUN conda config --set channel_priority strict \\\n    &amp;&amp; mamba env update --name base --file environment.yml \\\n    &amp;&amp; mamba clean --all --force-pkgs-dirs --yes\n\n# Start Bash shell by default\nCMD /bin/bash\n</code></pre>"},{"location":"pages/containers/containers-installation/","title":"Setup Jupyter tutorial","text":""},{"location":"pages/containers/containers-installation/#setup-course-material","title":"Setup course material","text":"Follow this instructions only if you start the course at this stage! Otherwise skip this step! <pre><code>This tutorial depends on files from the course GitHub repo. Please follow these instructions \non how to set it up if you haven't done so already.  \nLet's create a directory and clone the course GitHub repo.\n\n```bash\nmkdir -p  ~/training-reproducible-research-area\ncd ~/training-reproducible-research-area\ngit clone https://github.com/SouthGreenPlatform/training_reproducible_research\n```\n</code></pre>"},{"location":"pages/containers/containers-installation/#setup-environment","title":"Setup environment","text":"<p>First let's create a dedicated folder for this tutorial:</p> <pre><code>mkdir -p  ~/training-reproducible-research-area/containers\ncd ~/training-reproducible-research-area/containers\ncp -r ~/training-reproducible-research-area/training_reproducible_research/tutorials/containers/* . </code></pre> <p>This tutorial depends on Docker and Singularity. Take a look at the setup for instructions on how to install them if you haven't done so already.</p>"},{"location":"pages/containers/others/docker/","title":"Introduction","text":"<p>Docker is a tool designed to make it easier to create, deploy, and run applications by isolating them in \"containers\". The idea is to package your code together with everything it needs (other packages it depends on, various environment settings, data..) into one unit, i.e. a container. This way we can ensure that the code results in exactly the same output regardless of where it's executed. Containers are in many ways similar to virtual machines but more lightweight. Rather than starting up a whole new OS, Docker containers can use the same Linux kernel as the system that they're running on. This makes them much faster and smaller compared to virtual machines. While this might sound a bit technical, actually using Docker is quite easy, fun and very powerful.</p> <p>Just as with Git, Docker was designed for software development but is rapidly becoming widely used in scientific research. Say that you are building a web application. You could then run the web server in one container and the database in another, thereby reducing the risk of one system affecting the other in unpredictable ways. Docker containers have also proven to be a very good solution for packaging, running and distributing scientific data analyses. Some applications relevant for reproducible research can be:</p> <ul> <li>When publishing, package your whole analysis pipeline together with your data   in a Docker image and let it accompany the article. This way interested   readers can reproduce your analysis at the push of a button.</li> <li>Packaging your analysis in a Docker container enables you to develop on   e.g. your laptop and then seamlessly move to cluster or cloud to run the   actual analysis.</li> <li>Say that you are collaborating on a project and you are using Mac while your   collaborator is using Windows. You can then set up a Docker image specific   for your project to ensure that you are working in an identical environment.</li> </ul> <p>All of this might sound a bit abstract so far, but it'll become more clear during the exercises. If you want to read more you can check out these resources:</p> <ul> <li>A \"Get started with Docker\" at   docker.com.</li> <li>An early paper on the subject of using   Docker for reproducible research.</li> </ul> <p>This tutorial depends on files from the course GitHub repo. Take a look at the setup for instructions on how to set it up if you haven't done so already. Then open up a terminal and go to <code>workshop-reproducible-research/docker</code>.</p> <p>Attention</p> <p>Docker images tend to take up quite a lot of space. In order to do all the exercises in this tutorial you need to have ~10 GB available.</p>"},{"location":"pages/containers/others/docker/#the-basics","title":"The basics","text":"<p>We're almost ready to start, just one last note on nomenclature. You might have noticed that we sometimes refer to \"Docker images\" and sometimes to \"Docker containers\". A container is simply an instance of an image. To use a programming metaphor, if an image is a class, then a container is an instance of that class \u2014 a runtime object. You can have an image containing, say, a certain Linux distribution, and then start multiple containers running that same OS.</p> <p>Attention</p> <p>If you don't have root privileges you have to prepend all Docker commands with <code>sudo</code> (see the tip above)</p> <p>Docker containers typically run Linux, so let's start by downloading an image containing Ubuntu (a popular Linux distribution that is based on only open-source tools) through the command line.</p> <pre><code>docker pull ubuntu:latest\n</code></pre> <p>You will notice that it downloads different layers with weird hashes as names. This represents a very fundamental property of Docker images that we'll get back to in just a little while. The process should end with something along the lines of:</p> <pre><code>Status: Downloaded newer image for ubuntu:latest\ndocker.io/library/ubuntu:latest\n</code></pre> <p>Let's take a look at our new and growing collection of Docker images:</p> <pre><code>docker images\n</code></pre> <p>The Ubuntu image show show up in this list, with something looking like this:</p> <pre><code>REPOSITORY       TAG              IMAGE ID            CREATED             SIZE\nubuntu           latest           d70eaf7277ea        3 weeks ago         72.9MB\n</code></pre> <p>We can now start a container running our image. We can refer to the image either by \"REPOSITORY:TAG\" (\"latest\" is the default so we can omit it) or \"IMAGE ID\". The syntax for <code>docker run</code> is <code>docker run [OPTIONS] IMAGE [COMMAND] [ARG...]</code>. Let's run the command <code>uname -a</code> to get some info about the operating system. First run on your own system (skip this if you're using Windows via the Windows 10 PowerShell, or use <code>systeminfo</code> which is the Windows equivalent):</p> <pre><code>uname -a\n</code></pre> <p>This should print something like this to your command line:</p> <pre><code>Darwin liv433l.lan 15.6.0 Darwin Kernel Version 15.6.0: Mon Oct  2 22:20:08 PDT 2017; root:xnu-3248.71.4~1/RELEASE_X86_64 x86_64\n</code></pre> <p>Seems like I'm running the Darwin version of macOS. Then run it in the Ubuntu Docker container:</p> <pre><code>docker run ubuntu uname -a\n</code></pre> <p>Here I get the following result:</p> <pre><code>Linux 24d063b5d877 5.4.39-linuxkit #1 SMP Fri May 8 23:03:06 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux\n</code></pre> <p>And now I'm running on Linux! Try the same thing with <code>whoami</code>.</p> <p>So, seems we can execute arbitrary commands on Linux. Seems useful, but maybe a bit limited. We can also get an interactive terminal with the flags <code>-it</code>.</p> <pre><code>docker run -it ubuntu\n</code></pre> <p>This should put at a terminal prompt inside a container running Ubuntu. Your prompt should now look similar to:</p> <pre><code>root@1f339e929fa9:/#\n</code></pre> <p>Here you can do whatever; install, run, remove stuff. It will still be within the container and never affect your host system. Now exit the container with <code>exit</code>.</p> <p>Ok, so Docker lets us work in any OS in a quite convenient way. That would probably be useful on its own, but Docker is much more powerful than that.</p> <p>Quick recap</p> <p>In this section we've learned:</p> <ul> <li>How to use <code>docker pull</code> for downloading images from a central registry.</li> <li>How to use <code>docker images</code> for getting information about the images we   have on our system.</li> <li>How to use <code>docker run</code> for starting a container from an image.</li> <li>How to use the <code>-it</code> flag for running in interactive mode.</li> </ul>"},{"location":"pages/containers/others/docker/#building-a-docker-image","title":"Building a Docker image","text":"<p>In the previous section we downloaded a Docker image of Ubuntu and noticed that it was based on layers, each with a unique hash as id. An image in Docker is based on a number of read-only layers, where each layer contains the differences to the previous layers. If you've done the Git tutorial this might remind you of how a Git commit contains the difference to the previous commit. The great thing about this is that we can start from one base layer, say containing an operating system and some utility programs, and then generate many new images based on this, say 10 different project-specific images. The total space requirements would then only be $base+\\sum_{i=1}^{10}(specific_{i})$ rather than $\\sum_{i=1}^{10}(base+specific_{i})$. For example, Bioconda (see the Conda tutorial) has one base image and then one individual layer for each of the &gt;3000 packages available in Bioconda.</p> <p>Docker provides a convenient way to describe how to go from a base image to the image we want by using a \"Dockerfile\". This is a simple text file containing the instructions for how to generate each layer. Docker images are typically quite large, often several GBs, while Dockerfiles are small and serve as blueprints for the images. It is therefore good practice to have your Dockerfile in your project Git repository, since it allows other users to exactly replicate your project environment.</p> <p>If you've been doing these tutorials on Windows you've been using the Docker image <code>nbisweden/workshop-reproducible-research:slim</code>. The Dockerfile for generating that image is called <code>Dockerfile_slim</code> and is located in your <code>docker</code> directory (where you should hopefully be standing already). We will now go through that file and discuss the different steps and what they do. After that we'll build the image and test it out. Lastly, we'll start from that image and make a new one to reproduce the results from the Conda tutorial.</p> <p>Here are the first few lines of <code>Dockerfile_slim</code>. Each line in the Dockerfile will typically result in one layer in the resulting image. The format for Dockerfiles is <code>INSTRUCTION arguments</code>. A full specification of the format, together with best practices, can be found here.</p> <pre><code>FROM ubuntu:16.04\n\nLABEL description = \"Minimal image for the NBIS reproducible research course.\"\nMAINTAINER \"John Sundh\" john.sundh@scilifelab.se\n</code></pre> <p>Here we use the instructions <code>FROM</code>, <code>LABEL</code> and <code>MAINTAINER</code>. The important one is <code>FROM</code>, which specifies the base image our image should start from. In this case we want it to be Ubuntu 16.04, which is one of the official repositories. There are many roads to Rome when it comes to choosing the best image to start from. Say you want to run RStudio in a Conda environment through a Jupyter notebook. You could then start from one of the rocker images for R, a Miniconda image, or a Jupyter image. Or you just start from one of the low-level official images and set up everything from scratch. <code>LABEL</code> and <code>MAINTAINER</code> is just meta-data that can be used for organizing your various Docker components.</p> <p>Let's take a look at the next section of the Dockerfile.</p> <pre><code># Use bash as shell\nSHELL [\"/bin/bash\", \"-c\"]\n\n# Set workdir\nWORKDIR /course\n</code></pre> <p><code>SHELL</code> simply sets which shell to use. <code>WORKDIR</code> determines the directory the container should start in. The next few lines introduce the important <code>RUN</code> instruction, which is used for executing shell commands:</p> <pre><code># Install necessary tools\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y --no-install-recommends bzip2 \\\n                                               ca-certificates \\\n                                               curl \\\n                                               fontconfig \\\n                                               git \\\n                                               language-pack-en \\\n                                               tzdata \\\n                                               vim \\\n                                               unzip \\\n                                               wget \\\n    &amp;&amp; apt-get clean\n\n# Install Miniconda and add to PATH\nRUN curl -L https://repo.continuum.io/miniconda/Miniconda3-4.7.12.1-Linux-x86_64.sh -O &amp;&amp; \\\n    bash Miniconda3-4.7.12.1-Linux-x86_64.sh -bf -p /usr/miniconda3/ &amp;&amp; \\\n    rm Miniconda3-4.7.12.1-Linux-x86_64.sh &amp;&amp; \\\n    /usr/miniconda3/bin/conda clean -tipsy &amp;&amp; \\\n    ln -s /usr/miniconda3/etc/profile.d/conda.sh /etc/profile.d/conda.sh &amp;&amp; \\\n    echo \". /usr/miniconda3/etc/profile.d/conda.sh\" &gt;&gt; ~/.bashrc &amp;&amp; \\\n    echo \"conda activate base\" &gt;&gt; ~/.bashrc\n</code></pre> <p>As a general rule, you want each layer in an image to be a \"logical unit\". For example, if you want to install a program the <code>RUN</code> command should both retrieve the program, install it and perform any necessary clean up. This is due to how layers work and how Docker decides what needs to be rerun between builds. The first command uses Ubuntu's package manager APT to install some packages (similar to how we've previously used Conda). Say that the first command was split into two instead:</p> <pre><code># Update apt-get\nRUN apt-get update\n\n# Install packages\nRUN apt-get install -y --no-install-recommends bzip2 \\\n                                               ca-certificates \\\n                                               curl \\\n                                               fontconfig \\\n                                               git \\\n                                               language-pack-en \\\n                                               tzdata \\\n                                               vim \\\n                                               unzip \\\n                                               wget\n\n# Clear the local repository of retrieved package files\nRUN apt-get clean\n</code></pre> <p>The first command will update the apt-get package lists and the second will install the packages <code>bzip2</code>, <code>ca-certificates</code>, <code>curl</code>, <code>fontconfig</code>, <code>git</code>, <code>language-pack-en</code>, <code>tzdata</code>, <code>vim</code>, <code>unzip</code> and <code>wget</code>. Say that you build this image now, and then in a month's time you realize that you would have liked a Swedish language pack instead of an English. You change to <code>language-pack-sv</code> and rebuild the image. Docker detects that there is no layer with the new list of packages and reruns the second <code>RUN</code> command. However, there is no way for Docker to know that it should also update the apt-get package lists. You therefore risk to end up with old versions of packages and, even worse, the versions would depend on when the previous version of the image was first built.</p> <p>The next <code>RUN</code> command retrieves and installs Miniconda3. Let's see what would happen if we had that as separate commands instead.</p> <pre><code># Download Miniconda3\nRUN curl -L https://repo.continuum.io/miniconda/Miniconda3-4.7.12.1-Linux-x86_64.sh -O\n\n# Install it\nRUN bash Miniconda3-4.7.12.1-Linux-x86_64.sh -bf -p /usr/miniconda3/\n\n# Remove the downloaded installation file\nRUN rm Miniconda3-4.7.12.1-Linux-x86_64.sh\n\n# Remove unused packages and caches\nRUN /usr/miniconda3/bin/conda clean -tipsy\n\n# Permanently enable the Conda command\nRUN ln -s /usr/miniconda3/etc/profile.d/conda.sh /etc/profile.d/conda.sh\nRUN echo \". /usr/miniconda3/etc/profile.d/conda.sh\" &gt;&gt; ~/.bashrc\n\n# Add the base environment permanently to PATH\nRUN echo \"conda activate base\" &gt;&gt; ~/.bashrc\n</code></pre> <p>Remember that each layer contains the difference compared to the previous layer? What will happen here is that the first command adds the installation file and the second will unpack the file and install the software. The third layer will say \"the installation file should no longer exist on the file system\". However, the file will still remain in the image since the image is constructed layer-by-layer bottom-up. This results in unnecessarily many layers and bloated images. Line four is cleaning up conda to free up space, and the next two lines are there to make the Conda command available in the shell. The last command adds a code snippet to the bash startup file which automatically activates the Conda base environment in the container.</p> <pre><code># Add conda to PATH and set locale\nENV PATH=\"/usr/miniconda3/bin:${PATH}\"\nENV LC_ALL en_US.UTF-8\nENV LC_LANG en_US.UTF-8\n</code></pre> <p>Here we use the new instruction <code>ENV</code>. The first command adds <code>conda</code> to the path, so we can write <code>conda install</code> instead of <code>/usr/miniconda3/bin/conda install</code>. The next two commands set an UTF-8 character encoding so that we can use weird characters (and a bunch of other things).</p> <pre><code># Configure Conda channels and install Mamba\nRUN conda config --add channels bioconda \\\n    &amp;&amp; conda config --add channels conda-forge \\\n    &amp;&amp; conda config --set channel_priority strict \\\n    &amp;&amp; conda install mamba \\\n    &amp;&amp; mamba clean --all\n</code></pre> <p>Here we just configure Conda and install Mamba, for quicker installations of any subsequent Conda packages we might want to do.</p> <pre><code># Open port for running Jupyter Notebook\nEXPOSE 8888\n\n# Start Bash shell by default\nCMD /bin/bash\n</code></pre> <p><code>EXPOSE</code> opens up the port 8888, so that we can later run a Jupyter Notebook server on that port. <code>CMD</code> is an interesting instruction. It sets what a container should run when nothing else is specified. It can be used for example for printing some information on how to use the image or, as here, start a shell for the user. If the purpose of your image is to accompany a publication then <code>CMD</code> could be to run the workflow that generates the paper figures from raw data.</p> <p>Ok, so now we understand how a Dockerfile works. Constructing the image from the Dockerfile is really simple. Try it out now:</p> <pre><code>docker build -f Dockerfile_slim -t my_docker_image .\n</code></pre> <p>This should result in something similar to this:</p> <pre><code>=&gt; [internal] load build definition from Dockerfile_slim\n=&gt; =&gt; transferring dockerfile: 1.88kB\n=&gt; [internal] load .dockerignore\n=&gt; =&gt; transferring context: 2B\n=&gt; [internal] load metadata for docker.io/library/ubuntu:16.04\n=&gt; [auth] library/ubuntu:pull token for registry-1.docker.io\n=&gt; [1/5] FROM docker.io/library/ubuntu:16.04@sha256:bb84bbf2ff36d46acaf0bb0c6bcb33dae64cd93cba8652d74c9aaf438fada438\n=&gt; CACHED [2/5] WORKDIR /course\n=&gt; CACHED [3/5] RUN apt-get update &amp;&amp;     apt-get install -y --no-install-recommends bzip2                                                ca-certificates\n=&gt; CACHED [4/5] RUN curl -L https://repo.continuum.io/miniconda/Miniconda3-4.7.12.1-Linux-x86_64.sh -O &amp;&amp;   h Miniconda3-4.7.12.1-Linux-x86_64.sh -bf -p /usr/miniconda3\n=&gt; CACHED [5/5] RUN conda config --add channels bioconda     &amp;&amp; conda config --add channels conda-forge     &amp;&amp; conda config --set channel_priority strict     &amp;&amp; conda instal\n=&gt; exporting to image\n=&gt; =&gt; exporting layers\n=&gt; =&gt; writing image sha256:d14301f829d4554816df54ace927ec0aaad4a994e028371455f7a18a370f6af9\n=&gt; =&gt; naming to docker.io/library/my_docker_image\n</code></pre> <p>Exactly how the output looks depends on which version of Docker you are using. The <code>-f</code> flag sets which Dockerfile to use and <code>-t</code> tags the image with a name. This name is how you will refer to the image later. Lastly, the <code>.</code> is the path to where the image should be build (<code>.</code> means the current directory). This had no real impact in this case, but matters if you want to import files. Validate with <code>docker images</code> that you can see your new image.</p> <p>Now it's time to make our own Dockerfile to reproduce the results from the Conda tutorial. If you haven't done the tutorial, it boils down to creating a Conda environment file, setting up that environment, downloading three RNA-seq data files, and running FastQC on those files. We will later package and run the whole RNA-seq workflow in a Docker container, but for now we keep it simple to reduce the size and time required.</p> <p>The Conda tutorial uses a shell script, <code>run_qc.sh</code>, for downloading and running the analysis. A copy of this file should also be available in your current directory. If we want to use the same script we need to include it in the image. So, this is what we need to do:</p> <ol> <li> <p>Create the file <code>Dockerfile_conda</code>.</p> </li> <li> <p>Set <code>FROM</code> to the image we just built.</p> </li> <li> <p>Install the required packages with Conda. We could do this by adding    <code>environment.yml</code> from the Conda tutorial, but here we do it directly as    <code>RUN</code> commands. We need to add the conda-forge and bioconda channels with    <code>conda config --add channels &lt;channel_name&gt;</code> and install <code>fastqc=0.11.9</code> and    <code>sra-tools=2.10.1</code> with <code>conda install</code>. The packages will be installed to    the default environment named <code>base</code> inside the container.</p> </li> <li> <p>Add <code>run_qc.sh</code> to the image by using the <code>COPY</code> instruction. The syntax is    <code>COPY source target</code>, so in our case simply <code>COPY run_qc.sh .</code> to copy to    the work directory in the image.</p> </li> <li> <p>Set the default command for the image to <code>bash run_qc.sh</code>, which will    execute the shell script.</p> </li> </ol> <p>Try to add required lines to <code>Dockerfile_conda</code>. If it seems overwhelming you can take a look below</p> Click to show an example of <code>Dockerfile_conda</code> <pre><code>FROM my_docker_image:latest\nRUN conda config --add channels bioconda &amp;&amp; \\\n    conda config --add channels conda-forge &amp;&amp; \\\n    conda install -n base fastqc=0.11.9 sra-tools=2.10.1\nCOPY run_qc.sh .\nCMD bash run_qc.sh\n</code></pre> <p>Build the image and tag it <code>my_docker_conda</code>:</p> <pre><code>docker build -t my_docker_conda -f Dockerfile_conda .\n</code></pre> <p>Verify that the image was built using <code>docker images</code>.</p> <p>Quick recap</p> <p>In this section we've learned:</p> <ul> <li>How the keywords <code>FROM</code>, <code>LABEL</code>, <code>MAINTAINER</code>, <code>RUN</code>, <code>ENV</code>, <code>SHELL</code>,   <code>WORKDIR</code>, and <code>CMD</code> can be used when writing a Dockerfile.</li> <li>The importance of letting each layer in the Dockerfile be a \"logical   unit\".</li> <li>How to use <code>docker build</code> to construct and tag an image from a Dockerfile.</li> </ul>"},{"location":"pages/containers/others/docker/#managing-containers","title":"Managing containers","text":"<p>When you start a container with <code>docker run</code> it is given an unique id that you can use for interacting with the container. Let's try to run a container from the image we just created:</p> <pre><code>docker run my_docker_conda\n</code></pre> <p>If everything worked <code>run_qc.sh</code> is executed and will first download and then analyse the three samples. Once it's finished you can list all containers, including those that have exited.</p> <pre><code>docker container ls --all\n</code></pre> <p>This should show information about the container that we just ran. Similar to: <pre><code>CONTAINER ID        IMAGE                    COMMAND                  CREATED             STATUS                      PORTS               NAMES\n39548f30ce45        my_docker_conda     \"/bin/bash -c 'bas...\"    3 minutes ago       Exited (0) 3 minutes ago                             el\n</code></pre></p> <p>If we run <code>docker run</code> without any flags, your local terminal is attached to the container. This enables you to see the output of <code>run_qc.sh</code>, but also disables you from doing anything else in the meantime. We can start a container in detached mode with the <code>-d</code> flag. Try this out and run <code>docker container ls</code> to validate that the container is running.</p> <p>By default, Docker keeps containers after they have exited. This can be convenient for debugging or if you want to look at logs, but it also consumes huge amounts of disk space. It's therefore a good idea to always run with <code>--rm</code>, which will remove the container once it has exited.</p> <p>If we want to enter a running container, there are two related commands we can use, <code>docker attach</code> and <code>docker exec</code>. <code>docker attach</code> will attach local standard input, output, and error streams to a running container. This can be useful if your terminal closed down for some reason or if you started a terminal in detached mode and changed your mind. <code>docker exec</code> can be used to execute any command in a running container. It's typically used to peak in at what is happening by opening up a new shell. Here we start the container in detached mode and then start a new interactive shell so that we can see what happens. If you use <code>ls</code> inside the container you can see how the script generates file in the <code>data</code>, <code>intermediate</code> and <code>results</code> directories. Note that you will be thrown out when the container exits, so you have to be quick.</p> <pre><code>docker run -d --rm --name my_container my_docker_conda\ndocker exec -it my_container /bin/bash\n</code></pre> <p>Tip</p> <p>Sometimes you would like to enter a stopped container. It's not a common use case, but it's included here for those of you who are doing these tutorials on Windows using Docker. Inadvertently shutting down your container can result in loss of a lot of work if you're not able to restart it. If you were to use <code>docker start</code> it would rerun the command set by <code>CMD</code>, which may not be what you want. Instead we use <code>docker commit container_name new_image_name</code> to convert the container <code>container_name</code> to the image <code>new_image_name</code>. We can then start a new container in that image as we normally would with <code>docker run -it --rm new_image_name /bin/bash</code>. Confusing, right? In theory, this would allow you to bypass using Dockerfiles and instead generate your image by entering an empty container in interactive mode, install everything there, and then commit as a new image. However, by doing this you would lose many of the advantages that Dockerfiles provide, such as easy distribution and efficient space usage via layers.</p> <p>Quick recap</p> <p>In this section we've learned:</p> <ul> <li>How to use <code>docker run</code> for starting a container and how the flags <code>-d</code>   and <code>--rm</code> work.</li> <li>How to use <code>docker container ls</code> for displaying information about the   containers.</li> <li>How to use <code>docker attach</code> and <code>docker exec</code> to interact with running   containers.</li> </ul>"},{"location":"pages/containers/others/docker/#bind-mounts","title":"Bind mounts","text":"<p>There are obviously some advantages to isolating and running your data analysis in containers, but at some point you need to be able to interact with the host system to actually deliver the results. This is done via bind mounts. When you use a bind mount, a file or directory on the host machine is mounted into a container. That way, when the container generates a file in such a directory it will appear in the mounted directory on your host system.</p> <p>Tip</p> <p>Docker also has a more advanced way of data storage called volumes. Volumes provide added flexibility and are independent of the host machine's filesystem having a specific directory structure available. They are particularly useful when you want to share data between containers.</p> <p>Say that we are interested in getting the resulting html reports from FastQC in our container. We can do this by mounting a directory called, say, <code>fastqc_results</code> in your current directory to the <code>/course/results/fastqc</code> directory in the container. Try this out by running:</p> <pre><code>docker run --rm -v $(pwd)/fastqc_results:/course/results/fastqc my_docker_conda\n</code></pre> <p>Here the <code>-v</code> flag to docker run specifies the bind mount in the form of <code>directory/on/your/computer:/directory/inside/container</code>. <code>$(pwd)</code> simply evaluates to the working directory on your computer.</p> <p>Once the container finishes validate that it worked by opening one of the html reports under <code>fastqc_results/</code>.</p> <p>We can also use bind mounts for getting files into the container rather than out. We've mainly been discussing Docker in the context of packaging an analysis pipeline to allow someone else to reproduce its outcome. Another application is as a kind of very powerful environment manager, similarly to how we've used Conda before. If you've organized your work into projects, then you can mount the whole project directory in a container and use the container as the terminal for running stuff while still using your normal OS for editing files and so on. Let's try this out by mounting our current directory and start an interactive terminal. Note that this will override the <code>CMD</code> command, so we won't start the analysis automatically when we start the container.</p> <pre><code>docker run -it --rm -v $(pwd):/course/ my_docker_conda /bin/bash\n</code></pre> <p>If you run <code>ls</code> you will see that all the files in the <code>docker</code> directory are there. Now edit <code>run_qc.sh</code> on your host system to download, say, 12000 reads instead of 15000. Then rerun the analysis with <code>bash run_qc.sh</code>. Tada! Validate that the resulting html reports look fine and then exit the container with <code>exit</code>.</p>"},{"location":"pages/containers/others/docker/#distributing-your-images","title":"Distributing your images","text":"<p>There would be little point in going through all the trouble of making your analyses reproducible if you can't distribute them to others. Luckily, sharing Docker containers is extremely easy. The most common way is to use Dockerhub. Dockerhub lets you host unlimited public images and one private image for free, after that they charge a small fee. If you want to try it out here is how to do it:</p> <ol> <li> <p>Register for an account on Dockerhub.</p> </li> <li> <p>Use <code>docker login -u your_dockerhub_id</code> to login to the Dockerhub registry.</p> </li> <li> <p>When you build an image, tag it with <code>-t your_dockerhub_id/image_name</code>,    rather than just <code>image_name</code>.</p> </li> <li> <p>Once the image has been built, upload it to Dockerhub with <code>docker push    your_dockerhub_id/image_name</code>.</p> </li> <li> <p>If another user runs <code>docker run your_dockerhub_id/image_name</code> the image    will automatically be retrieved from Dockerhub. You can use <code>docker pull</code>    for downloading without running.</p> </li> </ol> <p>If you want to refer to a Docker image in for example a publication, it's very important that it's the correct version of the image. You can do this by adding a tag to the name like this <code>docker build -t your_dockerhub_id/image_name:tag_name</code>.</p> <p>Tip</p> <p>On Dockerhub it is also possible to link to your Bitbucket or GitHub account and select repositories from which you want to automatically build and distribute Docker images. The Dockerhub servers will then build an image from the Dockerfile in your repository and make it available for download using <code>docker pull</code>. That way, you don't have to bother manually building and pushing using <code>docker push</code>. The GitHub repository for this course is linked to Dockerhub and the Docker images are built automatically from <code>Dockerfile</code> and <code>Dockerfile_slim</code>, triggered by changes made to the GitHub repository. You can take a look at the course on Dockerhub here.</p>"},{"location":"pages/containers/others/docker/#packaging-the-case-study","title":"Packaging the case study","text":"<p>During these tutorials we have been working on a case study about the multiresistant bacteria MRSA. Here we will build and run a Docker container that contains all the work we've done so far.</p> <ul> <li>We've set up a GitHub repository for version control and for   hosting our project.</li> <li>We've defined a Conda environment that specifies the packages   we're depending on in the project.</li> <li>We've constructed a Snakemake workflow that performs the data   analysis and keeps track of files and parameters.</li> <li>We've written a R Markdown document that takes the results   from the Snakemake workflow and summarizes them in a report.</li> </ul> <p>The <code>docker</code> directory contains the final versions of all the files we've generated in the other tutorials: <code>environment.yml</code>, <code>Snakefile</code>, <code>config.yml</code>, <code>code/header.tex</code>, and <code>code/supplementary_material.Rmd</code>. The only difference compared to the other tutorials is that we have also included the rendering of the Supplementary Material HTML file into the Snakemake workflow as the rule <code>make_supplementary</code>. Running all of these steps will take some time to execute (around 20 minutes or so), in particular if you're on a slow internet connection, and result in a 3.75 GB image.</p> <p>Now take a look at <code>Dockerfile</code>. Everything should look quite familiar to you, since it's basically the same steps as in the image we constructed in the previous section, although some sections have been moved around. The main difference is that we add the project files needed for executing the workflow (mentioned in the previous paragraph), and install the conda packages listed in <code>environment.yml</code>. If you look at the <code>CMD</code> command you can see that it will run the whole Snakemake workflow by default.</p> <p>Now run <code>docker build</code> as before, tag the image with <code>my_docker_project</code>:</p> <p><pre><code>docker build -t my_docker_project -f Dockerfile .\n</code></pre> and go get a coffee while the image builds (or you could use <code>docker pull nbisweden/workshop-reproducible-research</code> which will download the same image).</p> <p>Validate with <code>docker images</code>. Now all that remains is to run the whole thing with <code>docker run</code>. We just want to get the results, so mount the directory <code>/course/results/</code> to, say, <code>mrsa_results</code> in your current directory.</p> <p>Well done! You now have an image that allows anyone to exactly reproduce your analysis workflow (if you first <code>docker push</code> to Dockerhub that is).</p> <p>Tip</p> <p>If you've done the Jupyter Notebook tutorial, you know that Jupyter Notebook runs as a web server. This makes it very well suited for running in a Docker container, since we can just expose the port Jupyter Notebook uses and redirect it to one of our own. You can then work with the notebooks in your browser just as you've done before, while it's actually running in the container. This means you could package your data, scripts and environment in a Docker image that also runs a Jupyter Notebook server. If you make this image available, say on Dockerhub, other researchers could then download it and interact with your data/code via the fancy interactive Jupyter notebooks that you have prepared for them. We haven't made any fancy notebooks for you, but we have set up a Jupyter Notebook server. Try it out if you want to (replace the image name with your version if you've built it yourself):</p> <pre><code>docker run -it -p 8888:8888 nbisweden/workshop-reproducible-research \\\njupyter notebook  --ip=0.0.0.0 --allow-root\n</code></pre>"},{"location":"pages/containers/others/docker/#cleaning-up","title":"Cleaning up","text":"<p>As mentioned before, Docker tends to consume a lot of disk space. In general, <code>docker image rm</code> is used for removing images and <code>docker container rm</code> for removing containers. Here are some convenient commands for cleaning up.</p> <pre><code># Remove unused images\ndocker image prune\n\n# Remove stopped containers\ndocker container prune\n\n# Remove unused volumes (not used here, but included for reference)\ndocker volume prune\n\n# Stop and remove ALL containers\ndocker container rm $(docker container ls -a -q)\n\n# Remove ALL images\ndocker image rm $(docker image ls -a -q)\n</code></pre>"},{"location":"pages/containers/others/singularity/","title":"Introduction","text":"<p>Singularity is a container software alternative to Docker. It was originally developed by researchers at Lawrence Berkeley National Laboratory with focus on security, scientific software, and HPC clusters. One of the ways in which Singularity is more suitable for HPC is that it very actively restricts permissions so that you do not gain access to additional resources while inside the container.</p> <p>If you want to read more, here are some additional resources:</p> <ul> <li>Singularity docs</li> <li>Singularity publication</li> <li>Uppmax Singularity user guide</li> </ul> <p>This tutorial depends on files from the course GitHub repo. Take a look at the setup for instructions on how to set it up if you haven't done so already. Then open up a terminal and go to <code>workshop-reproducible-research/singularity</code>.</p>"},{"location":"pages/containers/others/singularity/#the-basics","title":"The basics","text":"<p>In the Docker tutorial we started by downloading an Ubuntu image. Let's see how the same thing is achieved with Singularity:</p> <pre><code>singularity pull library://ubuntu\n</code></pre> <p>This pulls an ubuntu image from the Singularity library (somewhat equivalent to Dockerhub). The first thing you might have noticed is that this command produces a file <code>ubuntu_latest.sif</code> in the current working directory. Singularity, unlike Docker, stores its images as a single file. Docker on the other hand uses layers, which can be shared between multiple images, and thus stores downloaded images centrally (remember the <code>docker images</code> command?). A Singularity image file is self-contained (no shared layers) and can be moved around and shared like any other file.</p> <p>To run a command in a Singularity container (equivalent of e.g. <code>docker run ubuntu uname -a</code>) we can execute:</p> <pre><code>singularity exec ubuntu_latest.sif uname -a\n</code></pre> <p>This should result in something similar to: <pre><code>Linux (none) 4.19.10 #1 SMP Mon Apr 8 00:07:40 CDT 2019 x86_64 x86_64 x86_64 GNU/Linux\n</code></pre></p> <p>Now, try to also run the following commands in the ubuntu container in the same manner as above:</p> <ul> <li><code>whoami</code></li> <li><code>ls -lh</code></li> </ul> <p>Notice anything unexpected or different from what you learnt from the Docker tutorial?</p> <p>Unlike Docker, Singularity attempts to map parts of your local file system to the image. By default Singularity bind mounts <code>$HOME</code>, <code>/tmp</code>, and <code>$PWD</code> (the current working directory) into your container. Also, inside a Singularity container, you are the same user as you are on the host system.</p> <p>We can also start an interactive shell (equivalent of e.g. <code>docker run -it ubuntu</code>):</p> <pre><code>singularity shell ubuntu_latest.sif\n</code></pre> <p>While running a shell in the container, try executing <code>pwd</code> (showing the full path to your current working directory). See that it appears to be your local working directory? Try <code>ls /</code> to see the files and directory in the root. Exit the container (<code>exit</code>) and run <code>ls /</code> to see how it looks on your local system. Notice the difference?</p> <p>Quick recap</p> <p>In this section we covered:</p> <ul> <li>How to download a Singularity image using <code>singularity pull</code></li> <li>How to run a command in a Singularity container using <code>singularity exec</code></li> <li>How to start an interactive terminal in a Singularity container using   <code>singularity shell</code></li> </ul>"},{"location":"pages/containers/others/singularity/#bind-mounts","title":"Bind mounts","text":"<p>In the previous section we saw how Singularity differs from Docker in terms of images being stored in stand-alone files and much of the host filesystem being mounted in the container. We will now explore this further.</p> <p>Similarly to the <code>-v</code> flag for Docker we can use <code>-B</code> or <code>--bind</code> to bind mount directories into the container. For example, to mount a directory into the <code>/mnt/</code> directory in the container one would do:</p> <pre><code>singularity shell -B /path/to/dir:/mnt ubuntu_latest.sif\n</code></pre> <p>You can try this for example by mounting the <code>conda/</code> tutorial directory to <code>/mnt</code>:</p> <pre><code>singularity shell -B ../conda:/mnt ubuntu_latest.sif\n</code></pre> <p>In the container, to see that the bind mounting worked, run e.g.:</p> <pre><code>ls /mnt/code\n</code></pre> <p>Now, this was not really necessary since <code>conda/</code> would have been available to us anyway since it most likely is a sub-directory under your <code>$HOME</code>, but it illustrates the capabilities to get files from the host system into the container when needed. Note also that if you run Singularity on say an HPC cluster, the system admins may have enabled additional default directories that are bind mounted automatically.</p> <p>Quick recap</p> <p>In this section we covered how to bind mount specific directories using <code>-B</code>.</p>"},{"location":"pages/containers/others/singularity/#pulling-docker-images","title":"Pulling Docker images","text":"<p>Singularity has the ability to convert Docker images to the Singularity Image Format (SIF). We can try this out by running:</p> <pre><code>singularity pull docker://godlovedc/lolcow\n</code></pre> <p>This command generates a .sif file where the individual layers of the specified Docker image have been combined and converted to Singularity's native format. We can now use <code>run</code>, <code>exec</code>, and <code>shell</code> commands on this image file. Try it:</p> <pre><code>singularity run lolcow_latest.sif\nsingularity exec lolcow_latest.sif fortune\nsingularity shell lolcow_latest.sif\n</code></pre> <p>Quick recap</p> <p>In this section we covered how to use <code>singularity pull</code> to download and run Docker images as Singularity containers.</p>"},{"location":"pages/containers/others/singularity/#building-from-scratch","title":"Building from scratch","text":"<p>As we have seen, it is possible to convert Docker images to the Singularity format when needed and run them using Singularity. In terms of making a research project reproducible using containers, it may be enough to e.g. define a Dockerfile (recipe for a Docker image) as well as supply a Docker image for others to download and use, either directly through Docker, or by Singularity. Even better, from a reproducibility aspect, would be to also generate the Singularity image from the Docker image and provide that for potential future users (since the image is a static file, whereas running <code>singularity pull</code> or <code>singularity build</code> would rebuild the image at the time of issuing the command).</p> <p>A third option, would be to define a Singularity recipe, either on its own or in addition to the Dockerfile. The equivalent of a Dockerfile for Singularity is called a Singularity Definition file (\"def file\").</p> <p>The def file consists of two parts:</p> <ul> <li>A header that defines the core OS and related features</li> <li>Optional sections, each starting with a <code>%</code>-sign, that add content or execute   commands during the build process</li> </ul> <p>As an example, we can look at the def file used for the image we played with above in the <code>singularity/</code> directory (we previously pulled lolcow from Dockerhub, but it also exists in the Singularity library and can be pulled by <code>singularity pull library://godlovedc/funny/lolcow</code>). The lolcow def file looks like this:</p> <pre><code>BootStrap: docker\nFrom: ubuntu:16.04\n\n%post\n    apt-get -y update\n    apt-get -y install fortune cowsay lolcat\n\n%environment\n    export LC_ALL=C\n    export PATH=/usr/games:$PATH\n\n%runscript\n    fortune | cowsay | lolcat\n</code></pre> <p>The first part of the header sets the bootstrap agent. In the lolcow example DockerHub is used. Alternatively one could set it to library to use the Singularity Library. There are also other bootstrap agents available (see this link for details). Next, the base image that the new image starts from is defined, in this case the Docker image <code>ubuntu:16.04</code>.</p> <p>In the lolcow def file three sections are used (<code>%post</code>, <code>%environment</code>, and <code>runscript</code>).</p> <ul> <li><code>%post</code> is similar to the <code>RUN</code> instruction in Dockerfiles. Here is where you   include code to download files from the internet, install software, create   directories etc.</li> <li><code>%environment</code> is similar to the <code>ENV</code> instruction in Dockerfiles. It is used   to set environmental variables that will be available when running the   container. The variables set in this section will not however be available   during the build and should in the cases they are needed then also be set in   the <code>%post</code> section.</li> <li><code>%runscript</code> is similar to the <code>CMD</code> instruction in Dockerfiles and contains   the default command to be executed when running the container.</li> </ul> <p>Quick recap</p> <p>In this section we covered the basic parts of a Singularity definition file (a def file), including  <code>BootStrap</code>, <code>From</code> <code>%post</code>, <code>%environment</code> and <code>%runscript</code>.</p>"},{"location":"pages/containers/others/singularity/#singularity-def-file-for-the-mrsa-project","title":"Singularity def file for the MRSA project","text":"<p>Let's use the MRSA case study project to define our own Singularity def file! We will not make an image for the whole workflow but rather focus on the <code>run_qc.sh</code> script that we used in the end of the conda tutorial. This script is included in the <code>code</code> directory of your current working directory (<code>singularity</code>) and, when executed, downloads a few fastq-files and runs FastQC on them. To run the script we need the software SRA-Tools and FastQC.</p> <ul> <li>Make a new file called <code>run_qc.def</code> and add the following lines:</li> </ul> <pre><code>Bootstrap: library\nFrom: ubuntu:16.04\n\n%labels\n    DESCRIPTION Image for running the run_qc.sh script\n    AUTHOR &lt;Your Name&gt;\n</code></pre> <p>Here we'll use the Singularity Library as bootstrap agent, instead of DockerHub as in the lol_cow example above. The base Singularity image will be <code>ubuntu:16.04</code>. We can also add metadata to our image using any name-value pair.</p> <ul> <li>Next, add the <code>%environment</code> section:</li> </ul> <pre><code>%environment\n    export LC_ALL=C\n    export PATH=/usr/miniconda3/bin:$PATH\n</code></pre> <p>This sets the default locale as well as includes the PATH to conda (which we will soon install).</p> <ul> <li>Now add the <code>%post</code> section:</li> </ul> <pre><code>%post\n    apt-get update\n    apt-get install -y --no-install-recommends bzip2 ca-certificates curl\n    apt-get clean\n\n    # Install conda:\n    curl -L https://repo.continuum.io/miniconda/Miniconda3-4.7.12.1-Linux-x86_64.sh -O\n    bash Miniconda3-4.7.12.1-Linux-x86_64.sh -bf -p /usr/miniconda3/\n    rm Miniconda3-4.7.12.1-Linux-x86_64.sh\n\n    # Configure conda:\n    conda config --add channels bioconda\n    conda config --add channels conda-forge\n\n    # Install requirements:\n    conda install -c bioconda fastqc=0.11.9 sra-tools=2.10.0\n    conda clean --all\n</code></pre> <p>You should recognize parts of this from the Docker tutorial. Basically, we install some required basic tools like bzip2, ca-certificates and curl, then install and configure conda and finally install the required tools for the <code>run_qc.sh</code> script.</p> <ul> <li>Next, add a <code>%test</code> section:</li> </ul> <pre><code>%test\n    fastqc --version\n    fastq-dump --version\n</code></pre> <p>The test section runs at the end of the build process and you can include any code here to verify that your image works as intended. Here we just make sure that the <code>fastqc</code> and <code>fastq-dump</code> are available.</p> <ul> <li>Finally, add the <code>%runscript</code>:</li> </ul> <pre><code>%runscript\n    bash code/run_qc.sh\n</code></pre> <p>We should now be ready to build our image from this def file using <code>singularity build</code>. Now, depending on the system you are running on and the version of Singularity, you may not have the option to build locally. However, Singularity has the option to build images remotely. To do this, you need to:</p> <ul> <li>Go to https://cloud.sylabs.io/library and   create an account</li> <li>Log in and find \"Access Tokens\" in the menu and create a new token</li> <li>Copy the token</li> <li>In your terminal, run <code>singularity remote login</code> and hit ENTER. You should be   asked to enter the token (API Key). Paste the copied token and hit ENTER.   You should get a API Key Verified! message.</li> </ul> <p>Attention</p> <p>In case you are not asked to enter the API Key, you can try to run <code>singularity remote login SylabsCloud</code> instead.</p> <p>We can now try to build the MRSA Singularity image using the <code>--remote</code> flag:</p> <pre><code>singularity build --remote run_qc.sif run_qc.def\n</code></pre> <p>Did it work? Can you figure out why it failed? Tip: it has to do with the PATH (well, to be fair, it could fail for several reasons, but if you did everything else correct it should be related to the PATH).</p> Click to show the solution <p>You need to add conda to the PATH. <code>%environment</code> makes it available at runtime but not during build.</p> <p>Update the <code>%post</code> section as follows:</p> <pre><code># Install conda:\ncurl -L https://repo.continuum.io/miniconda/Miniconda3-4.7.12.1-Linux-x86_64.sh -O\nbash Miniconda3-4.7.12.1-Linux-x86_64.sh -bf -p /usr/miniconda3/\nrm Miniconda3-4.7.12.1-Linux-x86_64.sh\nexport PATH=/usr/miniconda3/bin:$PATH ## &lt;- add this line\n</code></pre> <p>You also need to update the <code>%test</code> section:</p> <pre><code>export PATH=/usr/miniconda3/bin:$PATH ## &lt;- add this line\nfastqc --version\nfastq-dump --version\n</code></pre> <p>The build should now hopefully work and produce a Singularity image called <code>run_qc.sif</code>. To run the image, i.e. executing <code>code/run_qc.sh</code> using the tools in the container, do the following:</p> <pre><code>singularity run run_qc.sif\n</code></pre> <p>The fastq-files should now be downloaded and FastQC should be run on these files producing output directories and files in your current working directory.</p> <p>Tip</p> <p>We do not cover it here but it is possible to build Singularity images as writable sandbox images. This enables starting a shell in the container and e.g. installing software. This may be convenient during the design of the definition file to test what commands to include. When everything is working as expected one should rebuild directly from the definition file to a final SIF file.</p> <p>Note</p> <p>A somewhat important section that we have not covered here is the <code>%files</code> section. This is similar to the <code>ADD</code> or <code>COPY</code> instructions in a Dockerfile. One simply defines, on each line, a file to be copied from host to the container image using the format <code>&lt;source&gt; &lt;destination&gt;</code>. This does not currently work with <code>--remote</code> building though.</p>"},{"location":"pages/containers/others/singularity/#singularity-and-the-case-study","title":"Singularity and the case study","text":"<p>As a final example we will use <code>singularity build</code> to convert the Docker image of the MRSA project, that we use as a case study in this course, to a Singularity image:</p> <pre><code>singularity build --remote mrsa_proj.sif docker://nbisweden/workshop-reproducible-research\n</code></pre> <p>This should result in a file called <code>mrsa_proj.sif</code>.</p> <p>In the Docker image we included the code needed for the workflow in the <code>/course</code> directory of the image. These files are of course also available in the Singularity image. However, a Singularity image is read-only (unless using the sandbox feature), and this will be a problem if we try to run the workflow within the <code>/course</code> directory, since the workflow will produce files and Snakemake will create a <code>.snakemake</code> directory.</p> <p>Instead, we need to provide the files externally from our host system and simply use the Singularity image as the environment to execute the workflow in (i.e. all the software).</p> <p>In your current working directory (<code>singularity/</code>) the vital MRSA project files are already available (<code>Snakefile</code>, <code>config.yml</code>, <code>code/header.tex</code> and <code>code/supplementary_material.Rmd</code>).</p> <p>Since Singularity bind mounts the current working directory we can simply execute the workflow and generate the output files using:</p> <pre><code>singularity run --vm-ram 2048 mrsa_proj.sif\n</code></pre> <p>This executes the default run command, which is <code>snakemake -rp --configfile config.yml</code> (as defined in the original <code>Dockerfile</code>).</p> <p>Note</p> <p>Note here that we have increased the allocated RAM to 2048 MiB (<code>--vm-ram 2048</code>), needed to fully run through the workflow. In case the command fails, you can try to increase the RAM to e.g. 4096 MiB, or you can try to run the command without the  <code>--vm-ram</code> parameter.</p> <p>The previous step in this tutorial included running the <code>run_qc.sh</code> script, so that part of the workflow has already been run and Snakemake will continue from that automatically without redoing anything. Once completed you should see a bunch of directories and files generated in your current working directory, including the <code>results/</code> directory containing the final HTML report.</p>"},{"location":"pages/containers/others/singularity/#extra-material","title":"Extra material","text":"<p>A common problem with Singularity is that you can only create local builds if you are working on a Linux system, as local builds for MacOS and Windows are currently not supported. This means that you might favour using Docker instead of Singularity, but what happens when you need to use a HPC cluster such as Uppmax? Docker won't work there, as it requires root privileges, so Singularity is the only solution. You can only run Singularity images there, however, not build them...</p> <p>So, how do you get a Singularity image for use on Uppmax if you can't build it either locally or on Uppmax? You might think that using remote builds will solve this, but for a lot of cases this won't help. Since most researchers will want to work in private Git repositories they can't supply their Conda <code>environment.yml</code> file to remote builds (which only works for public repositories), which means that you'll have to specify packages manually inside the container instead.</p> <p>There is, however, another solution: using Singularity inside Docker. By creating a bare-bones, Linux-based Docker image with Singularity you can build Singularity images locally on non-Linux operating systems. This can be either from Singularity definition files or directly from already existing Docker images. You can read more about this at the following GitHub repository.</p>"},{"location":"pages/course-information/code-of-conduct/","title":"SouthGreen Training Code of Conduct","text":"<p>Training is one of the core values of SouthGreen, and benefits from the contributions of the entire scientific community. We value the involvement of everyone in the community. We are committed to creating a friendly and respectful place for learning, teaching and contributing. All participants in our events and communications are expected to show respect and courtesy to others.</p> <p>To make clear what is expected, everyone participating in SouthGreen courses is required to conform to the Code of Conduct. This Code of Conduct applies to all spaces managed by SouthGreen including, but not limited to, courses, email lists, and online forums such as Studium, GitHub, Slack, Twitter and LinkedIn. Course organizers and teachers are expected to assist with the enforcement of the Code of Conduct.</p> <p>We are dedicated to providing a welcoming and supportive environment for all people, regardless of background or identity. By participating in this event, participants accept to abide by the SouthGreen Code of Conduct and accept the procedures by which any Code of Conduct incidents are resolved. Any form of behaviour to exclude, intimidate, or cause discomfort is a violation of the Code of Conduct. In order to foster a positive and professional learning environment we encourage the following kinds of behaviours in all platforms and training events:</p> <ul> <li>Use welcoming and inclusive language</li> <li>Be respectful of different viewpoints and experiences</li> <li>Gracefully accept constructive criticism</li> <li>Focus on what is best to all those involved in this training event</li> <li>Show courtesy and respect towards everyone involved in this training event</li> </ul> <p>For an extended description please see the ELIXIR Code of Conduct.</p>"},{"location":"pages/course-information/pre-course-setup/","title":"Unix environment","text":""},{"location":"pages/course-information/pre-course-setup/#setup-for-mac-linux-users","title":"Setup for Mac / Linux users","text":"<p>You are lucky, using a UNIX based system as always been ideal for bioinformatics. Open a bash shell terminal and follow the installation instructions.</p> <p>First, create and move into a directory on your computer where it makes sense to work during the course e.g. <code>~/training-reproducible-research-area</code>.</p> <pre><code>mkdir ~/training-reproducible-research-area\ncd ~/training-reproducible-research-area\n</code></pre>"},{"location":"pages/course-information/pre-course-setup/#setup-for-windows-users","title":"Setup for Windows users","text":"<p>Using a Windows computer for bioinformatic work has sadly not been ideal most of the time, but large advanced in recent years have made this quite feasible through the Windows 10 Linux subsystem. This is the only setup for Windows users that we allow for participants of this course, as all the material has been created and tested to work on Unix-based systems.</p> <p>Using the Linux subsystem will give you access to a full command-line bash shell based on Linux on your Windows 10 PC. For the difference between the Linux Bash Shell and the PowerShell on Windows 10, see e.g. this article.</p> <p>Install Bash on Windows 10, follow the instructions at e.g. one of these resources:</p> <ul> <li>Installing the Windows Subsystem and the Linux Bash</li> <li>Installing and using Linux Bash on Windows</li> <li>Installing Linux Bash on Windows</li> </ul> <p>Note</p> <p>If you run into error messages when trying to download files through the Linux shell (e.g. <code>curl:(6) Could not resolve host</code>) then try adding the Google nameserver to the internet configuration by running <code>sudo nano /etc/resolv.conf</code> then add <code>nameserver 8.8.8.8</code> to the bottom of the file and save it.</p> <p>Open a bash shell Linux terminal and clone the GitHub repository containing all files you will need for completing the tutorials as follows. First, create and move into  a directory on your computer where it makes sense to work during the course e.g. <code>~/training-reproducible-research-area</code>.</p> <p>Tip</p> <p>You can find the directory where the Linux distribution is storing all its files by typing <code>explorer.exe .</code>. This will launch the Windows File Explorer showing the current Linux directory. Alternatively, you can find the Windows C drive from within the bash shell Linux terminal by navigating to <code>/mnt/c/</code>.</p> <pre><code>mkdir ~/training-reproducible-research-area\ncd ~/training-reproducible-research-area\n</code></pre> <p>Whenever a setup instruction specifies Mac or Linux (i.e. only those two, with no alternative for Windows), please follow the Linux instructions.</p>"},{"location":"pages/course-information/pre-course-setup/#git","title":"Git","text":"<p>Chances are that you already have git installed on your computer. You can check by running e.g. <code>git --version</code> in your terminal. If you don't have git, install it following the instructions here. If you have a very old version of git you might want to update to a later version.</p> <p>Git configuration will be done together during the training.</p>"},{"location":"pages/course-information/pre-course-setup/#conda","title":"Conda","text":"<p>Conda installation and configuration will be done together during the training event.</p>"},{"location":"pages/course-information/pre-course-setup/#snakemake","title":"Snakemake","text":"<p>Snakemake installation and configuration will be done together during the training event.</p>"},{"location":"pages/course-information/pre-course-setup/#nextflow","title":"Nextflow","text":"<p>Nextflow installation and configuration will be done together during the training event.</p>"},{"location":"pages/course-information/pre-course-setup/#r-markdown","title":"R Markdown","text":"<p>R Markdown installation and configuration will be done together during the training event.</p>"},{"location":"pages/course-information/pre-course-setup/#jupyter","title":"Jupyter","text":"<p>Jupyter installation and configuration will be done together during the training event.</p>"},{"location":"pages/course-information/pre-course-setup/#docker","title":"Docker","text":"<p>Installing Docker is quite straightforward on Mac or Windows and a little more cumbersome on Linux. Note that Docker runs as root, which means that you have to have <code>sudo</code> privileges on your computer in order to install or run Docker. When you have finished installing docker, regardless of which OS you are on, please type <code>docker --version</code> to verify that the installation was successful!</p>"},{"location":"pages/course-information/pre-course-setup/#macos-intelm1m2","title":"macOS intel/M1/M2","text":"<p>Go to docker.com and select download option that is suitable for your computer's architecture (i.e. if you have an Intel chip or a newer Apple M1 chip). This will download a <code>dmg</code> file - click on it when it's done to start the installation. This will open up a window where you can drag the Docker.app to Applications. Close the window and click the Docker app from the Applications menu. Now it's basically just to click \"next\" a couple of times and we should be good to go. You can find the Docker icon in the menu bar in the upper right part of the screen.</p>"},{"location":"pages/course-information/pre-course-setup/#linux","title":"Linux","text":"<p>How to install Docker differs a bit depending on your Linux distribution, but the steps are the same. Please follow the instructions for your distribution on https://docs.docker.com/engine/install/#server.</p> <p>Tip</p> <p>As mentioned before, Docker needs to run as root. You can achieve this by prepending all Docker commands with <code>sudo</code>. This is the approach that we will take in this tutorial, since the set up becomes a little simpler that way. If you plan on continuing using Docker you can get rid of this by adding your user to the group <code>docker</code>. Here are instructions for how to do this: https://docs.docker.com/engine/installation/linux/linux-postinstall/.</p>"},{"location":"pages/course-information/pre-course-setup/#windows","title":"Windows","text":"<p>In order to run Docker on Windows your computer must support Hardware Virtualization Technology and virtualization must be enabled. This is typically done in BIOS. Setting this is outside the scope of this tutorial, so we'll simply go ahead as if though it's enabled and hope that it works.</p> <p>On Windows 10 we will install Docker for Windows, which is available at docker.com. Click the link Download from Docker Hub, and select Get Docker. Once the download is complete, execute the file and follow the instructions. You can now start Docker from the Start menu. You can search for it if you cannot find it; the Docker whale icon should appear in the task bar.</p> <p>You will probably need to enable integration with the Linux subsystem, if you haven't done so during the installation of Docker Desktop. Right-click on the Docker whale icon in the task bar and select Settings. Choose Resources and select WPS integration. Enable integration with the Linux subsystem and click Apply &amp; Restart; also restart the Linux subsystem.</p>"},{"location":"pages/course-information/pre-course-setup/#singularity","title":"Singularity","text":"<p>Singularity installation might be tiedous. No worries if you do not succeed the installation.  We can reiew the installation procedure during the training event.</p> <p>Installation of Singularity depends, again, on your operating system. When you have finished, regardless of your OS, please type <code>singularity --version</code> to verify that your installation was successful!</p> <p>Both Mac and Windows utilise Vagrant, for which the information in the box below may help you.</p> <p>Vagrant and VirtualBox  The Vagrant VirtualBox with Singularity can be started like this:</p> <ul> <li>Move into the folder <code>vm-singularity</code> where you installed Singularity.</li> <li>Type <code>vagrant up</code> and once this has finished, verify that the Vagrant   VirtualBox is running with <code>vagrant status</code>.</li> <li>Now, type <code>vagrant ssh</code>, which will open the Vagrant VirtualBox.</li> <li>The first time you open the Vagrant VirtualBox like this, you will have to</li> </ul>"},{"location":"pages/course-information/pre-course-setup/#macos-intel","title":"macOS intel","text":"<p>Please follow the Mac-specific instructions at the Singularity website.</p> <p>Notes</p> <p>the command  <code>/usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"</code> must be replaced by this one <code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"</code> </p>"},{"location":"pages/course-information/pre-course-setup/#macos-m1m2","title":"macOS M1/M2","text":"<p>You're out of luck this time, as often with new OS and chip architectures, the tools may not yet be compatible. The proper way here is to use a linux virtual machine. You will have to download a linux distribution iso image and install it via a virtual machine manager. Please follow the instruction here.</p> <p>Notes</p> <p>You will need a linux iso image ARM architecture compatible. You can find rockylinux iso image at this address: https://rockylinux.org/download  Or use Ubuntu Server for ARM available here: https://ubuntu.com/download/server/arm</p>"},{"location":"pages/course-information/pre-course-setup/#linux_1","title":"Linux","text":"<p>Follow the Linux-specific instruction at the Singularity website.</p>"},{"location":"pages/course-information/pre-course-setup/#windows_1","title":"Windows","text":"<p>Please follow the Windows-specific instructions at the Singularity website.</p> <p>Notes</p> <p>Last time we checked, the software \"Vagrant Manager\" was not available for download but the installation of Singularity was successful even without it.</p> <p>Version 6.1.28 of \"Virtual box for Windows\" may not work, please install Version 6.1.26 from here in case you encounter problems when trying to start the Vagrant VirtualBox.</p>"},{"location":"pages/course-information/schedule/","title":"Schedule","text":""},{"location":"pages/course-information/schedule/#day-1","title":"Day 1","text":"Time Topic 09:00 Setting up 10:00 Introduction to Reproducible Research 10:30 Break 10:45 Data management and project organization 11:15 Break-out rooms and ice breaker session 11:30 Distributing and version tracking your code - Introduction to version control and Git  - Practical tutorial: Git 12:00 Lunch 13:30 ... continued: Git tutorial 15:00 Wrap-up Git tutorial 15:15 Break 15:30 Master your dependencies - environments and reproducibility - Introduction to the package and environment manager Conda - Practical tutorial: Conda 17:15 Wrap up: Conda tutorial"},{"location":"pages/course-information/schedule/#day-2","title":"Day 2","text":"Time Topic 09:00 Organize your analysis using workflow managers - Introduction to Snakemake - Practical tutorial: Snakemake 10:30 Break 10:45 ... continued: Snakemake tutorial 12:00 Lunch 13:30 ... continued: Snakemake tutorial 14:45 Wrap-up Snakemake tutorial 15:00 Organize your analysis using workflow managers- Introduction to Nextflow- Practical tutorial: Nextflow 15:45 Break 16:00 Containerization - Introduction to containers- Practical tutorial: Containers 17:45 Wrap-up"},{"location":"pages/course-information/schedule/#day-3","title":"Day 3","text":"Time Topic 09:00 ... continued: Containers 10:15 Break 10:30 Computational notebooks and reproducible reports- Introduction to R Markdown- Practical tutorial: R Markdown 12:00 Lunch 13:00 Computational notebooks and reproducible reports- Introduction to Jypyter- Practical tutorial: Jupyter 14:15 Computational notebooks and reproducible reports- Introduction to Quarto- Practical tutorial: Quarto 15:00 Break 15:15 Wrap-up - Computational notebooks and reproducible reports 15:30 Putting the pieces together- How to put all the tools and procedures together- How to implement these procedures on a day-to-day basis 16:30 End of the course! <p>The above schedule is approximate; variations may occur.</p>"},{"location":"pages/data-management/data-management/","title":"Data management and project organization","text":"<p>Good data organisation is the foundation towards working reproducibly for any research project. It not only sets you up well for an analysis, but it also makes it easier to come back to the project later and share with collaborators, including your most important collaborator \u2013 the future you.</p> <p>Much of the information of a project is digital, and we need to keep track of our digital records in the same way we have a lab notebook and sample freezer. In this lesson, we\u2019ll go through the project organisation and documentation that will make an efficient research workflow possible. Not only will this make you a more effective researcher, it also prepares your data and project for publication. Grant agencies and publishers increasingly require this information.</p>"},{"location":"pages/git/git-1-introduction/","title":"Introduction","text":"<p>Git is a widely used system (both in academia and industry) for version controlling files and collaborating on code. It is used to track changes in (text) files, thereby establishing a history of all edits made to each file, together with short messages about each change and information about who made it. Git is mainly run from the command line, but there are several tools that have implemented a graphical user interface to run Git commands.</p> <p>Using version control for tracking your files, and edits to those, is an essential step in making your computational research reproducible. A typical Git workflow consists of:</p> <ul> <li>Making distinct and related edits to one or several files</li> <li>Committing those changes (i.e. telling Git to add those edits to the   history, together with a message about what those changes involve)</li> <li>Pushing the commit to a remote repository (i.e. syncing your local project   directory with one in the cloud)</li> </ul> <p>There are many benefits of using Git in your research project:</p> <ul> <li>You are automatically forced into a more organized way of working, which is   usually a first step towards reproducibility.</li> <li>If you have made some changes to a file and realize that those were probably   not a good idea after all, it is simple to view exactly what the changes were   and revert them.</li> <li>If there is more than one person involved in the project, Git makes it easy   to collaborate by tracking all edits made by each person. It will also handle   any potential conflicting edits.</li> <li>Using a cloud-based repository hosting service (the one you push your commits   to), like e.g. GitHub or   Bitbucket, adds additional features, such as being   able to discuss the project, comment on edits, or report issues.</li> <li>If at some point your project will be published GitHub or Bitbucket (or   similar) are excellent places to publicly distribute your code. Other   researchers can then use Git to access the code needed for reproducing your   results, in exactly the state it was when used for the publication.</li> <li>If needed, you can host private repositories on GitHub and Bitbucket as well.   This may be convenient during an ongoing research project, before it is   publicly published.</li> </ul> <p>These tutorials will walk you through the basics of using Git as a tool for reproducible research. The things covered in these tutorials are what you will be using most of the time in your day-to-day work with Git, but Git has many more advanced features that might be of use to you.</p>"},{"location":"pages/git/git-2-creating-repositories/","title":"Creating a git repository","text":"<p>Warning</p> <p>To properly start this training session, you must be within a folder where it makes sense to work during the course: <pre><code>mkdir ~/training-reproducible-research-area\ncd ~/training-reproducible-research-area\n</code></pre></p> <p>Let's start by running this command:</p> <pre><code>git --version\n</code></pre> <p>You should get something like <code>git version 2.37.1 (Apple Git-137.1)</code>. That means git is well installed on your computer and you can go ahead. Then try the following command and see what happens: </p> <pre><code>git status\n</code></pre> <p>Tip</p> <p>If you try to run <code>git status</code> in a non-Git directory, it will say that it is not a git repository. The way this works is that Git adds a hidden directory <code>.git/</code> in the root of a Git tracked directory (run <code>ls -a</code> to see it). This hidden directory contains all information and settings Git needs in order to run and version track your files. This also means that your Git-tracked directory is self-contained, i.e. you can simply delete it and everything that has to do with Git in connection to that directory will be gone</p> <p>Let start by retrieving the Reproducible Research course git repository somewhere on internet:</p> <pre><code>git clone https://github.com/SouthGreenPlatform/training_reproducible_research\n</code></pre> <p>The <code>git clone</code> command copies a remote git repository locally. No worries, we will review this concept later. You now have a local directory called  <code>training_reproducible_research</code>, run <code>ls -l</code> to see it</p> <p>Move in it and run the <code>git status</code> command again, what do you see?</p> <pre><code>cd training_reproducible_research\ngit status\n</code></pre> <p>The directory you are looking is a version-tracked directory. Indeed the  <code>git status</code> command should return something like this:</p> <pre><code>On branch main\n\nYour branch is up to date with 'origin/main'.\n\nnothing to commit, working tree clean\n</code></pre> <p>Note</p> <p>git repository == version-tracked directory</p> <p>Let's now create our own git repository. First get out of the <code>training_reproducible_research</code>  directory and go back to the <code>git_tutorial</code> one:</p> <pre><code>cd ..\n</code></pre> <p>Warning</p> <p>From here you must be back to the <code>~/training-reproducible-research-area</code> directory  </p> <p>In order to create a new Git repository, we first need a directory to track. Let's create one and move inside:</p> <pre><code>mkdir git_tutorial\ncd git_tutorial\n</code></pre> <p>This directory is not yet a version-tracked directory, you can check it using again the <code>git status</code> command. Now we can initialise Git with the following command:</p> <pre><code>git init\n</code></pre> <p>The directory is now a version-tracked directory. How can you know? Run the command <code>git status</code>, which will probably return something like this:</p> <pre><code>On branch main\n\nNo commits yet\n\nnothing to commit (create/copy files and use \"git add\" to track)\n</code></pre> <p>The text <code>nothing to commit (create/copy files and use \"git add\" to track)</code> tells us that while we are inside a directory that Git is currently tracking, there are currently no files being tracked; let's add some!</p> <p>Copy the following files from the <code>training_reproducible_research</code> directory we get at  the beginning of the excercice into your <code>git_tutorial</code> directory:</p> <pre><code>cp ../training_reproducible_research/tutorials/git/Dockerfile .\ncp ../training_reproducible_research/tutorials/git/Snakefile .\ncp ../training_reproducible_research/tutorials/git/config.yml .\ncp ../training_reproducible_research/tutorials/git/environment.yml .\n</code></pre> <p>Once you have done that, run <code>git status</code> again. It will tell you that there are files in the directory that are not version tracked by Git.</p> <p>Note</p> <p>For the purpose of this tutorial, the exact contents of the files you just copied are not important, but we provide a brief overview here:</p> <ul> <li>The <code>environment.yml</code> file contains the Conda environment with all the   software used for a defined analysis.</li> <li>The <code>Snakefile</code> and <code>config.yml</code> are both used to define a Snakemake   workflow.</li> <li>The <code>Dockerfile</code> contains the recipe for making a Docker container for   a defined analysis.</li> </ul> <p>Quick recap</p> <p>We have used two <code>git</code> commands this far:</p> <ul> <li><code>git init</code> tells Git to track the current directory.</li> <li><code>git status</code> is a command you should use a lot. It will tell you,   amongst other things, the status of the current directory (version-tracked  or not),   the status of files contained in your reposiory (Tracked or not), of a Git clone    in relation to the online remote repository, etc.</li> </ul>"},{"location":"pages/git/git-3-committing-changes/","title":"Committing changes","text":"<p>We will now commit the untracked files. A commit is essentially a set of changes to a set of files. Preferably, the changes making out a commit should be related to something, e.g. a specific bug fix or a new feature.</p> <ul> <li>Our first commit will be to add the copied files to the repository. Run the   following (as suggested by <code>git status</code>):</li> </ul> <pre><code>git add Dockerfile Snakefile\n</code></pre> <ul> <li> <p>Run <code>git status</code> again! See that we have added Dockerfile and Snakefile to   our upcoming commit (listed under \"Changes to be committed\"). This is   called the staging area, and the files there are staged to be committed.</p> </li> <li> <p>We might as well commit all files in one go! Use <code>git add</code> on the remaining   files as well:</p> </li> </ul> <pre><code>git add config.yml environment.yml\n</code></pre> <ul> <li> <p>Run <code>git status</code> and see that all files are in the staging area, and that no   files are listed as untracked.</p> </li> <li> <p>We are now ready to commit! Run the following:</p> </li> </ul> <pre><code>git commit -m \"Add initial files\"\n</code></pre> <p>The <code>-m</code> option adds a commit message. This should be a short description of what the commit contains.</p> <p>Good commit messages</p> <p>It exists git commands to see what changed between commits and how it changed.  A commit message should tell you why it changed and provide context. Writing informative and succinct commit messages can be tricky when you're just starting out. Here are some general guidelines that can help you write good commit messages from the start:</p> <ul> <li>Separate subject from body with a blank line</li> <li>Limit the subject line to 50 characters</li> <li>Capitalize the subject line</li> <li>Do not end the subject line with a period</li> <li>Use the imperative mood   in the subject line</li> <li>Wrap the body at 72 characters</li> <li>Use the body to explain what and why vs. how</li> </ul> <p>In the command above we just added a short subject line (\"Add initial files\"). It is capitalized, less than 50 characters, does not end with a period, and uses imperative mood (Add!). It is possible to add a descriptive body text as well, as hinted by the points above. This is easiest done in a text editor. If you run <code>git commit</code> without the <code>-m</code> flag, Git will open the default terminal text editor (which can be configured with the <code>core.editor</code> variable) where you can write a longer commit message and body. If you want to read more about the motivation for these points, please see this website.</p> <ul> <li>Run <code>git status</code> again. It should tell you \"nothing to commit, working   directory clean\".</li> </ul> <p>What have we done, so far? We had some files in our working directory that we added to the Git staging area, which we subsequently committed to our Git repository. A schematic overview of this process can be seen in the following figure:</p> <p></p> <p>Let's repeat this process by editing a file!</p> <ul> <li> <p>Open up <code>environment.yml</code> in your favorite editor, and change the version of   bowtie2 to a different value, e.g. <code>bowtie2=2.2.4</code>.</p> </li> <li> <p>Run <code>git status</code>. It will tell you that there are modifications in one file   (<code>environment.yml</code>) compared to the previous commit. This is nice! We don't   have to keep track of which files we have edited, Git will do that for us.</p> </li> <li> <p>Run <code>git diff environment.yml</code>. This will show you the changes made to the   file. A <code>-</code> means a deleted line, a <code>+</code> means an added line. There are also   shown a few lines before and after the changes, to put them in context.</p> </li> <li> <p>Let's edit another file! Open <code>config.yml</code> and change the line <code>genome_id:   NCTC8325</code> to <code>genome_id: ST398</code>. Run <code>git status</code>. Run <code>git diff</code>. If we   don't specify a file, it will show all changes made in any file, compared to   the previous commit. Do you see your changes?</p> </li> <li> <p>Ok, we made our changes. Let's commit them! Run:</p> </li> </ul> <pre><code>git add config.yml environment.yml\n</code></pre> <p>This will add both our files to the staging area at the same time. Run <code>git status</code> and see that the changes in both <code>config.yml</code> and <code>environment.yml</code> are ready to be committed.</p> <p>But wait a minute! Shouldn't each commit optimally be a conceptual unit of change? Here we have one change to the genome ID used for an analysis and one change where another software version is specified: these should probably be separate. We thus want to make two commits, one for each change.</p> <ul> <li>Let's remove <code>environment.yml</code> from the staging area. <code>git status</code> tells us   how to do this: \"(use \"git reset HEAD ...\" to unstage)\". So run: <pre><code>git reset HEAD environment.yml\n</code></pre> <p>Note</p> <p>Maybe you didn't see the same message as indicated above? Is Git telling you to use a <code>git restore</code> instead? This is another one of Git's newer and experimental commands, which aims to remove some confusion about what commands do what (as many have multiple functions). While we have opted to stick with the old and stable commands until the new commands are no longer considered experimental, you are very welcome to use <code>git restore</code> instead of <code>git reset</code> to unstage the file above!</p> <ul> <li> <p>Run <code>git status</code> again. See that now only <code>config.yml</code> is staged for being   committed, whereas the changes in <code>environment.yml</code> are tracked by Git, but   not ready to be committed.</p> </li> <li> <p>Commit the changes in <code>config.yml</code>:</p> </li> </ul> <pre><code>git commit -m \"Change to ST398 for alignment\"\n</code></pre> <ul> <li>Add and commit the changes in <code>environment.yml</code>:</li> </ul> <pre><code>git status\ngit add environment.yml\ngit status\ngit commit -m \"Change bowtie2 version\"\ngit status\n</code></pre> <p>You don't have to run <code>git status</code> between each command, but it can be useful in the beginning while learning what each command does.</p> <p>As you can see, each commit is a point in history. The more often you commit, and the more specific you keep your commits, the better (more fine-grained) history and version tracking you will have of your files.</p> <ul> <li>We can also try to delete a file:</li> </ul> <pre><code>rm Dockerfile\n</code></pre> <ul> <li>Run <code>git status</code>. As you can see, Git tells us that the file is deleted, but   that the deletion is not committed. In the same way as we commit edits to   files, we need to commit a deletion of a file:</li> </ul> <pre><code>git add Dockerfile\ngit status\ngit commit -m \"Remove Dockerfile\"\ngit status\n</code></pre> <p>Here we used <code>rm Dockerfile</code> to delete the file and <code>git add Dockerfile</code> to stage the deletion. You can also use <code>git rm Dockerfile</code> to do both these operations in one step.</p> <ul> <li>To see a history of our changes so far, run:</li> </ul> <pre><code>git log\ngit log --oneline\n</code></pre> <p>What difference do you see between the two commands?</p> <p>Tip</p> <p>Since Git keeps track of changes in text, e.g. code and text-based documentation, there are some files which you should not commit. Examples of such files are file formats that are not text-based, e.g. Microsoft Word/Excel files or PDFs - although one might sometimes want to track one of these files regardless, such as when you have a static PDF report you received from a sequencing platform that's never going to change. Other files you shouldn't track are vary large text files, e.g. those larger than 50 MB.</p> <p>Quick recap</p> <p>We added four important Git commands to our repertoire:</p> <ul> <li><code>git add</code> adds a file to the staging area</li> <li><code>git commit</code> commits the changes we have staged</li> <li><code>git reset HEAD &lt;file&gt;</code> remove a file from the staging area</li> <li><code>git rm</code> is shorthand for <code>rm &lt;file&gt;; git add &lt;file&gt;</code></li> <li><code>git log</code> shows us the commit history</li> <li><code>git log --oneline</code> overview of the log, one line by commit</li> </ul>"},{"location":"pages/git/git-4-ignoring-files/","title":"Ignoring files","text":"<p>Git is aware of all files within the repository. However, it is not uncommon to have files that we don't want Git to track. For instance, our analysis might produce several intermediate files and results. We typically don't track such files. Rather, we want to track the actual code and other related files (e.g. configuration files) that produce the intermediate and result files, given the raw input data.</p> <ul> <li>Let's make some mock-up intermediate and result files. These are some of the   files that would have been generated by the Snakemake workflow if it was run.</li> </ul> <pre><code>mkdir intermediate\nmkdir results\ntouch intermediate/multiqc_general_stats.txt\ntouch results/supplementary.pdf\ntouch log.tmp\n</code></pre> <ul> <li>Run <code>git status</code>. You will see that Git tells you that you have untracked   files. However, we don't want Git to track these files anyway. To tell Git   what files to ignore we use a file called <code>.gitignore</code>. Let's create it:</li> </ul> <pre><code>touch .gitignore\n</code></pre> <ul> <li>Open the <code>.gitignore</code> file in a text editor and add the following lines to it:</li> </ul> <pre><code># Ignore these directories:\nresults/\nintermediate/\n\n# Ignore temporary files:\n*.tmp\n</code></pre> <ul> <li> <p>Run <code>git status</code> again. Now there is no mention of the <code>results</code> and   <code>intermediate</code> directories or the <code>log.tmp</code> file. Notice that we can use   wildcards (*) to ignore files with a given pattern, e.g. a specific   file extension.</p> </li> <li> <p>Sometimes you want to ignore all files in a directory with one or two   exceptions. For example, you don't want to track all your huge raw data files,   but there may be a smaller data file that you do want to track, e.g.   metadata or a list of barcodes used in your experiment. Let's add some mock   data:</p> </li> </ul> <pre><code>mkdir data\ntouch data/huge.fastq.gz\ntouch data/metadata.txt\n</code></pre> <ul> <li>Git allows you to ignore all files using the aforementioned wildcard, but   then exclude certain files from that ignore command. Open the <code>.gitignore</code>   file again and add the following:</li> </ul> <pre><code># Ignore all files in the data/ directory\ndata/*\n\n# Exclude the metadata file by prefixing it with an exclamation mark\n!data/metadata.txt\n</code></pre> <ul> <li>Finish up by adding the <code>.gitignore</code> and <code>data/metadata.txt</code> files to the   staging area and committing them:</li> </ul> <pre><code>git add .gitignore\ngit commit -m \"Add .gitignore file\"\ngit add data/metadata.txt\ngit commit -m \"Add metadata file\"\n</code></pre> <p>Tip</p> <p>It is common for certain programming languages or text editors to leave e.g. swap files or hidden data files in the working directory, which you don't want to track using Git. Instead of manually adding these to every single project you have, you can use the <code>.gitignore_global</code> file, which should be placed in your home directory. It works exactly like a normal gitignore file, but is applied to all Git repositories that you are using on your machine. Some common file extensions that might be put in the global gitignore are <code>.DS_Store</code> if you're working in R or <code>.swp</code> if you're coding in vim. To configure git to use the <code>.gitignore_global</code> file you can run <code>git config --global core.excludesfile ~/.gitignore_global</code>.</p> <p>Quick recap</p> <p>We now learned how to ignore certain files and directories:</p> <ul> <li>The <code>.gitignore</code> file controls which files and directories Git should ignore, if any.</li> <li>Specific files can be excluded from ignored directories using the exclamation mark (<code>!</code>) prefix.</li> </ul>"},{"location":"pages/git/git-5-branches/","title":"Branching and merging","text":"<p>One of the most useful features of Git is called branching. Branching allows you to diverge from the main line of work and edit or update your code and files (e.g. to test out a new analysis or some experimental feature) without affecting your main work. If the work you did in the branch turns out to be useful you can merge that back into your <code>main</code> branch. On the other hand, if the work didn't turn out as planned, you can simply delete the branch and continue where you left off in your main line of work. Another use case for branching is when you are working in a project with multiple people. Branching can be a way of compartmentalizing your team's work on different parts of the project and enables merging back into the <code>main</code> branch in a controlled fashion; we will learn more about this in the section about working remotely.</p> <ul> <li>Let's start trying out branching! We can see the current branch by running:</li> </ul> <pre><code>git branch\n</code></pre> <p>This tells us that there is only the <code>main</code> branch at the moment.</p> <p>Main and Master</p> <p>If your branch is called <code>master</code> instead of <code>main</code> that's perfectly fine as well, but do check out the Git section of the pre-course setup for more details about the choice of default branch names.</p> <ul> <li>Let's make a new branch:</li> </ul> <pre><code>git branch test_alignment\n</code></pre> <ul> <li> <p>Run <code>git branch</code> again to see the available branches. Do you note which one   is selected as the active branch?</p> </li> <li> <p>Let's move to our newly created branch using the <code>checkout</code> command:</p> </li> </ul> <pre><code>git checkout test_alignment\n</code></pre> <p>Tip</p> <p>You can create and checkout a new branch in one line with <code>git checkout -b branch_name</code>.</p> <p>Let's add some changes to our new branch! We'll use this to try out a different set of parameters on the sequence alignment step of the case study project.</p> <ul> <li>Edit the <code>Snakefile</code> so that the shell command of the <code>align_to_genome</code> rule   looks like this (add the <code>--very-sensitive-local</code> option):</li> </ul> <pre><code>bowtie2 --very-sensitive-local -x $indexBase -U {input.fastq} &gt; {output} 2&gt; {log}\n</code></pre> <ul> <li> <p>Add and commit the change!</p> </li> <li> <p>To get a visual view of your branches and commits you can use the command:</p> </li> </ul> <pre><code>git log --graph --all --oneline\n</code></pre> <p>It is often useful to see what differences exist between branches. You can use the <code>diff</code> command for this:</p> <pre><code>git diff main\n</code></pre> <p>This shows the difference between the active branch (<code>test_alignment</code>) and <code>main</code> on a line-per-line basis. Do you see which lines have changed between <code>test_alignment</code> and <code>main</code> branches?</p> <p>Tip</p> <p>We can also add the <code>--color-words</code> flag to <code>git diff</code>, which instead displays the difference on a word-per-word basis rather than line-per-line.</p> <p>Note</p> <p>Git is constantly evolving, along with some of its commands. While the <code>checkout</code> command is quite versatile (it's used for more than just switching branches), this versatility can sometimes be confusing. The Git team thus added a new command, <code>git switch</code>, that can be used instead. This command is still experimental, however, so we have opted to stick with <code>checkout</code> for the course - for now.</p> <p>Now, let's assume that we have tested our code and the alignment analysis is run successfully with our new parameters. We thus want to merge our work into the <code>main</code> branch. It is good to start with checking the differences between branches (as we just did) so that we know what we will merge.</p> <ul> <li>Checkout the branch you want to merge into, i.e. <code>main</code>:</li> </ul> <pre><code>git checkout main\n</code></pre> <ul> <li>To merge, run the following code:</li> </ul> <pre><code>git merge test_alignment\n</code></pre> <p>Run <code>git log --graph --all --oneline</code> again to see how the merge commit brings back the changes made in <code>test_alignment</code> to <code>main</code>.</p> <p>Tip</p> <p>If working on different features or parts of an analysis on different branches, and at the same time maintaining a working <code>main</code> branch for the stable code, it is convenient to periodically merge the changes made to <code>main</code> into relevant branches (i.e. the opposite to what we did above). That way, you keep your experimental branches up-to-date with the newest changes and make them easier to merge into <code>main</code> when time comes.</p> <ul> <li>If we do not want to do more work in <code>test_alignment</code> we can delete that   branch:</li> </ul> <pre><code>git branch -d test_alignment\n</code></pre> <ul> <li>Run <code>git log --graph --all --oneline</code> again. Note that the commits and   the graph history are still there? A branch is simply a pointer to a   specific commit, and that pointer has been removed.</li> </ul> <p>Tip</p> <p>There are many types of so-called \"branching models\", each with varying degrees of complexity depending on the developer's needs and the number of collaborators. While there certainly isn't a single branching model that can be considered to be the \"best\", it is very often most useful to keep it simple. An example of a simple and functional model is to have a <code>main</code> branch that is always working (i.e. can successfully run all your code and without known bugs) and develop new code on feature branches (one new feature per branch). Feature branches are short-lived, meaning that they are deleted once they are merged into <code>main</code>.</p> <p>Quick recap</p> <p>We have now learned how to divide our work into branches and how to manage them:</p> <ul> <li><code>git branch &lt;branch&gt;</code> creates a new branch.</li> <li><code>git checkout &lt;branch&gt;</code> moves the repository to the state in which the specified branch is currently in.</li> <li><code>git merge &lt;branch&gt;</code> merges the specified branch into the current one.</li> </ul>"},{"location":"pages/git/git-6-tags/","title":"Tagging commits","text":"<p>Git allows us to tag commits, i.e. give names to specific points in the history of our project. This can be particularly important for reproducible research, but also for development projects that want to highlight specific versions of a software. A tag can be, for example, the version of the repository that was used for the manuscript submission, the version used during resubmission, and, most importantly, the version used for the final publication. The first two examples are mainly useful internally, but the latter is essential for other researchers to be able to rerun your published analysis.</p> <ul> <li>Let's assume that the status of the repository as it is now is ready for   a submission to a journal. It may for example contain the scripts that were   used to generate the manuscript figures. Let's add a tag:</li> </ul> <pre><code>git tag \"submission1\"\n</code></pre> <ul> <li>We can now list all the tags available in the current repository:</li> </ul> <pre><code>git tag\n</code></pre> <p>Tip</p> <p>You can use the flag <code>-a</code> or <code>--annotate</code> to give more detailed information about a specific tag, similar to a commit message. This can be quite useful when there are many changes that happened, in that it allows you to summarise them. You can, for example, do <code>git tag -a submission1 -m \"Annotation for tag submission1\"</code> to write the annotation along with the command (similar to the <code>-m</code> flag for committing) or just <code>git tag -a submission1</code> to write the annotation with your default editor. To list all your tags along with their annotations you can use e.g. <code>git tag -n10</code> (which will list the first 10 lines of each tag's annotation).</p> <ul> <li>Let's assume we now got comments from the reviewers, and by fixing   those we had to update our code. Open <code>config.yml</code> and change the line   <code>max_reads: 25000</code> to <code>max_reads: 50000</code>. Commit and tag the changes:</li> </ul> <pre><code>git add config.yml\ngit commit -m \"Increase number of reads\"\ngit tag \"revision1\"\n</code></pre> <ul> <li>Now let's say that the reviewers were happy and the manuscript was   accepted for publication. Let's immediately add a tag:</li> </ul> <pre><code>git tag \"publication\"\n</code></pre> <ul> <li>A good thing about using tags is that you can easily switch between versions   of your code. Let's move to the first submission version:</li> </ul> <pre><code>git checkout submission1\n</code></pre> <ul> <li>Open <code>config.yml</code> and note that the <code>max_reads</code> variable is <code>25000</code>! To go   back to the latest version, run:</li> </ul> <pre><code>git checkout main\n</code></pre> <ul> <li>Open <code>config.yml</code> and see that the value is now <code>50000</code>.</li> </ul> <p>Tip</p> <p>You can also see the difference between tags in the same way as for branches and commits using e.g. <code>git diff &lt;tag1&gt; &lt;tag2&gt;</code>.</p> <p>At this point could run <code>git log --oneline --decorate</code> to get a condensed commit history, where you should also be able to see the tagged commits.</p> <p>Quick recap</p> <p>We have now learned how to tag important commits:</p> <ul> <li><code>git tag</code> adds a tag to a commit.</li> <li><code>git checkout</code> moves between tags in a similar fashion as between branches.</li> </ul>"},{"location":"pages/git/git-7-working-remotely/","title":"Working remotly","text":"<p>So far we've only been working on files present on our own computer, i.e. locally. While Git is an amazing tool for reproducibility even if you're working alone, it really starts to shine in collaborative work. This entails working with remote repositories, i.e. repositories that are stored somewhere online; some of the most common places to store your repositories are GitHub, BitBucket and GitLab. GitHub is the most popular of these, and is what we'll be using for this tutorial.</p> <p>An important thing to keep in mind here is the difference between Git (the version control system) and online hosting of Git repositories (such as GitHub): the former is the core of keeping track of your code's history, while the latter is how to store and share that history with others.</p>"},{"location":"pages/git/git-7-working-remotely/#github-setup","title":"GitHub setup","text":"<p>If you have not done so already, go to github.com and create an account. You can also create an account on another online hosting service for version control, e.g. Bitbucket or GitLab. The exercises below are written with examples from GitHub (as that is the most popular platform with the most extensive features), but the same thing can be done on alternative services, although the exact menu structure and link placements differ a bit.</p> <p>Any upload to and from GitHub requires you to authenticate yourself. GitHub used to allow authentication with your account and password, but this is no longer the case - using SSH keys is favoured instead. Knowing exactly what these are is not necessary to get them working, but we encourage you to read the box below to learn more about them! GitHub has excellent, platform-specific instructions both on how to generate and add SSH keys to your account, so please use them before moving on!</p> <p>SSH keys and authentication</p> <p>Using SSH (Secure Shell) for authentication basically entails setting up a pair of keys: one private and one public. You keep the private key on your local computer and give the public key to anywhere you want to be able to connect to, e.g. GitHub. The public key can be used to encrypt messages that only the corresponding private key can decrypt. A simplified description of how SSH authentication works goes like this:</p> <ol> <li>The client (i.e. the local computer) sends the ID of the SSH key pair it   would like to use for authentication to the server (e.g. GitHub)</li> <li>If that ID is found, the server generates a random number and encrypts this   with the public key and sends it back to the client</li> <li>The client decrypts the random number with the private key and sends it   back to the server</li> </ol> <p>Notice that the private key always remains on the client's side and is never transferred over the connection; the ability to decrypt messages encrypted with the public key is enough to ascertain the client's authenticity. This is in contrast with using passwords, which are themselves sent across a connection (albeit encrypted). It is also important to note that even though the keys come in pairs it is impossible to derive the private key from the public key. If you want to read more details about how SSH authentication work you can check out this website, which has more in-depth information than we provide here.</p>"},{"location":"pages/git/git-7-working-remotely/#create-a-remote-repository","title":"Create a remote repository","text":"<p>Log in to your GitHub account and press the New button:</p> <ul> <li>Make sure you are listed as the owner</li> <li>Add a repository name, e.g. <code>git_tutorial</code></li> <li>You can keep the repo private or make it public, as you wish</li> <li>Skip including a README, a <code>.gitignore</code> and license</li> </ul> <p>Note</p> <p>When creating a new repository the best practices is to directly set the README,  the <code>.gitignore</code> and the licence. In our case we must not because we already have initiated a  repository locally that we want to push remotly. If we put anything  in the remote repository before linking it to the local one, a new tracking history will  be created in the remote and it will be divergent compared to the local tracking history.  You will end up with a <code>fatal: refusing to merge unrelated histories</code> error.</p> <p></p> <p>You will now be redirected to the repository page which will list several ways for you to start adding content (files) to the repository. What we will do is to connect the local repository we've been working on so far to the remote GitHub server using SSH:</p> <ul> <li>Add a remote SSH address to your local repository (make sure you change   <code>user</code> to your GitHub username and <code>git_tutorial</code> to your repository name):</li> </ul> <pre><code>git remote add origin git@github.com:user/git_tutorial.git\n</code></pre> <ul> <li>Run <code>git remote -v</code>. This will show you what remote location is connected to   your local Git clone. The short name of the default remote is usually   \"origin\" by convention.</li> </ul> <p>Note</p> <p>Make sure you've used an SSH address (i.e. starting with <code>git@github.com</code> rather than an HTTPS address (starting with <code>https://github.com</code>)!</p> <ul> <li>We have not yet synced the local and remote repositories, though, we've simply   connected them. Let's sync them now:</li> </ul> <pre><code>git push origin main\n</code></pre> <p>The <code>push</code> command sends our local history of the <code>main</code> branch to the same branch on the remote (<code>origin</code>). Our Git repository is now stored on GitHub!</p> <ul> <li>Run <code>git status</code>. This should tell you that:</li> </ul> <pre><code>On branch main\nnothing to commit, working tree clean\n</code></pre> <p>You always need to specify <code>git push origin main</code> by default, but you can circumvent this by telling Git that you always want to push to <code>origin/main</code> when you're on your local <code>main</code> branch. To do this, use the command <code>git branch --set-upstream-to origin/main</code>. Try it out now.</p> <ul> <li>Now run <code>git-status</code> again. You should see that now git additionally tells you that your local branch is up to date with the remote branch.</li> </ul> <p>If you go to the repository's GitHub page you should now be able to see all your files and your code there! It should look something like this:</p> <p></p> <p>You can see a lot of things there, such as each file and the latest commit that changed them, the repository's branches and a message from GitHub at the bottom: \"Help people interested in this repository understand your project by adding a README.\" This refers to GitHub's built-in functionality of automatically rendering any markdown document named <code>README</code> or <code>README.md</code> in the repository's root directory and displaying it along with what you can already see. Let's try it out!</p> <p>Info</p> <p>You can find nice README Template on recherche.data.gouv.fr web site.</p> <ul> <li>Let's create a <code>README.md</code> file and fill it with the following text:</li> </ul> <pre><code># A Git tutorial\n\nThis repository contains tutorial information related to the **SouthGreen** course\n*Tools for Reproducible Research* based on the **NBIS/ELIXIR** material, specifically the session on using the `git`\nsoftware for version control.\n\n## Links\n\nYou can find the latest stable version of the Git tutorial for the course\n[here](https://uppsala.instructure.com/courses/73110/pages/git-1-introduction?module_item_id=367079).\n</code></pre> <ul> <li>Add, commit and push these changes to GitHub.</li> </ul> <pre><code>git add README.md\ngit commit -m \"Add README.md\"\ngit push origin main\n</code></pre> <p>You should now be able to see the rendered markdown document, which looks a bit different from the text you copied in from above. Note that there are two different header levels, which come from the number of hash signs (<code>#</code>) used. You can also see bold text (which was surrounded by two asterisks), italic text (surrounded by one asterisk), in-line code (surrounded by acute accents) and a link (link text inside square brackets followed by link address inside parentheses).</p> <p>It is important to add README-files to your repositories so that they are better documented and more easily understood by others and, more likely, your future self. In fact, documentation is an important part of reproducible research! While the tools that you are introduced to by this course are all directly related to making science reproducible, you will also need good documentation. Make it a habit of always adding README-files for your repositories, fully explaining the ideas and rationale behind the project. You can even add README-files to sub-directories as well, giving you the opportunity to go more in-depth where you so desire.</p> <p>Tip</p> <p>There are a lot more things you can do with markdown than what we show here. Indeed, this entire course is mostly written in markdown! You can read more about markdown here.</p> <p>Quick recap</p> <p>We learned how to connect local Git repositories to remote locations such as GitHub and how to upload commits using <code>git push</code>. We also learned the basics of markdown and how it can be used to document Git repositories.</p>"},{"location":"pages/git/git-7-working-remotely/#browsing-github","title":"Browsing GitHub","text":"<p>GitHub and the rest of the websites that offer remote hosting of git repositories all have numerous features, which can be somewhat difficult to navigate in the beginning. We here go through some of the basics of what you can do with GitHub.</p> <ul> <li>Go to your GitHub repository in your browser again and click on Code to the   left. Click on <code>config.yml</code>. You will see the contents of the file. Notice   that it is the latest version, where we previously changed the <code>genome_id</code>   variable:</li> </ul> <p></p> <ul> <li>Click on History. You will see an overview of the commits involving changes   made to this file:</li> </ul> <p></p> <ul> <li>Click on the <code>Change to ST398 for alignment</code> commit. You will see the changes   made to <code>config.yml</code> file compared to the previous commit.</li> </ul> <p></p> <ul> <li>Go back to the repository's main page and click on the commit tracker on the   right above the list of files, which will give you an overview of all commits   made. Clicking on a specific commit lets you see the changes introduced by   that commit. Click on the commit that was the initial commit, where we added   all the files.</li> </ul> <p></p> <p>You will now see the files as they were when we first added them. Specifically you can see that the <code>Dockerfile</code> is back, even though we deleted it! Click on the Code tab to the left to return to the overview of the latest repository version.</p> <p>Quick recap</p> <p>We learned some of the most important features of the GitHub interface and how repositories can be viewed online.</p>"},{"location":"pages/git/git-7-working-remotely/#working-with-remote-repositories","title":"Working with remote repositories","text":"<p>While remote repositories are extremely useful as backups and for collaborating with others, that's not their only use: remotes also help when you are working from different computers, a computer cluster or a cloud service.</p> <ul> <li>Let's pretend that you want to work on this repository from a different   computer. First, create a different directory (e.g. <code>git_remote_tutorial</code>)   in a separate location that is not already tracked by Git and <code>cd</code> into it.   Now we can download the repository we just uploaded using the following:</li> </ul> <pre><code>git clone git@github.com:user/git_tutorial.git .\n</code></pre> <p>Again, make sure to replace <code>user</code> with your GitHub user name.</p> <p>Notice the dot at the end of the command above, which will put the clone into the current directory, instead of creating a new directory with the same name as the remote repository. You will see that all your files are here, identical to the original <code>git_tutorial</code> repository!</p> <ul> <li> <p>Since you already gave the address to Git when you cloned the repository, you   don't have to add it manually as before. Verify this with <code>git remote -v</code>.</p> </li> <li> <p>Let's say that we now want to change the <code>multiqc</code> software to an earlier   version: open the <code>environment.yml</code> file in the second local repo and change   <code>multiqc=1.12</code> to <code>multiqc=1.7</code>; add and commit the change.</p> </li> <li> <p>We can now use <code>push</code> again to sync our remote repository with the new local   changes. Refresh your web page again and see that the changes have taken   effect.</p> </li> </ul> <p>Since we have now updated the remote repository with code that came from the second local repository, the first local repository is now outdated. We thus need to update the first local repo with the new changes. This can be done with the <code>pull</code> command.</p> <ul> <li> <p><code>cd</code> back into the first local repository (e.g. <code>git_tutorial</code>) and run the   <code>git pull</code> command. This will download the newest changes from the remote   repository and merge them locally automatically.</p> </li> <li> <p>Check that everything is up-to-date with <code>git status</code>.</p> </li> </ul> <p>Another command is <code>git fetch</code>, which will download remote changes without merging them. This can be useful when you want to see if there are any remote changes that you may want to merge, without actually doing it, such as in a collaborative setting. In fact, <code>git pull</code> in its default mode is just a shorthand for <code>git fetch</code> followed by <code>git merge FETCH_HEAD</code> (where <code>FETCH_HEAD</code> points to the tip of the branch that was just fetched).</p> <p>That's quite a few concepts and commands you've just learnt! It can be a bit hard to keep track of everything and the connections between local and remote Git repositories and how you work with them, but hopefully the following figure will give you a short visual summary:</p> <p></p> <p>Quick recap</p> <p>We have learned the difference between local and remote copies of git repositories and how to sync them:</p> <ul> <li><code>git push</code> uploads commits to a remote repository</li> <li><code>git pull</code> downloads commits from a remote repository and merges them to the local branch</li> <li><code>git fetch</code> downloads commits from a remote repository without merging them to the local branch</li> <li><code>git clone</code> makes a local copy of a remote repository</li> </ul>"},{"location":"pages/git/git-7-working-remotely/#remote-branches","title":"Remote branches","text":"<p>Remote branches work much in the same way a local branches, but you have to push them separately; you might have noticed that GitHub only listed our repository as having one branch (you can see this by going to the Code tab). This is because we only pushed our <code>main</code> branch to the remote. Let's create a new local branch and add some changes that we'll push as a separate branch to our remote - you should do this in the original <code>git_tutorial</code> repository, so move back into that directory.</p> <ul> <li>Create a new branch named <code>trimming</code> and add the <code>--trim5 5</code> flag to the   bowtie2-command part of the <code>Snakefile</code>, which should now look like this:</li> </ul> <pre><code>bowtie2 --trim5 5 --very-sensitive-local -x $indexBase -U {input.fastq} &gt; {output} 2&gt; {log}\n</code></pre> <ul> <li> <p>Add and commit the change to your local repository.</p> </li> <li> <p>Instead of doing what we previously did, i.e. merge the <code>trimming</code> branch   into the <code>main</code> branch, we'll push <code>trimming</code> straight to our remote:</p> </li> </ul> <pre><code>git push origin trimming\n</code></pre> <ul> <li>Go the repository at GitHub and see if the new branch has appeared. Just above   the file listing click the Branch drop-down and select the new branch to   view it. Can you see the difference in the <code>Snakefile</code> depending on which   branch you choose?</li> </ul> <p>We now have two branches both locally and remotely: <code>main</code> and <code>trimming</code>. We can continue working on our <code>trimming</code> branch until we're satisfied (all the while pushing to the remote branch with the same name), at which point we want to merge it into <code>main</code>.</p> <ul> <li> <p>Checkout your local <code>main</code> branch and merge it with the <code>trimming</code> branch.</p> </li> <li> <p>Push your <code>main</code> branch to your remote and subsequently delete your local   <code>trimming</code> branch.</p> </li> </ul> <p>The above command only deleted the local branch. If you want to remove the branch from the remote repository as well, run:</p> <pre><code>git push origin --delete trimming\n</code></pre> <p>Quick recap</p> <p>We learned how to push local branches to a remote with <code>git push origin &lt;branch&gt;</code> and how to delete remote branches with <code>git push origin --delete &lt;branch&gt;</code>.</p>"},{"location":"pages/git/git-7-working-remotely/#sharing-tags","title":"Sharing tags","text":"<p>Your local repository tags are not included when you do a normal push. To push tags to the remote you need to supply the <code>--tags</code> flag to the <code>git push</code> command:</p> <pre><code>git push --tags\n</code></pre> <ul> <li> <p>Go to the repository overview page on GitHub. You will see that the repository   now has three tags! If you click on Tags you will be given an overview of   the existing tags for your repository - if you click Releases you will see   more or less the same information. Confusing? Well, a tag is a Git concept   while a release is a GitHub concept that is based on Git tags. Releases add   some extra features that can be useful for distributing software and are done   manually from the repository's GitHub page.</p> </li> <li> <p>Click on one of the tags. Here users can download a compressed file   containing the repository at the version specified by the tags.</p> </li> </ul> <p></p> <p>Alternatively, Git users who want to reproduce your analysis with the code used for the publication can clone the GitHub repository and then run <code>git checkout publication</code>.</p> <p>Quick recap</p> <p>We learned how to push Git tags to a remote by using the <code>--tags</code> flag.</p>"},{"location":"pages/git/git-8-conflicts/","title":"Conflict","text":"<p>It is not uncommon to run into conflicts when you are trying to merge separate branches, and it's even more common when you're working in a collaborative setting with remote repositories. It'll happen sooner or later, even if you're only working locally, so it's important to know how to deal with them! We'll now introduce a conflict on purpose, which we can then solve.</p> <ul> <li>Remember that we have two separate local copies of the same repository? Let's   go into the first one, <code>git_tutorial</code>, and change the MultiQC version in the   <code>environment.yml</code> file:</li> </ul> <pre><code>multiqc=1.8\n</code></pre> <ul> <li>Add, commit and push your change to the remote.</li> </ul> <p>Now we have a change in our remote and one of our local copies, but not in the other. This could happen if a collaborator of yours committed a change and pushed it to GitHub. Let's create a conflict!</p> <ul> <li> <p>Move into your other local repository, <code>git_remote_tutorial</code>, which doesn't   have the new change. Run <code>git status</code>. Notice that Git says: \"Your branch is   up-to-date with 'origin/main'.\". We know that this is not true, but this   local clone is not yet aware of the remote changes.</p> </li> <li> <p>Let's change the <code>environment.yml</code> file in this local repository as well, but   to version 1.6, instead! It may be the case that your collaborator thought it   was good to use MultiQC version 1.8, whereas you thought it would be better to   use MultiQC version 1.6, but neither of you communicated that to the other.</p> </li> <li> <p>Add and commit your change and try to push the commit, which should give you   an error message that looks like this:</p> </li> </ul> <pre><code> ! [rejected]        main -&gt; main (fetch first)\nerror: failed to push some refs to 'https://github.com/user/git_tutorial.git'\nhint: Updates were rejected because the remote contains work that you do\nhint: not have locally. This is usually caused by another repository pushing\nhint: to the same ref. You may want to first integrate the remote changes\nhint: (e.g., 'git pull ...') before pushing again.\nhint: See the 'Note about fast-forwards' in 'git push --help' for details.\n</code></pre> <p>This error message is thankfully quite informative in regards to what is going on and what might be done about it. In essence it will not allow you to push to the remote since there are conflicting changes made to it.</p> <ul> <li>Let's download the changes made to the remote, but without trying to merge   them directly. This can be done using the following command:</li> </ul> <pre><code>git fetch\n</code></pre> <p>Note</p> <p>The <code>fetch</code> command is very similar to <code>pull</code> in that it downloads remote changes that are not present locally, but differs in that it doesn't try to merge them locally; <code>pull</code> both downloads and merges (unless there's a conflict, in which case it will tell you so and raise an error like the one above). You can thus skip <code>fetch</code> and just do <code>pull</code> straight away, if you prefer.</p> <ul> <li> <p>Now run <code>git status</code>. Unlike before, our local Git clone now is aware of the   latest changes pushed to the remote. It will tell you something along the   lines: \"Your branch and 'origin/main' have diverged, and have 1 and   1 different commit each, respectively.\".</p> </li> <li> <p>We can now run the following to see what the difference is between the current   state of our local clone and the <code>main</code> branch on the remote origin:</p> </li> </ul> <pre><code>git diff origin/main\n</code></pre> <ul> <li>Now let's try to integrate the remote changes with our local changes and get   up to sync with the remote:</li> </ul> <pre><code>git merge\n</code></pre> <p>Unsurprisingly, the <code>git merge</code> command resulted in a conflict. Git tells us about this and suggests that we should fix the conflicts and commit that.</p> <ul> <li>As always, run <code>git status</code> to get an overview: you will see that you have   so-called unmerged paths and that the conflicting file is <code>environment.yml</code>,   since both modified the same line in this file. To fix a conflict, open the   affected file in a text editor. You will see that it now looks something like   this:</li> </ul> <pre><code>channels:\n  - conda-forge\n  - bioconda\n  - main\n  - r\ndependencies:\n  - python=3.9.12\n  - fastqc=0.11.9\n  - sra-tools=2.10.1\n  - snakemake=7.3.8\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\n  - multiqc=1.6\n=======\n  - multiqc=1.8\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; refs/remotes/origin/main\n  - bowtie2=2.4.5\n  - tbb=2020.2\n  - samtools=1.15.1\n  - subread=2.0.1\n  - bedtools=2.29.2\n  - wget=1.20.3\n  - graphviz=3.0.0\n  - r-base=4.1.3\n  - r-ggplot2=3.3.5\n  - r-reshape2=1.4.4\n  - r-stringi=1.7.6\n  - r-pheatmap=1.0.12\n  - r-rmarkdown=2.13\n  - r-r.utils=2.11.0\n  - bioconductor-rtracklayer=1.54.0\n  - bioconductor-geoquery=2.62.0\n  - xorg-libxrender\n  - xorg-libxpm\n</code></pre> <p>The part between <code>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD</code> and <code>=======</code> is your local version, and the part between <code>=======</code> and <code>&gt;&gt;&gt;&gt;&gt;&gt;&gt; refs/remotes/origin/main</code> is the one added to the remote and which caused the conflict when you tried to merge those changes to your local repository. It is now up to you to decide which version to keep, or to change it to a third alternative.</p> <ul> <li>Let's say that you are confident that it is better to run MultiQC 1.6 rather   than 1.8. Edit the file so that it looks like you want it to, i.e. remove   the lines added by Git and delete the line with <code>multiqc=1.8</code>. The final file   should look like this:</li> </ul> <pre><code>channels:\n  - conda-forge\n  - bioconda\n  - main\n  - r\ndependencies:\n  - python=3.9.12\n  - fastqc=0.11.9\n  - sra-tools=2.10.1\n  - snakemake=7.3.8\n  - multiqc=1.6\n  - bowtie2=2.4.5\n  - tbb=2020.2\n  - samtools=1.15.1\n  - subread=2.0.1\n  - bedtools=2.29.2\n  - wget=1.20.3\n  - graphviz=3.0.0\n  - r-base=4.1.3\n  - r-ggplot2=3.3.5\n  - r-reshape2=1.4.4\n  - r-stringi=1.7.6\n  - r-pheatmap=1.0.12\n  - r-rmarkdown=2.13\n  - r-r.utils=2.11.0\n  - bioconductor-rtracklayer=1.54.0\n  - bioconductor-geoquery=2.62.0\n  - xorg-libxrender\n  - xorg-libxpm\n</code></pre> <ul> <li>Run <code>git status</code> again. Notice that it says <code>use \"git add &lt;file&gt;...\" to mark   resolution</code>? Let's do that!</li> </ul> <pre><code>git add environment.yml\n</code></pre> <ul> <li>Run <code>git status</code> again! It will now tell us: <code>All conflicts fixed but you are   still merging. (use \"git commit\" to conclude merge)</code>. So, you probably   guessed it, run:</li> </ul> <pre><code>git commit -m \"Merge and set multiqc to v1.6\"\n</code></pre> <ul> <li>Finally, push these changes to GitHub:</li> </ul> <pre><code>git push\n</code></pre> <ul> <li>Go to GitHub in the browser and click the commit tracker again. You will see   a list of commits including where MultiQC was first changed to version <code>1.7</code>   from our previous work, then to <code>1.8</code>, <code>1.6</code> and, finally, followed by a merge   where the version was set to <code>1.6</code>.</li> </ul> <p>Note</p> <p>While the example we've used here is from a collaborative setting, conflicts also arise when you are working alone. They usually happen when you have several feature branches that you want to merge into <code>main</code> and you've forgot to keep all branches up-to-date with each other.</p> <p>Quick recap</p> <p>We learned about how conflicting commits can happen and how to deal with them by inspecting the affected files and looking for the source of the conflict.</p>"},{"location":"pages/git/git-9-extra-material/","title":"Extra material","text":"<p>The following extra material contains some more advanced things you can do with Git and the command line in general, which is not part of the main course materials. All the essential skills of Git are covered by the previous sections; the material here should be considered tips and tricks from people who use Git every day. You thus don't need to use these things unless you want to, and you can even skip this part of the lesson if you like!</p>"},{"location":"pages/git/git-9-extra-material/#decorating-your-prompt","title":"Decorating your prompt","text":"<p>When you are working on the command line interface (CLI), you will usually have some small pieces of information relating to your current directory, the name of the computer or host you're working on, and so forth. You've probably already seen your prompt while working with Git throughout this lesson, but here's an example of what one might look like:</p> <pre><code>jdainat:~/training-reproducible-research-area/git_tutorial $\n</code></pre> <p>The above prompt contains the name of the computer, a colon, the current working directory, the username and a dollar-sign; it is stored in the variable <code>PS1</code>. You can type <code>echo $PS1</code> to see what variables your prompt is made up of; the above example contains <code>\\h:\\W \\u\\$</code>, where <code>\\h</code> is the hostname, <code>\\W</code> the working directory and <code>\\u</code> the username.</p> <p>Some people like to also show the current branch on their prompt, thus avoiding having to type <code>git branch</code> continuously. There are several ways you might do this, and we're only presenting one of them here: a bash function.</p> <pre><code>git_branch() {\ngit branch 2&gt; /dev/null | sed -e '/^[^*]/d' -e 's/* \\(.*\\)/ (\\1)/'\n}\n</code></pre> <p>This function does a number of things:</p> <ol> <li>Ejects the error message from Git if the current directory isn't a part of a    Git repository into <code>/dev/null</code> (i.e. into nothing).</li> <li>Find the current branch by searching for a line that starts with <code>*</code> (i.e.    the current branch) using the command line program <code>sed</code>.</li> <li>Put the current branch into parentheses with a space before it.</li> </ol> <p>We can then build our new prompt by adding this function into it:</p> <pre><code># The first part of the old prompt\nPS1='\\h:\\W \\u'\n\n# Add the Git branch\nPS1=$PS1'$(git_branch)'\n\n# Add the last part of the old prompt\nPS1=$PS1' \\$'\n</code></pre> <p>Now you should see the current Git branch on your prompt! The only problem now is that this only works for your current session: once you restart your CLI you'll have to re-define your prompt again. This can be circumvented, though. What you need to do is to add the code defining your prompt into your so-called bash profile: <code>~/.bash_profile</code>. Every time you load a new CLI session this file is read and any code inside it is executed. You might already have this file, so make sure you don't overwrite it!</p>"},{"location":"pages/git/git-9-extra-material/#bash-aliases-for-git","title":"Bash aliases for git","text":"<p>Some Git commands are used over and over again when working with git, such as <code>git status</code>. Some people like to have aliases (i.e. shortcuts) for these common commands. Here is a small list of such aliases that you may find useful or, even better, might inspire you to create your own! Add them to your <code>~/.bash_profile</code> as above, so that they're available across sessions.</p> <pre><code># Basic git commands\nalias gb='git branch'\nalias ga='git add'\nalias gd='git diff'\nalias gcm='git commit'\nalias gp='git push'\nalias gu='git pull'\nalias gm='git merge'\nalias gco='git checkout'\nalias gl='git log'\n\n# Git status in short format\nalias gst='git status -s'\n\n# Add and commit all tracked and modified files\nalias gca='git commit -a'\n\n# Create and checkout a new branch\nalias gcob='git checkout -b'\n\n# Git log with one line per commit\nalias glo='git log --oneline'\n</code></pre>"},{"location":"pages/git/git-9-extra-material/#forking","title":"Forking","text":"<p>When you want to work on an Open Source project that is available on e.g. GitHub, you usually don't have permission to directly push code to the project's repository - this is so that the project's maintainers are the only ones that can directly change anything in their codebase. How do you then contribute to projects that don't allow you to push your code to their repository? Simple: use forking!</p> <p>Forking is when you make your own copy of a repository on your GitHub account, which you will then have permissions to change as you see fit. You can then create pull requests from your fork to the original repository, rather than pushing code to a new branch and making a pull request from that. Working with forks just adds an additional step to the whole workflow: instead of being \"clone; code and commit changes on a new branch; push branch to remote; pull request from branch\" it becomes \"fork; clone; code and commit changes; push code to fork; pull request from fork\".</p> <p>You might also want to do a fork of a project simply because you want to have your own copy of it as well, without ever having the intention of changing it. This is, of course, perfectly fine as well, but do keep in mind that developers are usually quite happy to incorporate new changes from contributors if they are reasonable and fulfil a purpose and add functionality to the project. It is quite common that you have a use-case the maintainer didn't think of before, and that you've helped the project grow by contributing your code!</p>"},{"location":"pages/git/git-setup/","title":"Setup","text":""},{"location":"pages/git/git-setup/#install-git","title":"Install git","text":"<p>This tutorial depends on GIT. Take a look at the setup for instructions on how to set it up if you haven't done so already.</p>"},{"location":"pages/git/git-setup/#configure-git","title":"Configure git","text":"<p>If it is the first time you use git on your computer, you may want to configure it so that it is aware of your username and email. These should match those that you have registered on GitHub. This will make it easier when you want to sync local changes with your remote GitHub repository.</p> <pre><code>git config --global user.name \"Mona Lisa\"\ngit config --global user.email \"mona_lisa@gmail.com\"\n</code></pre> <p>Tip</p> <p>If you have several accounts (e.g. both a GitHub and Bitbucket account), and thereby several different usernames, you can configure git on a per-repository level. Change directory into the relevant local git repository and run <code>git config user.name \"Mona Lisa\"</code>. This will set the default username for that repository only.</p> <p>You will also need to configure the default branch name to be <code>main</code> instead of <code>master</code>:</p> <pre><code>git config --global init.defaultBranch \"main\"\n</code></pre> <p>The short version of why you need to do this is that GitHub uses <code>main</code> as the default branch while Git itself is still using <code>master</code>; please read the box below for more information.</p> <p>Note</p> <p>The default branch name for Git and many of the online resources for hosting Git repositories has traditionally been <code>master</code>, which historically comes from the \"master/slave\" repositories of BitKeeper. This has been heavily discussed and in 2020 the decision was made by  many (including GitHub) to start using <code>main</code> instead. Any repository created with GitHub uses this new naming scheme since October of 2020, and Git itself is currently discussing implementing a similar change. Git did, however, introduce the ability to set the default branch name when using <code>git init</code> in version 2.28, instead of using a hard-coded <code>master</code>. We have chosen to use <code>main</code> for this course.</p>"},{"location":"pages/git/git-undoing-and-recovering/","title":"Undoing and recovering","text":"<p>One of the main points of version control is that you can go back in time to recover. Let's have look at safe commands that do not modify the commit history.</p>"},{"location":"pages/git/git-undoing-and-recovering/#undoing-uncommitted-changes","title":"Undoing uncommitted changes","text":""},{"location":"pages/git/git-undoing-and-recovering/#unstaged-file","title":"Unstaged file","text":"<ul> <li>Let's edit <code>config.yml</code> and remove the line  <code>sample_ids: [\"SRR935090\",\"SRR935091\",\"SRR935092\"]</code>. Run <code>git status</code>, it will tell you  that there are modifications in one file (<code>config.yml</code>) compared to the previous commit.</li> </ul> <p>For unstaged file it is possible to have details about the modifications made. <pre><code>git diff config.yml\n</code></pre></p> <p>You now realize that this modification was wrong. Use <code>git restore</code> to remove all  modifications made since last commit. <pre><code>git restore config.yml\ngit status\n</code></pre> As you can see, Git tells us that this file has not been modified since last commit.</p>"},{"location":"pages/git/git-undoing-and-recovering/#staged-file","title":"Staged file","text":"<ul> <li>Let's edit <code>config.yml</code>, remove the line  <code>sample_ids: [\"SRR935090\",\"SRR935091\",\"SRR935092\"]</code>, stage the file and run <code>git status</code>. </li> </ul> <p>Stage and commit the first file  <pre><code>git add config.yml\ngit status\n</code></pre></p> <p>Git tells us that there is a staged file. You now realize that this modification was  wrong. You have to first unstage the file before to clean the modifications:</p> <pre><code>git restore --stage config.yml\ngit status\ngit restore config.yml\ngit status\n</code></pre> <p>Tip</p> <p>In case you want to throws away everything that is not in last commit (HEAD) you can use <code>git reset --hard HEAD</code>. It is apply on all files, staged and unstaged. </p>"},{"location":"pages/git/git-undoing-and-recovering/#undoing-committed-changes","title":"Undoing committed changes","text":"<ul> <li>Let's edit <code>config.yml</code>, remove the line  <code>sample_ids: [\"SRR935090\",\"SRR935091\",\"SRR935092\"]</code>, stage the file, commit, and run  <code>git status</code>.  <pre><code>git add config.yml\ngit commit -m \"remove sample_ids line\"\ngit status\n</code></pre></li> </ul> <p>The <code>git status</code> is not really helpfull here as we already commited our modifications  <pre><code>git log\n</code></pre></p> <p>The <code>git log</code> command show us the message we entered in previous commits.</p> <p>To see modification made in the previous commit you can use: <pre><code>git diff HEAD^\n</code></pre></p> <p>Tip</p> <p>Too see the modifications between the last commit and a particular commit you can use  <code>git diff &lt;commit_id&gt;</code>. It is also possible to compare the differences between two commits: <code>git diff &lt;commit_id1&gt; &lt;commit_id2&gt;</code>.</p> <p>You now realize that the latest commit was a mistake and you wish to undo it: <pre><code>git log --online\ngit revert &lt;commit&gt;\n</code></pre></p> <p>This creates a new commit that does the opposite of the reverted commit. The old commit remains in the history.</p> <p>Note</p> <p>You can revert any commit, no matter how old it is. It doesn\u2019t affect other commits you have done since then - but if they touch the same code, you may get a conflict (which we\u2019ll learn about later).</p> <p>Quick recap</p> <p>We added four important Git commands to our repertoire:</p> <ul> <li><code>git diff &lt;file&gt;</code> list all modifications made on a file since last commit</li> <li><code>git restore &lt;file&gt;</code> cancels the modifications of an unstaged file</li> <li><code>git restore --staged &lt;file&gt;</code> unstage a staged file</li> <li><code>git revert &lt;commit&gt;</code> does the opposite of the reverted commit</li> </ul>"},{"location":"pages/git/license/","title":"A quick word about the licenses...","text":"<p>licensing a repository</p> <p>https://choosealicense.com</p>"},{"location":"pages/introduction/introduction_rr/","title":"Introduction Reproducible Research","text":"<p>Welcome to the tutorials! Here we will learn how to make a computational research project reproducible using several different tools, described in the figure below:</p> <p></p> <p>The figure above gives an overview of the different parts of computational reproducibility (data, code, workflow and environment), as well as the various tools that are used for each part; Git is, arguably, integral to all of the parts, but we only listed it in the code section for a less cluttered figure.</p> <p>The course has a tutorial for each of the tools, all made so that they can be completed independently of each other. It is therefore perfectly possible to go through them in whatever order you prefer, but we suggest the following order:</p> <ol> <li>Git</li> <li>Conda</li> <li>Snakemake</li> <li>Nextflow</li> <li>R Markdown</li> <li>Jupyter</li> <li>Containers</li> </ol> <p>You will find the tutorials in the Modules section in the navigation menu.</p> <p>Please make sure to carefully follow the pre-course setup to install the tools and download the course material before starting with any of the tutorials. These will create quite a lot of files on your computer, some of which will actually take up a bit of storage space too. In order to remove any traces of these after completing the tutorials, please refer to the Take down section.</p> <p>Before going into the tutorials themselves, we first describe the case study from which the example data comes from.</p>"},{"location":"pages/introduction/introduction_rr/#the-case-study","title":"The case study","text":"<p>We will be running a small bioinformatics project as a case study, and use that to exemplify the different steps of setting up a reproducible research project. To give you some context, the study background and analysis steps are briefly described below.</p>"},{"location":"pages/introduction/introduction_rr/#background","title":"Background","text":"<p>The data is taken from Osmundson, Dewell, and Darst (2013), who have studied methicillin-resistant Staphylococcus aureus (MRSA). MRSA is resistant to\u00a0broad spectrum beta-lactam antibiotics and lead to difficult-to-treat\u00a0infections\u00a0in humans. Lytic bacteriophages have been suggested as potential therapeutic agents, or as the source of novel antibiotic proteins or peptides. One such protein, gp67, was identified as a transcription-inhibiting transcription factor with an antimicrobial effect. To identify S. aureus genes repressed by gp67, the authors expressed gp67 in S. aureus cells. RNA-seq was then performed on three S. aureus strains:</p> <ul> <li>RN4220 with pRMC2 with gp67</li> <li>RN4220 with empty pRMC2</li> <li>NCTC8325-4</li> </ul>"},{"location":"pages/introduction/introduction_rr/#analysis","title":"Analysis","text":"<p>The graph below shows the different steps of the analysis that are included in this project:</p> <p></p> <p>The input files are:</p> <ul> <li>RNA-seq raw data (FASTQ files) for the three strains</li> <li>S. aureus genome sequence (a FASTA file)</li> <li>S. aureus genome annotation (a GFF file)</li> </ul> <p>The workflow itself will perform the following tasks:</p> <ul> <li>Downloading and indexing of the reference genome using Bowtie2</li> <li>Downloading the raw FASTQ data from the Sequence Read Archive (SRA)</li> <li>Run some quality controls on the data using FastQC and MultiQC</li> <li>Align the raw data to the genome and calculate the gene expression using   featureCounts</li> <li>Produce supplementary materials using data from quality controls, gene   expression and the workflow figure shown above</li> </ul>"},{"location":"pages/introduction/introduction_southgreen/","title":"Introduction","text":""},{"location":"pages/introduction/introduction_southgreen/#note-taking","title":"Note taking","text":"<p>We have set up a slido workplace where you can ask questions and vote for others' questions at any time. We will try to take the time to answer these questions along the training.</p> <p>The slido for the training is accessible here : https://app.sli.do/event/qD5C3sKAG2Q2bN2JMXsZZA/live/questions</p>"},{"location":"pages/introduction/roundtable/","title":"Roundtable Introduce Yourself","text":""},{"location":"pages/jupyter/jupyter-1-introduction/","title":"Introduction","text":"<p>The Jupyter Notebook is an open-source web application that allows you to create and share documents that contain code, equations, visualizations and text. The functionality is partly overlapping with R Markdown (see the tutorial), in that they both use markdown and code chunks to generate reports that integrate results of computations with the code that generated them. Jupyter Notebook comes from the Python community while R Markdown was developed by RStudio, but you could use most common programming languages in either alternative. In practice though, it's quite common that R developers use Jupyter but probably not very common that Python developers use RStudio. </p> <p>Some reasons to use Jupyter include:</p> <ul> <li>Python is lacking a really good IDE for doing exploratory scientific data   analysis, like RStudio or Matlab. Some people use Jupyter simply as an   alternative for that.</li> <li>The community around Jupyter notebooks is large and dynamic, and there are   lots of tools for sharing, displaying or interacting with notebooks.</li> <li>An early ambition with Jupyter notebooks (and its predecessor IPython   notebooks) was to be analogous to the lab notebook used in a wet lab. It   would allow the data scientist to document his or her day-to-day work and   interweave results, ideas, and hypotheses with the code. From   a reproducibility perspective, this is one of the main advantages.</li> <li>Jupyter notebooks can be used, just like R Markdown, to provide a tighter   connection between your data and your results by integrating results of   computations with the code that generated them. They can also do this in an   interactive way that makes them very appealing for sharing with others.</li> </ul> <p>As always, the best way is to try it out yourself and decide what to use it for!</p> <p>This tutorial depends on files from the course GitHub repo. Take a look at the setup for instructions on how to set it up if you haven't done so already. Then open up a terminal and go to <code>training_reproducible_research/tutorials/jupyter</code> and activate your <code>jupyter-env</code> Conda environment.</p> <p>A note on nomenclature</p> <ul> <li>Jupyter: a project to develop open-source software, open-standards, and services for interactive computing across dozens of programming languages. Lives at jupyter.org.</li> <li>Jupyter Notebook: A web application that you use for creating and managing notebooks. One of the outputs of the Jupyter project.</li> <li>Jupyter notebook: The actual <code>.ipynb</code> file that constitutes your notebook.</li> </ul>"},{"location":"pages/jupyter/jupyter-2-the-basics/","title":"The basics","text":"<p>One thing that sets Jupyter Notebook apart from what you might be used to is that it's a web application, i.e. you edit and run your code from your browser. But first you have to start the Jupyter Notebook server:</p> <pre><code>jupyter notebook --allow-root\n</code></pre> <p>You should see something similar to this printed to your terminal:</p> <pre><code>[I 18:02:26.722 NotebookApp] Serving notebooks from local directory: /Users/john/workshop-reproducible-research/tutorials/jupyter\n[I 18:02:26.723 NotebookApp] 0 active kernels\n[I 18:02:26.723 NotebookApp] The Jupyter Notebook is running at:\n[I 18:02:26.723 NotebookApp] http://localhost:8888/?token=e03f10ccb40efc3c6154358593c410a139b76acf2cae000\n[I 18:02:26.723 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n[C 18:02:26.724 NotebookApp]\n\n    Copy/paste this URL into your browser when you connect for the first time,\n    to login with a token:\n        http://localhost:8888/?token=e03f10ccb40efc3c6154358593c410a139b76acf2cae785c\n[I 18:02:27.209 NotebookApp] Accepting one-time-token-authenticated connection from ::1\n</code></pre> <p>A note for Windows users</p> <p>If you see the error message <code>Start : This command cannot be run due to the error: The system cannot find the file specified. ...</code> then try starting jupyter with <code>jupyter notebook --no-browser</code> then copy the URL given into the browser directly.</p> <p>Jupyter Notebook probably opened up a web browser for you automatically, otherwise go to the address specified in the message in the terminal. Note that the server is running locally (as <code>http://localhost:8888</code>) so this does not require that you have an active internet connection. Also note that it says:</p> <pre><code>Serving notebooks from local directory: &lt;/some/local/path/workshop-reproducible-research/tutorials/jupyter&gt;\n</code></pre> <p>Everything you do in your Notebook session will be stored in this directory, so you won't lose any work if you shut down the server.</p> <p></p> <p>What you're looking at is the Notebook dashboard. This is where you manage your files, notebooks, and kernels. The Files tab shows the files in your directory. The Running tab keeps track of all your processes. The third tab, Clusters, is used for parallel computing and won't be discussed further in this tutorial. Finally, the Nbextensions tab shows a list of configurable notebook extensions that you can use to add functionality to your notebook (as we'll see below).</p> <p>Let's start by creating an empty notebook by selecting the Files tab and clicking New &gt; Python 3. This will open up a new tab or window looking like this:</p> <p></p> <p>Tip</p> <p>If you want to start Jupyter Notebooks on a cluster that you SSH to (e.g. Uppmax) see the section in the Extra material</p> <p>Jupyter notebooks are made up of cells, and you are currently standing in the first cell in your notebook. The fact that it has a green border indicates that it's in \"Edit mode\", so you can write stuff in it. A blue border indicates \"Command mode\" (see below). Cells in Jupyter notebooks can be of two types: markdown or code.</p> <ul> <li>Markdown: These cells contain static material such as captions, text, lists, images and so on. You express this using Markdown, which is a lightweight markup language. Markdown documents can then be converted to other formats for viewing (the document you're reading now is written in Markdown and then converted to HTML). The format is discussed a little more in detail in the R Markdown tutorial. Jupyter Notebook uses a dialect of Markdown called Github Flavored Markdown, which is described here.</li> <li>Code: These are the cells that actually do something, just as code chunks   do in R Markdown. You can write code in dozens of languages and all do all   kinds of clever tricks. You then run the code cell and any output the code   generates, such as text or figures, will be displayed beneath the cell. We   will get back to this in much more detail, but for now it's enough to   understand that code cells are for executing code that is interpreted by   a kernel (in this case the Python version in your Conda environment).</li> </ul> <p>Before we continue, here are some shortcuts that can be useful. Note that they are only applicable when in command mode (blue frames). Most of them are also available from the menus. These shortcuts are also available from the Help menu in your notebook (there's even an option there to edit shortcuts).</p> Shortcut Effect <code>enter</code> Enter Edit mode <code>escape</code> Enter Command mode <code>ctrl</code> - <code>enter</code> Run the cell <code>shift</code> - <code>enter</code> Run the cell and select the cell below <code>alt</code> - <code>enter</code> Run the cell and insert a new cell below <code>s</code> Save the notebook <code>tab</code> For code completion or indentation <code>m</code>, <code>y</code> Toggle between Markdown and Code cells <code>d</code>- <code>d</code> Delete a cell <code>a</code> Insert cells above current cell <code>b</code> Insert cells below current cell <code>x</code> Cut currently selected cells <code>o</code> Toggle output of current cell"},{"location":"pages/jupyter/jupyter-2-the-basics/#writing-markdown","title":"Writing markdown","text":"<p>Let's use our first cell to create a header. Change the format from Code to Markdown using the drop-down list in the Notebook Toolbar, or by pressing the <code>m</code> key when in command mode. Double click on the cell, or hit <code>enter</code> to enter editing mode (green frame) and input \"# My notebook\" (\"#\" is used in Markdown for header 1). Run the cell with <code>ctrl</code>-<code>enter</code>.</p> <p>Tada!</p> <p>Markdown is a simple way to structure your notebook into sections with descriptive notes, lists, links, images etc.</p> <p>Below are some examples of what you can do in markdown. Paste all or parts of it into one or more cells in your notebook to see how it renders. Make sure you set the cell type to Markdown.</p> <pre><code>## Introduction\nIn this notebook I will try out some of the **fantastic** concepts of Jupyter\nNotebooks.\n\n## Markdown basics\nExamples of text attributes are:\n\n* *italics*\n* **bold**\n* `monospace`\n\nSections can be separated by horizontal lines.\n\n---\n\nBlockquotes can be added, for instance to insert a Monty Python quote:\n\n    Spam!\n    Spam!\n    Spam!\n    Spam!\n\nSee [here](https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Working%20With%20Markdown%20Cells.html) for more information.    \n</code></pre>"},{"location":"pages/jupyter/jupyter-2-the-basics/#writing-code","title":"Writing code","text":"<p>Now let's write some code! Since we chose a Python kernel, Python would be the native language to run in a cell. Enter this code in the second cell and run it:</p> <pre><code>print(\"Hello world!\")\n</code></pre> <p>Note how the output is displayed below the cell. This interactive way of working is one of the things that sets Jupyter Notebook apart from RStudio and R Markdown. R Markdown is typically rendered top-to-bottom in one run, while you work in a Jupyter notebook in a different way. This has partly changed with newer versions of RStudio, but it's probably still how most people use the two tools.</p> <p>What is a Jupyter notebook? Let's look a little at the notebook we're currently working in. Jupyter Notebooks are autosaved every minute or so, so you will already have it available. We can be a little meta and do this from within the notebook itself. We do it by running some shell commands in the third code cell instead of Python code. This very handy functionality is possible by prepending the command with <code>!</code>. Try <code>!ls</code> to list the files in the current directory.</p> <p>Aha, we have a new file called <code>Untitled.ipynb</code>! This is our notebook. Look at the first ten lines of the file by using <code>!head Untitled.ipynb</code>. Seems like it's just a plain old JSON file. Since it's a text file it's suitable for version control with for example Git. It turns out that Github and Jupyter notebooks are the best of friends, as we will see more of later. This switching between languages and whatever-works mentality is very prominent within the Jupyter notebook community.</p> <p>Variables defined in cells become variables in the global namespace. You can therefore share information between cells. Try to define a function or variable in one cell and use it in the next. For example:</p> <pre><code>def print_me(str):\n    print(str)\n</code></pre> <p>... and ...</p> <pre><code>print_me(\"Hi!\")\n</code></pre> <p>Your notebook should now look something like this.</p> <p></p> <p>The focus of this tutorial is not on how to write Markdown or Python; you can make really pretty notebooks with Markdown and you can code whatever you want with Python. Rather, we will focus on the Jupyter Notebook features that allow you to do a little more than that.</p> <p>Quick recap</p> <p>In this section we've learned:</p> <ul> <li>That a Jupyter notebook consists of a series of cells, and that they can be either markdown or code cells.</li> <li>That we execute the code in a code cell with the kernel that we chose when opening the notebook.</li> <li>We can run shell commands by prepending them with <code>!</code>.</li> <li>A Jupyter notebook is simply a text file in JSON format.</li> </ul>"},{"location":"pages/jupyter/jupyter-3-magics/","title":"Magics","text":"<p>Magics constitute a simple command language that significantly extends the power of Jupyter notebooks. There are two types of magics:</p> <ul> <li>Line magics: Commands that are prepended by <code>%</code>, and whose arguments only   extend to the end of the line.</li> <li>Cell magics: Commands that start with <code>%%</code> and then applies to the whole   cell. Must be written on the first line of a cell.</li> </ul> <p>Now list all available magics with <code>%lsmagic</code> (which itself is a magic). You add a question mark to a magic to show the help (e.g. <code>%lsmagic?</code>). Some of them act as shortcuts for commonly used shell commands (<code>%ls</code>, <code>%cp</code>, <code>%cat</code>, ..). Others are useful for debugging and optimizing your code (<code>%timeit</code>, <code>%debug</code>, <code>%prun</code>, ..). For more information see the magics documentation.</p> <p>A very useful magic, in particular when using shell commands a lot in your work, is <code>%%capture</code>. This will capture the stdout/stderr of any code cell and store them in a Python object. Run <code>%%capture?</code> to display the help and try to understand how it works. Try it out with either some Python code, other magics or shell commands. Here is an example of how you can make it work:</p> <pre><code>%%capture output\n%%bash\necho \"Print to stdout\"\necho \"Print to stderr\" &gt;&amp;2\n</code></pre> <p>... and in another cell:</p> <pre><code>print(\"stdout:\" + output.stdout)\nprint(\"stderr:\" + output.stderr)\n</code></pre> <p>Tip</p> <p>You can capture the output of some magics directly like this: <code>my_dir = %pwd</code>.</p> <p>The <code>%%script</code> magic is used for specifying a program (bash, perl, ruby, ..) with which to run the code (similar to a shebang). For some languages it's possible to use these shortcuts:</p> <ul> <li><code>%%ruby</code></li> <li><code>%%perl</code></li> <li><code>%%bash</code></li> <li><code>%%html</code></li> <li><code>%%latex</code></li> <li><code>%%R</code> (here you have to first install the rpy2 extension, for example with   Conda, and then load with <code>%load_ext rpy2.ipython</code>)</li> </ul> <p>Try this out if you know any of the languages above. Otherwise you can always try to print the quadratic formula with LaTeX!</p> <pre><code>\\begin{align}\n(a+b)^3 &amp;= (a+b)(a+b)^2 \\\\\n        &amp;= (a+b)(a^2+2ab+b^2) \\\\\n        &amp;= a^3+3a^2b+3ab^2+b^3\n\\end{align}\n</code></pre> <p>Another useful magic is <code>%precision</code> which sets the floating point precision in the notebook. As a quick example, add the following to a cell and run it:</p> <pre><code>float(100/3)\n</code></pre> <p>Next set the precision to 4 decimal points by running a cell with:</p> <pre><code>%precision 4\n</code></pre> <p>Now run the cell with <code>float(100/3)</code> again to see the difference.</p> <p>Running <code>%precision</code> without additional arguments will restore the default.</p> <p>Quick recap</p> <p>In this section we've learned:</p> <ul> <li>The basics of Jupyter magics and the difference between line magics and cell magics</li> <li>How to capture and use output from notebook cells with <code>%%capture</code></li> <li>How to use magics to run non-Python code in notebooks</li> </ul>"},{"location":"pages/jupyter/jupyter-4-plotting/","title":"Plotting","text":""},{"location":"pages/jupyter/jupyter-4-plotting/#matplotlib","title":"Matplotlib","text":"<p>An essential feature of Jupyter Notebooks is of course the ability to visualize data and results via plots. A full guide to plotting in Python is beyond the scope of this course, but we'll offer a few glimpses into the plotting landscape of Python.</p> <p>First of all, Python has a library for plotting called matplotlib, which comes packed with functionality for creating high-quality plots. Below is an example of how to generate a line plot of a sine wave.</p> <pre><code># Import packages\nimport numpy as np\nimport matplotlib.pyplot as plt\n# Generate a set of evenly spaced numbers between 0 and 100\nx = np.linspace(0,3*np.pi,100)\n# Use the sine function to generate y-values\ny = np.sin(x)\n# Plot the data\nline, = plt.plot(x, y, color='red', linestyle=\"-\")\n</code></pre> <p>By default plots are rendered in the notebook as rasterized images which can make the quality poor. To render in scalable vector graphics format use the <code>set_matplotlib_formats</code> from the matplotlib_inline package:</p> <pre><code>import matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats('pdf', 'svg')\n</code></pre> <p>Now try running the code for the sine wave plot again.</p>"},{"location":"pages/jupyter/jupyter-4-plotting/#other-packages-for-plotting","title":"Other packages for plotting","text":""},{"location":"pages/jupyter/jupyter-4-plotting/#seaborn","title":"Seaborn","text":"<p>As we mentioned Matplotlib comes with a lot of functionality which is great because it allows you to create all sorts of plots and modify them exactly to your liking. However, this can also mean that creating very basic plots might involve a lot of cumbersome coding, when all you want is a simple bar chart!</p> <p>Fortunately there are a number of Python packages that build upon matplotlib but with a much simplified interface. One such popular package is seaborn. Below we'll see how to generate a nice looking bar plot with error bars.</p> <p>First import the seaborn package (using an abbreviated name to simplify typing):</p> <pre><code>import seaborn as sns\n</code></pre> <p>Next we'll load some example data of penguins collected at the Palmer Station, in Antarctica.</p> <pre><code>penguins = sns.load_dataset(\"penguins\")\n# Look at first 5 lines of the data\npenguins.head(5)\n</code></pre> <p>The most basic way to generate a bar plot of this data with seaborn is:</p> <pre><code>sns.barplot(data=penguins)\n</code></pre> <p>Simple right? Yes, but maybe not very informative. Here seaborn simply calculates the mean of all numeric variables for the penguins and plots them with error bars representing a 95% confidence interval.</p> <p>Let's say that instead we want to plot the mean value of the body mass of the penguins at the different islands where they were examined.</p> <pre><code>sns.barplot(data=penguins, x=\"island\", y=\"body_mass_g\", ci=\"sd\", errwidth=.5);\n</code></pre> <p>Here we specified to use values in the 'island' column as categories for the x-axis, and values in the 'body_mass_g' column as values for the y-axis. The barplot function of seaborn will then calculate the mean body mass for each island and plot the bars. With <code>ci=\"sd\"</code> we tell the function to draw the standard deviation as error bars, instead of computing a confidence interval. Finally <code>errwidth=.5</code> sets the linewidth of the error bars.</p> <p>If we instead want to visualize the data as a scatterplot we can use the <code>sns.scatterplot</code> function. Let's plot the body mass vs. bill length for all penguins and color the data points by species. We'll also move the legend outside of the plotting area and modify the x and y-axis labels:</p> <pre><code># Store the matplotlib axes containing the plot in a variable called 'ax'\nax = sns.scatterplot(data=penguins, x=\"bill_length_mm\", y=\"body_mass_g\",\n                     hue=\"species\")\n# Modify the labels of the plot\nax.set_xlabel(\"Bill length (mm)\")\nax.set_ylabel(\"Body mass (g)\")\n# Set legend position outside of plot\nax.legend(bbox_to_anchor=(1,1));\n</code></pre> <p>If you want to save a plot to file you can use the <code>plt.savefig</code> function. Add the following to the bottom of the cell with the scatterplot code:</p> <pre><code>plt.savefig(\"scatterplot.pdf\", bbox_inches=\"tight\")\n</code></pre> <p>The <code>bbox_inches=\"tight\"</code> setting ensures that the figure is not clipped when saved to file.</p> <p>The Seaborn website contains great tutorials and examples of other ways to plot data!</p>"},{"location":"pages/jupyter/jupyter-4-plotting/#plotly","title":"Plotly","text":"<p>Another package? Do you know plotly? The plotly Python library is a versatile and user-friendly plotting tool that offers a rich selection of over 40 chart types, including statistical, financial, geographic, scientific, and 3-dimensional options, enabling users to create interactive and engaging visualizations. It is also an open-source library, making it accessible to anyone who wants to use it.</p> <p>What's the most important thing about it compared to the others? Its dynamic side! It is possible to zoom in, to hover over a point to obtain information, etc.  </p> <p>A classic scatter plot</p> <pre><code>from plotly.offline import init_notebook_mode, iplot\nimport plotly.express as px\n\ninit_notebook_mode(connected=True)   # initiate notebook for offline plot\n\nfig = px.scatter(x=[0, 1, 2, 3, 4], y=[0, 1, 4, 9, 16])\nfig.show()\n</code></pre> <p>A bubble plot</p> <pre><code>df = px.data.gapminder()\n\nfig = px.scatter(df.query(\"year==2007\"), x=\"gdpPercap\", y=\"lifeExp\",\n             size=\"pop\", color=\"continent\",\n                 hover_name=\"country\", log_x=True, size_max=60)\nfig.show()\n</code></pre> <p>The Plotly website contains great tutorials and examples of other ways to plot data! And it's also available for R, JS, Julia and MATLAB\u00ae.</p> <p>Quick recap</p> <p>In this section we've learned:</p> <ul> <li>How to generate simple plots with <code>matplotlib</code></li> <li>How to import and use the <code>seaborn</code> package for plotting</li> <li>How to import and use the <code>plotly</code> package for plotting</li> <li>How to save plots from notebooks to a file</li> </ul>"},{"location":"pages/jupyter/jupyter-5-widgets/","title":"Widgets","text":"<p>Since we're typically running our notebooks in a web browser, they are quite well suited for also including more interactive elements. A typical use case could be that you want to communicate some results to a collaborator or to a wider audience, and that you would like them to be able to modify how the results are displayed. It could, for example, be to select which gene to plot for, or to see how some parameter value affects a clustering. Jupyter notebooks has great support for this in the form of widgets.</p> <p>Widgets are eventful Python objects that have a representation in the browser, often as a control like a slider, textbox, etc. These are implemented in the <code>ipywidgets</code> package.</p> <p>The easiest way to get started with using widgets are via the <code>interact</code> and <code>interactive</code> functions. These functions autogenerate widgets from functions that you define, and then call those functions when you manipulate the widgets. Too abstract? Let's put it into practice!</p> <p>Let's try to add sliders that allow us to change the frequency, amplitude and phase of the sine curve we plotted previously.</p> <pre><code># Import the interactive function from ipywidgets\nfrom ipywidgets import interactive\n# Also import numpy (for calculating the sine curve)\n# and pyplot from matplotlib for plotting\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the function for plotting the sine curve\ndef sine_curve(A, f, p):\n    # Set up the plot\n    plt.figure(1, figsize=(4,4))\n    # Create a range of 100 evenly spaced numbers between 0 and 100\n    x = np.linspace(0,10,100)\n    # Calculate the y values using the supplied parameters\n    y = A*np.sin(x*f+p)\n    # Plot the x and y values ('r-' specifies color and line style)\n    plt.plot(x, y, color='red', linestyle=\"-\")\n    plt.show()\n\n# Here we supply the sine_curve function to interactive,\n# and set some limits on the input parameters\ninteractive_plot = interactive(sine_curve,\n            A=(1, 5, 1),\n            f=(0, 5, 1),\n            p=(1, 5, 0.5))\n\n# Display the widgets and the plot\ninteractive_plot\n</code></pre> <p>The code above defines a function called <code>sine_curve</code> which takes three arguments:</p> <ul> <li><code>A</code> = the amplitude of the curve</li> <li><code>f</code> = the frequency of the curve</li> <li><code>p</code> = the phase of the curve</li> </ul> <p>The function creates a plot area, generates x-values and calculates y-values using the <code>np.sin</code> function and the supplied parameters. Finally, the x and y values are plotted.</p> <p>Below the function definition we use <code>interactive</code> with the <code>sine_curve</code> function as the first parameter. This means that the widgets will be tied to the <code>sine_curve</code> function. As you can see we also supply the <code>A</code>, <code>f</code> and <code>p</code> keyword arguments. Importantly, all parameters defined in the <code>sine_curve</code> function must be given in the <code>interactive</code> call and a widget is created for each one.</p> <p>Depending on the <code>type</code> of the passed argument different types of widgets will be created by <code>interactive</code>. For instance:</p> <ul> <li><code>int</code> or <code>float</code> arguments will generate a slider</li> <li><code>bool</code> arguments (True/False) will generate checkbox widgets</li> <li><code>list</code> arguments will generate a dropdown</li> <li><code>str</code> arguments will generate a text-box</li> </ul> <p>By supplying the arguments in the form of tuples we can adjust the properties of the sliders. <code>f=(1, 5, 1)</code> creates a widget with minimum value of <code>1</code>, maximum value of <code>5</code> and a step size of <code>1</code>. Try adjusting these numbers in the <code>interactive</code> call to see how the sliders change (you have to re-execute the cell).</p> <p>The final line of the cell (<code>interactive_plot</code>) is where the actual widgets and plot are displayed. This code can be put in a separate cell, so that you can define functions and widgets in one part of your notebook, and reuse them somewhere else.</p> <p>This is how it should look if everything works. You can now set the frequency amplitude and phase of the sine curve by moving the sliders.</p> <p></p> <p>There are lots of widgets, e.g.:</p> <ul> <li>Dropdown menus</li> <li>Toggle buttons</li> <li>Range sliders</li> <li>File uploader</li> </ul> <p>... and much, much more. Here is a list of all available widgets together with documentation and examples. Some of these widgets cannot be autogenerated by <code>interactive</code>, but fear not! Instead of relying on autogeneration we can define the widget and supply it directly to <code>interactive</code>.</p> <p>To see this in practice, change out the <code>A</code> argument to a pre-defined <code>IntSlider</code> widget. First define the slider:</p> <pre><code>from ipywidgets import widgets\nA = widgets.IntSlider(value=2, min=1, max=5, step=1)\n</code></pre> <p>Then replace the call to <code>interactive</code> so that it looks like this:</p> <pre><code>interactive_plot = interactive(sine_curve, A=A, f=5, p=5)\n</code></pre>"},{"location":"pages/jupyter/jupyter-5-widgets/#extra-challenge","title":"Extra challenge","text":"<p>If you can't get enough of widgets you might want to try this out: see if you can figure out how to add a widget that lets you pick the color for the sine curve line. Search for the appropriate widget in the Widget list. You'll need to update the <code>sine_curve</code> function and pass the new widget as an argument in the call to <code>interactive</code>. If you need help, see the code chunk below:</p> Click to show the solution <pre><code># Import the interactive function from ipywidgets\nfrom ipywidgets import interactive\n# Also import numpy (for calculating the sine curve)\n# and pyplot from matplotlib for plotting\nimport numpy as np\nfrom ipywidgets import widgets ## &lt;- import widgets\nimport matplotlib.pyplot as plt\n\n# Define the function for plotting the sine curve\ndef sine_curve(A, f, p, color): ## &lt;- add parameter here\n    # Set up the plot\n    plt.figure(1, figsize=(4,4))\n    # Create a range of 100 evenly spaced numbers between 0 and 100\n    x = np.linspace(0,10,100)\n    # Calculate the y values using the supplied parameters\n    y = A*np.sin(x*f+p)\n    # Plot the x and y values\n    plt.plot(x, y, color=color) ## &lt;- Use color from widget here\n    plt.show()\n\n# Here we supply the sine_curve function to interactive,\n# and set some limits on the input parameters\n# Define the colorpicker widget\ncolorpicker = widgets.ColorPicker(description='color',value=\"red\")\ninteractive_plot = interactive(sine_curve,\n            A=(1, 5, 1),\n            f=(0, 5, 1),\n            p=(1, 5, 0.5),\n            color=colorpicker) ## &lt;- Supply the colorpicker to the function\n\n# Display the widgets and the plot\ninteractive_plot\n</code></pre> <p>Warning</p> <p>Note that you may have to close the color picker once you've made your choice in order to make the plot update.</p>"},{"location":"pages/jupyter/jupyter-5-widgets/#other-interactive-plots","title":"Other interactive plots","text":"<p>Jupyter widgets, like we used here, is the most vanilla way of getting interactive graphs in Jupyter notebooks. Some other alternatives are:</p> <ul> <li>Plotly is actually an   API to a web service that renders your graph and returns it for display in   your Jupyter notebook. Generates very visually appealing graphs, but from   a reproducibility perspective it's maybe not a good idea to be so reliant on   a third party.</li> <li>Bokeh   is another popular tool for interactive graphs. Most plotting packages for   Python are built on top of matplotlib, but Bokeh has its own library. This   can give a steeper learning curve if you're used to the standard packages.</li> <li>mpld3 tries to integrate matplotlib with   Javascript and the D3js package. It doesn't scale well for very large   datasets, but it's easy to use and works quite seamlessly.</li> </ul> <p>Quick recap</p> <p>In the three previous sections we've learned:</p> <ul> <li>How to implement interactive widgets in notebooks</li> </ul>"},{"location":"pages/jupyter/jupyter-6-extensions/","title":"Extensions","text":"<p>Jupyter Notebook extensions are add-ons that can increase the functionality of your notebooks. These were installed in the setup section for this tutorial by including the <code>jupyter_contrib_nbextensions</code> package in the conda environment file. You can read more about the extensions here.</p> <p>To manage extensions go to the Jupyter dashboard in your browser and click the Nbextensions tab. You should see something similar to this:</p> <p></p> <p>Clicking an extension in the list displays more information about it. To enable/disable extensions simply click the checkbox next to the extension name in the list. Some useful extensions include</p> <ul> <li> <p>Hide input all, which allows you to hide all code cells with the click of   a button.</p> </li> <li> <p>Collapsible Headings, which allows you to collapse sections below markdown   headings to increase readability.</p> </li> <li> <p>Table of Contents (2), which adds a table of contents to the notebook   making navigation a lot quicker especially for long notebooks.</p> </li> </ul> <p>Feel free to peruse the list and find your own favourites! Keep in mind that these are unofficial, community-contributed extensions and as such they come with few, if any, guarantees.</p> <p>Quick recap</p> <p>In this section we've learned:</p> <ul> <li>What Jupyter extensions are and how to enable/disable them</li> </ul>"},{"location":"pages/jupyter/jupyter-7-converting-notebooks/","title":"Converting notebooks","text":"<p>Notebooks can be converted to various output formats such as HTML, PDF, LaTeX etc. directly from the File -&gt; Download as menu.</p> <p>Conversion can also be performed on the command line using the <code>jupyter nbconvert</code> command. <code>nbconvert</code> is installed together with the <code>jupyter</code> Conda package and is executed on the command line by running <code>jupyter nbconvert</code>.</p> <p>The syntax for converting a Jupyter notebook is:</p> <pre><code>jupyter nbconvert --to &lt;FORMAT&gt; notebook.ipynb\n</code></pre> <p>Here <code>&lt;FORMAT&gt;</code> can be any of <code>asciidoc</code>, <code>custom</code>, <code>html</code>, <code>latex</code>, <code>markdown</code>, <code>notebook</code>, <code>pdf</code>, <code>python</code>, <code>rst</code>, <code>script</code>, <code>slides</code>. Converting to some output formats (e.g. PDF) may require you to install separate software such as Pandoc or a TeX environment.</p> <p>Try converting the <code>Untitled.ipynb</code> notebook that you have been working on so far to HTML using <code>jupyter nbconvert</code>.</p> <p>Tip</p> <p>To export notebooks in the form they appear with Jupyter Extensions activated you can make use of the <code>nbextensions</code> template that is installed with the <code>jupyter_contrib_nbextensions</code> package. Adding <code>--template=nbextensions</code> to the <code>jupyter nbconvert</code> call should do the trick, but note that not all extensions are guaranteed to display right after exporting.</p> <p><code>nbconvert</code> can also be used to run a Jupyter notebook from the command line by running:</p> <pre><code>jupyter nbconvert --execute --to &lt;FORMAT&gt; notebook.ipynb\n</code></pre> <p><code>nbconvert</code> executes the cells in a notebook, captures the output and saves the results in a new file. Try running it on the <code>Untitled.ipynb</code> notebook.</p> <p>You can also specify a different output file with <code>--output &lt;filename&gt;</code>.</p> <p>So in order to execute your <code>Untitled.ipynb</code> notebook and save it to a file named <code>report.html</code> you could run:</p> <pre><code>jupyter nbconvert --to html --output report.html --execute Untitled.ipynb\n</code></pre> <p>Quick recap</p> <p>In this section we've learned:</p> <ul> <li>How to convert Jupyter notebooks to various other formats</li> <li>How to use <code>nbconvert</code> to convert notebooks on the command line</li> </ul>"},{"location":"pages/jupyter/jupyter-8-the-mrsa-case-study/","title":"The MRSA case study","text":"<p>As you might remember from the intro, we are attempting to understand how lytic bacteriophages can be used as a future therapy for the multiresistant bacteria MRSA (methicillin-resistant Staphylococcus aureus). We have already seen how to define the project environment in the Conda tutorial and how to set up the workflow in the Snakemake tutorial. Here we explore the results from the Snakemake tutorial and generate a Supplementary Material file with some basic stats.</p> <p>In the <code>training-reproducible-research/tutorials/jupyter/</code> directory you will find a notebook called <code>supplementary_material.ipynb</code>. Open this notebook with Jupyter by running:</p> <pre><code>jupyter notebook supplementary_material.ipynb\n</code></pre> <p>Tip</p> <p>Using what you've learned about markdown in notebooks, add headers and descriptive text to subdivide sections as you add them. This will help you train how to structure and keep note of your work with a notebook.</p> <p>You will see that the notebook contains only a little markdown text and a code cell with a function <code>get_geodata</code>. We'll start by adding a cell with some import statements.</p> <p>Run the cell with the <code>get_geodata</code> function and add a new cell directly after it. Then add the following to the new cell:</p> <pre><code>import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n</code></pre> <p>This imports the <code>pandas</code> (for working with tables), <code>seaborn</code> and <code>matplotlib.pyplot</code> (for plotting) and <code>numpy</code> (for numerical operations) Python modules.</p> <p>Also add:</p> <pre><code>import matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats('pdf', 'svg')\n</code></pre> <p>to set high-quality output for plots.</p> <p>Run the cell and create a new one below it.</p> <p>In the next cell we'll define some parameters to use for the notebook:</p> <pre><code>counts_file=\"results/tables/counts.tsv\"\nsummary_file=\"results/tables/counts.tsv.summary\"\nmultiqc_file=\"intermediate/multiqc_general_stats.txt\"\nrulegraph_file=\"results/rulegraph.png\"\nSRR_IDs=[\"SRR935090\",\"SRR935091\",\"SRR935092\"]\nGSM_IDs=[\"GSM1186459\",\"GSM1186460\",\"GSM1186461\"]\nGEO_ID=\"GSE48896\"\n</code></pre> <p>As you can see we add paths to results files and define lists with some sample IDS. Run this cell and add a new one below it.</p> <p>Next, we'll fetch some sample information from NCBI using the <code>get_geodata</code> function defined at the start of the notebook and collate it into a dataframe.</p> <pre><code>id_df = pd.DataFrame(data=GSM_IDs, index=SRR_IDs, columns=[\"geo_accession\"])\ngeo_df = get_geodata(GEO_ID)\nname_df = pd.merge(id_df, geo_df, left_on=\"geo_accession\", right_index=True)\n# Create a dictionary to rename sample ids in downstream plots\nname_dict = name_df.to_dict()\n</code></pre> <p>Run the cell and take a look at the contents of the <code>name_df</code> dataframe (e.g. run a cell with that variable only to output it below the cell).</p> <p>Now we'll load some statistics from the QC part of the workflow, specifically the 'general_stats' file from <code>multiqc</code>. Add the following to a new cell and run it:</p> <pre><code>qc = pd.read_csv(multiqc_file, sep=\"\\t\")\nqc.rename(columns=lambda x: x.replace(\"FastQC_mqc-generalstats-fastqc-\", \"\").replace(\"_\", \" \"), inplace=True)\nqc = pd.merge(qc, name_df, left_on=\"Sample\", right_index=True)\nqc\n</code></pre> <p>In the code above we load the multiqc file, rename the columns by stripping the <code>FastQC_mqc-generalstats-fastqc-</code> part from column names and replace underscores with spaces. Finally the table is merged with the information obtained in the step above and output to show summary statistics from the QC stage.</p> <p>Next it's time to start loading gene count results from the workflow. Start by reading the counts and summary results, then edit the columns and index:</p> <pre><code># Read count data\ncounts = pd.read_csv(counts_file, sep=\"\\t\", header=0, comment=\"#\", index_col=0)\n# Read summary data\ncounts_summary = pd.read_csv(summary_file, sep=\"\\t\", index_col=0)\n# Rename columns to extract SRR ids\ncounts.rename(columns = lambda x: x.split(\"/\")[-1].replace(\".sorted.bam\",\"\"), inplace=True)\ncounts_summary.rename(columns = lambda x: x.split(\"/\")[-1].replace(\".sorted.bam\",\"\"), inplace=True)\n</code></pre> <p>Take a look at the <code>counts</code> dataframe to get an idea of the data structure. As you can see the dataframe shows genes as rows while the columns shows various information such as start and stop, strand and length of the genes. The last three columns contain counts of the genes in each of the samples.</p> <p>If you have a look at the <code>counts_summary</code> dataframe you will see statistics from the read assignment step, showing number of reads that could be properly assigned as well as number of reads that could not be assigned to genes for various reasons.</p> <p>Now let's generate a barplot of the summary statistics. Before we plot, we'll remove rows that have only zero values:</p> <pre><code># Remove rows with only zero values\nsummary_plot_data = counts_summary.loc[counts_summary.sum(axis=1)&gt;0]\n</code></pre> <p>Now for the plotting:</p> <pre><code># Set color palette to 'Set2'\ncolors = sns.color_palette(\"Set2\")\n# Create a stacked barplot\nax = summary_plot_data.T.plot(kind=\"bar\", stacked=True, color=colors)\n# Move legend and set legend title\nax.legend(bbox_to_anchor=(1,1), title=\"Category\");\n</code></pre> <p>The final plot will be a heatmap of gene counts for a subset of the genes. We'll select genes whose standard deviation/mean count across samples is greater than 1.5, and have a maximum of at least 5 reads in 1 or more sample:</p> <pre><code># Slice the dataframe to only sample counts\ncount_data = counts.loc[:, SRR_IDs]\n# Filter to genes with std/mean &gt; 1.2 and with a max of at least 5\nheatmap_data = count_data.loc[(count_data.std(axis=1).div(count_data.mean(axis=1))&gt;1.2)&amp;(count_data.max(axis=1)&gt;5)]\n</code></pre> <p>We'll also replace the SRR ids with the title of samples used in the study, using the <code>name_dict</code> dictionary created further up in the notebook:</p> <pre><code>heatmap_data = heatmap_data.rename(columns = name_dict['title'])\n</code></pre> <p>Now let's plot the heatmap. We'll log-transform the counts, set color scale to Blue-Yellow-Red and cluster both samples and genes using 'complete' linkage clustering:</p> <pre><code>with sns.plotting_context(\"notebook\", font_scale=0.7):\n    ax = sns.clustermap(data=np.log10(heatmap_data+1), cmap=\"RdYlBu_r\",\n                        method=\"complete\", yticklabels=True, linewidth=.5,\n                        cbar_pos=(.7, .85, .05, .1), figsize=(3,9))\n    plt.setp(ax.ax_heatmap.get_xticklabels(), rotation=270)\n</code></pre> <p>In the code above we use the seaborn <code>plotting_context</code> function to scale all text elements of the heatmap in one go.</p> <p>As a final step we'll add some info for reproducibility under the Reproducibility section. To add the overview image of the workflow found in <code>results/rulegraph.png</code> we can use the <code>Image</code> function from <code>IPython.display</code>:</p> <pre><code>from IPython.display import Image\nImage(rulegraph_file)\n</code></pre> <p>Let's also output the full conda environment so that all packages and versions are included in the notebook. There are several ways this can be done, for example you could simply add:</p> <pre><code>!conda list\n</code></pre> <p>to the end of the notebook.</p> <p>Tip</p> <p>If you want to know more about how notebooks can be integrated into Snakemake worfklows, see the Extra material at the end of this tutorial</p>"},{"location":"pages/jupyter/jupyter-8-the-mrsa-case-study/#sharing-your-work","title":"Sharing your work","text":"<p>The files you're working with come from a GitHub repo. Both GitHub and Bitbucket can render Jupyter notebooks as well as other types of Markdown documents. Now go to our GitHub repo at https://github.com/SouthGreenPlatform/training_reproducible_research and navigate to <code>tutorials/jupyter/supplementary_material.ipynb</code>.</p> <p></p> <p>As you can imagine, having this very effortless way of sharing results can greatly increase the visibility of your work. You work as normal on your project, and push regularly to the repository as you would anyways, and the output is automatically available for anyone to see. Or for a select few if you're not ready to share your findings with the world quite yet.</p> <p>Say your notebook isn't on Github/Bitbucket. All hope isn't lost there. Jupyter.org provides a neat functionality called nbviewer, where you can paste a URL to any notebook and they will render it for you. Go to https://nbviewer.jupyter.org and try this out with our notebook.</p> <pre><code>https://raw.githubusercontent.com/NBISweden/workshop-reproducible-research/main/tutorials/jupyter/supplementary_material.ipynb\n</code></pre>"},{"location":"pages/jupyter/jupyter-8-the-mrsa-case-study/#shared-interactive-notebooks","title":"Shared interactive notebooks","text":"<p>So far we've only shared static representations of notebooks. A strong trend at the moment is to run your notebooks in the cloud, so that the person you want to share with could actually execute and modify your code. This is a great way of increasing visibility and letting collaborators or readers get more hands-on with your data and analyses. From a reproducibility perspective, there are both advantages and drawbacks. On the plus side is that running your work remotely forces you to be strict when it comes to defining the environment it uses (probably in the form of a Conda environment or Docker image). On the negative side is that you become reliant on a third-party service that might change input formats, go out of business, or change payment model.</p> <p>Here we will try out a service called Binder, which lets you run and share Jupyter Notebooks in Git repositories for free. There are a number of example repositories that are setup to be used with Binder. Navigate to https://github.com/binder-examples/conda/ to see one such example. As you can see the repository contains a LICENSE file, a README, an environment file and a notebook. To use a repository with Binder the environment file should contain all the packages needed to run notebooks in the repo. So let's try to run the <code>index.ipynb</code> file using Binder:</p> <p>Just go to https://mybinder.org and paste the link to the GitHub repo. Note the link that you can use to share your notebook. Then press \"launch\".</p> <p></p> <p>What will happen now is that:</p> <ul> <li>Binder detects the <code>environment.yml</code> file in the root of the repo.   Binder then builds a Docker image based on the file. This might take   a minute or two. You can follow the progress in the build log.</li> <li>Binder then launches the Jupyter Notebook server in the Docker   container..</li> <li>..and opens a browser tab with it for you.</li> </ul> <p>Once the process is finished you will be presented with a Jupyter server overview of the contents in the repository. Click on the <code>index.ipynb</code> notebook to open it. Tada! You are now able to interact with (and modify) someone else's notebook online.</p> <p>Applied to your own projects you now have a way to run analyses in the cloud and in an environment that you define yourself. All that's needed for someone to replicate your analyses is that you share a link with them. Note that notebooks on Binder are read-only; its purpose is for trying out and showing existing notebooks rather than making new ones.</p> <p>Tip</p> <p>By default Binder looks for configuration files such as environment.yml in the root of the repository being built. But you may also put such files outside the root by making a <code>binder/</code> folder in the root and placing the file there.  </p> <p>A note on transparency</p> <p>Resources like Github/Bitbucket and Jupyter Notebooks have changed the way we do scientific research by encouraging visibility, social interaction and transparency. It was not long ago that the analysis scripts and workflows in a lab were well-guarded secrets that we only most reluctantly shared with others. Assuming that it was even possible. In most cases, the only postdoc who knew how to get it to work had left for a new position in industry, or no one could remember the password to the file server. If you're a PhD student, we encourage you to embrace this new development wholeheartedly, for it will make your research better and make you into a better scientist. And you will have more fun.</p> <p>Quick recap</p> <p>In this section we've learned:</p> <ul> <li>How notebooks can be used to generate summary statistics and plots using the results of a workflow run</li> <li>How to share notebooks via nbviewer and Binder</li> </ul>"},{"location":"pages/jupyter/jupyter-9-extra-material/","title":"Extra material","text":"<p>Here are some useful resources if you want to read more about Jupyter in general:</p> <ul> <li>The Jupyter project site contains a lot of information   and inspiration.</li> <li>The Jupyter Notebook documentation.</li> <li>A guide to using   widgets for creating interactive notebooks.</li> </ul>"},{"location":"pages/jupyter/jupyter-9-extra-material/#running-jupyter-notebooks-on-a-cluster","title":"Running jupyter notebooks on a cluster","text":"<ul> <li>Login to Uppmax, making sure to use a specific login node, e.g. <code>rackham1</code>:</li> </ul> <pre><code>ssh &lt;your-user-name&gt;@rackham1.uppmax.uu.se\n</code></pre> <ul> <li>Create/activate a conda environment containing <code>jupyter</code> then run:</li> </ul> <pre><code>jupyter notebook\n</code></pre> <p>When the Jupyter server starts up you should see something resembling: <pre><code>[I 11:00:00.000 NotebookApp] Serving notebooks from local directory: &lt;path-to-your-local-dir&gt;\n[I 11:00:00.000 NotebookApp] Jupyter Notebook 6.4.6 is running at:\n[I 11:00:00.000 NotebookApp] http://localhost:8889/?token=357d65100058efa40a0641fce7005addcff339876c5e8000\n[I 11:00:00.000 NotebookApp]  or http://127.0.0.1:8889/?token=357d65100058efa40a0641fce7005addcff339876c5e8000\n[I 11:00:00.000 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n</code></pre></p> <p>Now a Jupyter notebook server is running on the Uppmax end. The line that says: <pre><code>[I 11:00:00.000 NotebookApp] http://localhost:8889/?token=357d65100058efa40a0641fce7005addcff339876c5e8000\n</code></pre></p> <p>contains information on the port used on the server side (8889 in this case) and the token required to use the server (<code>357d65100058efa40a0641fce7005addcff339876c5e8000</code>).</p> <p>Next step is to use this information to login to the server from your local computer.  </p> <p>On your local computer</p> <p>In a terminal, run the following command to start port forwarding of port 8080 on your local computer to the remote port on the Uppmax side. Replace  with the port given when you started the server on Uppmax. Also replace  with your user name on Uppmax. <pre><code>ssh -N -L localhost:8080:localhost:&lt;remote-port&gt; &lt;your-user-name&gt;@rackham1.uppmax.uu.se\n</code></pre> <p>As long as this process is running the port forwarding is running. To disable it simply interrupt it with <code>CTRL + C</code>.</p> <p>Connect to the jupyter server by opening <code>localhost:8080</code> in your browser. When prompted, paste the token you got when starting the server on Uppmax.</p> <p>You are now (hopefully) accessing the jupyter server that's running on Uppmax, via your local browser.</p>"},{"location":"pages/jupyter/jupyter-9-extra-material/#integrating-notebooks-with-snakemake-workflows","title":"Integrating notebooks with Snakemake workflows","text":"<p>In the case study section of this tutorial we created a Jupyter notebook that used output from a Snakemake workflow and produced some summary results and plots. Wouldn't it be nice if this was actually part of the workflow itself? To generate a HTML version of the notebook we can use what we learned in the section about converting notebooks. The command to execute the notebook and save it in HTML format in a file <code>results/supplementary.html</code> would be:</p> <pre><code>jupyter nbconvert --to HTML --output-dir results --output supplementary.html --execute supplementary_material.ipynb\n</code></pre> <p>This command could be used in a rule, e.g. <code>make_supplementary</code>, the input of which would be <code>results/tables/counts.tsv</code>, <code>intermediate/multiqc_general_stats.txt</code>, and <code>results/rulegraph.png</code>. See if you can work out how to implement such a rule at the end of the <code>Snakefile</code> found in the <code>jupyter/</code> directory. You can find an example in the code chunk below:</p> <pre><code>rule make_supplementary:\n    input:\n        counts = \"results/tables/counts.tsv\",\n        summary = \"results/tables/counts.tsv.summary\",\n        multiqc_file = \"intermediate/multiqc_general_stats.txt\",\n        rulegraph = \"results/rulegraph.png\"\n    output:\n        \"results/supplementary.html\"\n    params:\n        base = lambda wildcards, output: os.path.basename(output[0]),\n        dir = lambda wildcards, output: os.path.dirname(output[0])\n    shell:\n\"\"\"\n        jupyter nbconvert --to HTML --output-dir {params.dir} --output {params.base} \\\n            --execute supplementary_material.ipynb\n        \"\"\"\n</code></pre> <p>Note</p> <p>The Conda enivronment for the jupyter tutorial does not contain packages required to run the full snakemake workflow. So if you wish to test jupyter integration fully you should update the conda environment by running <code>conda install snakemake-minimal fastqc sra-tools multiqc bowtie2 tbb samtools htseq bedtools wget graphviz</code></p>"},{"location":"pages/jupyter/jupyter-9-extra-material/#more-integrations","title":"More integrations","text":"<p>Snakemake actually supports the execution of notebooks via the <code>notebook:</code> rules directive. See more about Jupyter integration in the snakemake docs. In the <code>notebook:</code> directive of such a rule you specify the path to a jupyter notebook (relative to the Snakefile) which is then executed when the rule is run.</p> <p>So how is this useful?</p> <p>In the notebook itself this gives you access to a <code>snakemake</code> object containing information about input and output files for the rule via <code>snakemake.input</code> and <code>snakemake.output</code>. Similarly, you can access rule wildcards with <code>snakemake.wildcards</code>, params with <code>snakemake.params</code>, and config settings with <code>snakemake.config</code>.</p> <p>When snakemake runs the rule with the <code>notebook:</code> directive <code>jupyter-nbconvert</code> is used to execute the notebook. No HTML output is generated here but it is possible to store a version of the notebook in its final processed form by adding the following to the rule:</p> <pre><code>log:\n    notebook=\"&lt;path&gt;/&lt;to&gt;/&lt;processed&gt;/&lt;notebook.ipynb&gt;\"\n</code></pre> <p>Because you won't get the notebook in full HTML glory, this type of integration is better suited if you want to use a notebook to generate figures and store these in local files (e.g. pdf/svg/png formats).</p> <p>We'll use the <code>supplementary_material.ipynb</code> notebook as an example! Let's say that instead of exporting the entire notebook to HTML we want a rule that outputs pdf versions of the barplot and heatmap figures we created.</p> <p>Let's start by setting up the rule. For simplicity we'll use the same input as when we edited the notebook in the first place. The output will be <code>results/barplot.pdf</code> and <code>results/heatmap.pdf</code>. Let's also output a finalized version of the notebook using the <code>log: notebook=</code> directive:</p> <pre><code>rule make_supplementary_plots:\n    input:\n        counts = \"results/tables/counts.tsv\",\n        summary = \"results/tables/counts.tsv.summary\",\n        multiqc = \"intermediate/multiqc_general_stats.txt\",\n        rulegraph = \"results/rulegraph.png\"\n    output:\n        barplot = \"results/barplot.pdf\",\n        heatmap = \"results/heatmap.pdf\"\n    log:\n        notebook = \"results/supplementary.ipynb\"\n</code></pre> <p>The notebook will now have access to <code>snakemake.input.counts</code>, <code>snakemake.output.barplot</code> and <code>snakemake.output.heatmap</code> when executed from within the workflow. Let's go ahead and edit the notebook! In the cell where we defined notebook parameters edit the code so that it looks like this:</p> <pre><code>counts_file=snakemake.input.counts\nsummary_file=snakemake.input.summary\nmultiqc_file=snakemake.input.multiqc\nrulegraph_file=snakemake.input.rulegraph\n\nSRR_IDs=snakemake.params.SRR_IDs\nGSM_IDs=snakemake.params.GSM_IDs\nGEO_ID=snakemake.params.GEO_ID\n</code></pre> <p>Notice that we set the <code>SRR_IDs</code>, <code>GSM_IDs</code> and <code>GEO_ID</code> variables using variables in <code>snakemake.params</code>? However, we haven't defined these in our rule yet so let's go ahead and do that now. Add the <code>params</code> section so that the <code>make_supplementary_plots</code> in the Snakefile looks like this:</p> <pre><code>rule make_supplementary_plots:\n    input:\n        counts = \"results/tables/counts.tsv\",\n        multiqc = \"intermediate/multiqc_general_stats.txt\",\n        rulegraph = \"results/rulegraph.png\"\n    output:\n        barplot = \"results/barplot.pdf\",\n        heatmap = \"results/heatmap.pdf\"\n    log:\n        notebook = \"results/supplementary.ipynb\"\n    params:\n        SRR_IDs = [\"SRR935090\",\"SRR935091\",\"SRR935092\"],\n        GSM_IDs = [\"GSM1186459\", \"GSM1186460\", \"GSM1186461\"],\n        GEO_ID = \"GSE48896\"\n    notebook: \"supplementary_material.ipynb\"\n</code></pre> <p>Tip</p> <p>One way to further generalize this rule could be to define the SRR_IDs, GSM_IDs and GEO_ID parameters in a config file instead, in which case they would be directly accessible from within the notebook using <code>snakemake.config['SRR_IDs']</code> etc.</p> <p>Now the rule contains everything needed, but we still need to edit the notebook to save the plots to the output files. First, edit the cell that generates the barplot so that it looks like this:</p> <pre><code># Create a stacked barplot\nax = summary_plot_data.T.plot(kind=\"bar\", stacked=True, color=colors)\n# Move legend and set legend title\nax.legend(bbox_to_anchor=(1,1), title=\"Category\");\nplt.savefig(snakemake.output.barplot, dpi=300, bbox_inches=\"tight\") ## &lt;-- Add this line!\n</code></pre> <p>Finally, edit the cell that generates the heatmap so that it looks like this:</p> <pre><code>count_data = counts.loc[:, SRR_IDs]\nheatmap_data = count_data.loc[(count_data.std(axis=1).div(count_data.mean(axis=1))&gt;1.2)&amp;(count_data.max(axis=1)&gt;5)]\nheatmap_data = heatmap_data.rename(columns = name_dict['title'])\nwith sns.plotting_context(\"notebook\", font_scale=0.7):\n    ax = sns.clustermap(data=np.log10(heatmap_data+1), cmap=\"RdYlBu_r\",\n                        method=\"complete\", yticklabels=True, linewidth=.5,\n                        cbar_pos=(.7, .85, .05, .1), figsize=(3,9))\n    plt.setp(ax.ax_heatmap.get_xticklabels(), rotation=270)\n    plt.savefig(snakemake.output.heatmap, dpi=300, bbox_inches=\"tight\") ## &lt;-- Add this line!\n</code></pre> <p>Now you can run the following to generate the plots:</p> <pre><code>snakemake -j 1 make_supplementary_plots\n</code></pre>"},{"location":"pages/jupyter/jupyter-9-extra-material/#presentations-with-jupyter","title":"Presentations with Jupyter","text":"<p>As if all the above wasn't enough you can also create presentations/slideshows with Jupyter! Simply use conda to install the RISE extension to your jupyter environment:</p> <pre><code>conda install -c conda-forge rise\n</code></pre> <p>then open up a notebook of your choice. In the menu click View -&gt; Cell Toolbar -&gt; Slideshow. Now every cell will have a drop-down in the upper right corner allowing you to set the cell type:</p> <ul> <li>Slide: a regular slide</li> <li>Sub-Slide: a regular slide that will be displayed below the previous</li> <li>Fragment: these cells split up slides so that content (fragments) are   added only when you press Space</li> <li>Skip: these cells will not appear in the presentation</li> <li>Notes: these cells act as notes, shown in the speaker view but not in the   main view</li> </ul> <p>The presentation can be run directly from the notebook by clicking the 'Enter/Exit RISE Slideshow' button (looks like a bar chart) in the toolbar, or by using the keyboard shortcut <code>Alt-r</code>. Running it directly from a notebook means you can also edit and run cells during your presentation. The downside is that the presentation is not as portable because it may rely on certain software packages that other people are not comfortable with installing.</p> <p>You can also export the notebook to an HTML-file with <code>jupyter nbconvert --execute --to SLIDES &lt;your-notebook.ipynb&gt;</code>. The resulting file, with the slideshow functionality included, can be opened in any browser. However, in this format you cannot run/edit cells.</p> <p>An example ?</p>"},{"location":"pages/jupyter/jupyter-9-extra-material/#jupyterlab","title":"JupyterLab","text":"<p>JupyterLab is a cutting-edge web-based development environment that offers an interactive and dynamic interface for working with notebooks, code, and data. Its adaptable design empowers users to customize and organize their workflows, catering to various applications such as data science, scientific computing, and machine learning. Moreover, its modular architecture is conducive to adding extensions that augment and diversify the functionality of the platform. </p> <p></p> <p>Want to try? You can go to https://jupyter.org/try or https://jupyterhub.cluster.france-bioinformatique.fr/ (if you have an account on the IFB cluster).</p>"},{"location":"pages/jupyter/jupyter-9-extra-material/#jupyter-book","title":"Jupyter book","text":"<p>With Jypter book, build an online book using a collection of Jupyter Notebooks and Markdown files with</p> <ul> <li>Interactivity</li> <li>Citations</li> <li>Build and host it online with GitHub/GitHub Pages...</li> <li>or locally on your own laptop</li> </ul> <p>An example ? You can find here a Linux introduction create with Jupyter Book. </p>"},{"location":"pages/jupyter/jupyter-installation/","title":"Setup Jupyter tutorial","text":""},{"location":"pages/jupyter/jupyter-installation/#setup-course-material","title":"Setup course material","text":"Follow this instructions only if you start the course at this stage! Otherwise skip this step! <pre><code>This tutorial depends on files from the course GitHub repo. Please follow these instructions \non how to set it up if you haven't done so already.  \nLet's create a directory and clone the course GitHub repo.\n\n```bash\nmkdir -p  ~/training-reproducible-research-area\ncd ~/training-reproducible-research-area\ngit clone https://github.com/SouthGreenPlatform/training_reproducible_research\n```\n</code></pre>"},{"location":"pages/jupyter/jupyter-installation/#setup-environment","title":"Setup environment","text":"<p>First let's create a dedicated folder for this tutorial:</p> <pre><code>mkdir -p  ~/training-reproducible-research-area/jupyter\ncd ~/training-reproducible-research-area/jupyter\ncp -r ~/training-reproducible-research-area/training_reproducible_research/tutorials/jupyter/* . </code></pre> <p>Let's continue using Conda for installing software, since it's so convenient to do so! Create a Conda environment from the environment.yml file and test the installation of Jupyter, like so:</p> <pre><code>conda env create -f environment.yml -n jupyter-env\nconda activate jupyter-env\n</code></pre>"},{"location":"pages/nextflow/nextflow-1-introduction/","title":"Introduction","text":"<p>Nextflow is a workflow management system (WfMS), and is one of the most common such systems within the bioinformatic and academic communities. These systems are important for scientific reproducibility in that they greatly facilitate keeping track of which files have been processed in what way throughout an entire project.</p> <p>Nextflow is built from the ground-up to be portable, scalable, reproducible and usable in a platform-agnostic sense. This means that any workflow you write in Nextflow can be run locally on your laptop, a computer cluster or a cloud service (as long as your architecture has the necessary computational resources). You can also define the compute environment in which each task is carried out on a per-task basis. You might thus develop your workflow on your local computer using a minimal test dataset, but run the full analyses with all samples on e.g. a computer cluster. Nextflow can work on both files and arbitrary values, oftentimes connected in useful and advanced ways.</p> <p>Nextflow can easily work with dynamic inputs where the exact output is unknown, e.g. the exact number of files or which samples pass some arbitrary quality control threshold. While Nextflow is based on the Groovy language, you don't need to know how to code Groovy to be able to write good Nextflow workflows. Nextflow has a large community centered around it, including the nf-core curated collection of high quality pipelines used by e.g. the National Genomics Infrastructure.</p> <p>This tutorial depends on files from the course GitHub repo. Take a look at the setup for instructions on how to set it up if you haven't done so already, then open up a terminal and go to <code>workshop-reproducible-research/tutorials/nextflow</code> and activate your <code>nextflow-env</code> Conda environment.</p>"},{"location":"pages/nextflow/nextflow-2-the-basics/","title":"The basics","text":"<p>We'll start by creating a very simple workflow from scratch, to show how Nextflow works: it will take two input files and convert them to UPPERCASE letters.</p> <ul> <li>Start by running the following commands:</li> </ul> <pre><code>touch main.nf\necho \"This is a.txt\" &gt; a.txt\necho \"This is b.txt\" &gt; b.txt\n</code></pre> <p>Open the <code>main.nf</code> file with an editor of your choice. This is the main workflow file used in Nextflow, where workflows and their processes are defined.</p> <ul> <li>Copy the following code into your <code>main.nf</code> file:</li> </ul> <pre><code>// Enable DSL2 functionality\nnextflow.enable.dsl = 2\n\n// Workflow definition\nworkflow {\n    // Define input files\n    ch_input = Channel.fromPath(\"a.txt\")\n\n    // Run workflow\n    CONVERT_TO_UPPER_CASE(ch_input)\n}\n\n// Process definition\nprocess CONVERT_TO_UPPER_CASE {\n    publishDir \"results/\", mode: \"copy\"\n\n    input:\n    path(file)\n\n    output:\n    path(\"a.upper.txt\")\n\n    script:\n    \"\"\"\n    tr [a-z] [A-Z] &lt; ${file} &gt; a.upper.txt\n    \"\"\"\n}\n</code></pre> <p>Here we have three separate parts. The first part enables the DSL2 (Domain Specific Language 2) functionality, and is required to use some of the newer and more powerful features of Nextflow. The next part is the workflow definition, while the last is a process. Let's go through the last two in more detail!</p> <p>Nextflow comments  Double-slashes (<code>//</code>) are used for comments in Nextflow.</p> <p>Nextflow and whitespace  Nextflow is not indentation-sensitive. In fact, Nextflow doesn't care at all about whitespace, so go ahead and use it in whatever manner you think is easiest to read and work with! Do keep in mind that indentations and other types of whitespace does improve readability, so it's generally not a good idea to forego it entirely, even though you can.</p>"},{"location":"pages/nextflow/nextflow-2-the-basics/#workflow-definitions","title":"Workflow definitions","text":"<p>The workflow definition here has two parts, each doing an important job for any Nextflow workflow. The first part defines a channel, which is an asynchronous first-in-first-out stream of data that connect a workflow's various inputs and outputs. In this particular case, we define a <code>Channel</code> using the <code>.fromPath</code> channel factory on the specific file path <code>a.txt</code>, and name the channel <code>ch_input</code>. You can read this as \"create the channel <code>ch_input</code> and send the file <code>a.txt</code> into it\".</p> <p>Naming channels  A channel can be named anything you like, but it is good practice to prepend them with <code>ch_</code>, as that makes it clear which variables are channels and which are just normal variables.</p> <p>How do we use these channels then? Channels pass data to and from processes through our workflow. By providing channels as arguments to processes, we describe how we want data to flow. This is exactly what we do in the second part: we call our <code>CONVERT_TO_UPPER_CASE</code> process with the <code>ch_input</code> as input argument - this is very similar to functional programming.</p> <p>This is our entire workflow, for now: the creation of a channel followed by using the contents of that channel as input to a single process. Let's look at how processes themselves are defined!</p>"},{"location":"pages/nextflow/nextflow-2-the-basics/#process-definitions","title":"Process definitions","text":"<p>Looking at the process in the code above, we can see several parts. The process block starts with its name, in this case <code>CONVERT_TO_UPPER_CASE</code>, followed by several sections: <code>publishDir</code>, <code>input</code>, <code>output</code> and <code>script</code>.</p> <p>Naming processes  A process can be named using any case, but a commonly used convention is to use UPPERCASE letters for processes to visually distinguish them in the workflow. You do not have to follow this if you don't want to, but we do so here.</p> <p>Let's ignore the first section for now and focus on the last three. The <code>input</code> and <code>output</code> sections describe the data expected to come through the channel for this specific process. Each line of <code>input</code> describes the data expected for each process argument, in the order used in the workflow. In this case, <code>CONVERT_TO_UPPER_CASE</code> expects a single channel (one line of input), and expects the data to be filenames (of type <code>path</code>). Notice that there is a difference between how the inputs and outputs are declared? The <code>output</code> is an explicit string (i.e surrounded by quotes), while the input is a variable named <code>file</code>. This means inputs can be referenced in the process without naming the data explicitly, unlike the output where the name needs to be explicit. We'll get back to exactly how this works in just a moment.</p> <p>Let's move on to the first section: <code>publishDir</code>. This tells Nextflow where the output of the process should be stored when it is finished; setting <code>mode</code> to <code>\"copy\"</code> just means that we want to copy the output files to the publishing directory, rather than using a symbolic link (which is the default).</p>"},{"location":"pages/nextflow/nextflow-2-the-basics/#executing-workflows","title":"Executing workflows","text":"<p>Let's try running the workflow we just created!</p> <ul> <li>Type the following in your terminal:</li> </ul> <pre><code>nextflow run main.nf\n</code></pre> <p>This will make Nextflow run the workflow specified in your <code>main.nf</code> file. You should see something along these lines:</p> <pre><code>N E X T F L O W  ~  version 21.04.0\nLaunching `./main.nf` [mad_legentil] - revision: 87f0c253ed\nexecutor &gt;  local (1)\n[32/9124a1] process &gt; CONVERT_TO_UPPER_CASE (1) [100%] 1 of 1 \u2714\n</code></pre> <p>The first few lines are information about this particular run, including the Nextflow version used, which workflow definition file was used, a randomly generated run name (an adjective and a scientist), the revision ID as well as where the processes were executed (locally, in this case).</p> <p>What follows next is a list of all the various processes for this particular workflow. The order does not necessarily reflect the order of execution (depending on each process\u2019 input and output dependencies), but they are in the order they were defined in the workflow file - there's only the one process here, of course. The first part (e.g <code>[32/9124a1]</code>) is the process ID, which is also the first part of the subdirectory in which the process is run (before the outputs are transferred to the publish directory). We then get the process and its name. Lastly, we get how many instances of each process are currently running or have finished. Here we only have the one process, of course, but this will soon change.</p> <ul> <li> <p>Let's check that everything worked: type <code>ls results/</code> and see that it   contains the output we expected.</p> </li> <li> <p>Let's explore the working directory: change into whatever directory is   specified by the process ID (your equivalent to <code>work/32/9124a1[...]</code>).</p> </li> </ul> <p>What do you see when you list the contents of this directory? You should, hopefully, see a symbolic link named <code>a.txt</code> pointing to the real location of this file, plus a normal file <code>a.upper.txt</code>, which is the output of the process that was run in this directory. While it seems cumbersome to manually move into these work directories it is something you only do when debugging errors in your workflow, and Nextflow has some tricks to make this process a lot easier - more on this later.</p> <p>So, how does this all work? Well, we have three components: a set of inputs, a set of processes and a workflow that defines which processes should be run. We tell Nextflow to push the inputs through the entire workflow, so to speak.</p> <ul> <li> <p>Now it's your turn! Move back to the workflow root and make it use only the   <code>b.txt</code> input file and give you the <code>b.upper.txt</code> instead.</p> </li> <li> <p>Run your workflow and make sure it works before you move on.</p> </li> </ul>"},{"location":"pages/nextflow/nextflow-2-the-basics/#files-and-sample-names","title":"Files and sample names","text":"<p>Having to manually change inputs and outputs like you just did is not really ideal, is it? Hard-coding outputs is rarely good, so let's try to change that. One powerful feature of Nextflow is that it can handle complex data structures as input, and not only filenames. One strategy we can follow is to create a prefix for our output and pass it together with the filename.</p> <ul> <li>Change the channel definition to the following:</li> </ul> <pre><code>ch_input = Channel\n    .fromPath(\"a.txt\")\n    .map{ file -&gt; tuple(file.getBaseName(), file) }\n</code></pre> <p>Okay, so what does that do, exactly? Well, the added line containing the <code>.map{}</code> statement changes the data stream to be <code>[prefix, file]</code> instead of just <code>[file]</code> - we generate the prefix from the base name of the file itself, i.e. the file without extension or directory. We now have to change the process itself to make use of this new information contained in the <code>ch_input</code> channel.</p> <ul> <li>Change the process definition to the following:</li> </ul> <pre><code>process CONVERT_TO_UPPER_CASE {\n    publishDir \"results/\", mode: \"copy\"\n\n    input:\n    tuple val(prefix), path(file)\n\n    output:\n    path(\"${prefix}.upper.txt\")\n\n    script:\n    \"\"\"\n    tr [a-z] [A-Z] &lt; ${file} &gt; ${prefix}.upper.txt\n    \"\"\"\n}\n</code></pre> <p>Notice how the input now is aware that we're passing a tuple as input, which allows us to use both the <code>file</code> variable (as before) and the new <code>prefix</code> variable. All that's left now is to change the input to our pipeline!</p> <ul> <li>Change the channel definition line from <code>.fromPath(\"a.txt\")</code> to   <code>.fromPath([\"a.txt\", \"b.txt\"])</code> and try running the pipeline. Make sure it   works before you move on!</li> </ul>"},{"location":"pages/nextflow/nextflow-2-the-basics/#adding-more-processes","title":"Adding more processes","text":"<p>It's time to add more processes to our workflow! We have the two files <code>a.upper.txt</code> and <code>b.upper.txt</code>; the next part of the workflow is a step that concatenates the content of all these UPPERCASE files.</p> <p>We already have a channel containing the two files we need: the output of the <code>CONVERT_TO_UPPER_CASE</code> process called <code>CONVERT_TO_UPPER_CASE.out</code>. We can use this output as input to a new process using the syntax: <code>CONVERT_TO_UPPER_CASE.out.collect()</code>. The <code>collect()</code> operator, groups all the outputs in the channel into a single data object for the next process. This is a many-to-one type of operation: a stream with several files (many) is merged into a lone list of files (one). If <code>collect()</code> was not used, the next process would try to run a task for each file in the output channel.</p> <p>Let's put this in use by adding a new process to the workflow definition. We'll call this process <code>CONCATENATE_FILES</code> and it will take the output from <code>CONVERT_TO_UPPER_CASE</code> as input, grouped using the <code>collect()</code> operator.</p> <ul> <li>Add a line to your workflow definition for this new process with the   appropriate input - click below if you're having trouble.</li> </ul> Click to show the solution <pre><code>CONCATENATE_FILES( CONVERT_TO_UPPER_CASE.out.collect() )\n</code></pre> <p>Now all we have to do is define the actual <code>CONCATENATE_FILES</code> process in the process definition section.</p> <ul> <li>Copy the following code as a new process into your workflow:</li> </ul> <pre><code>process CONCATENATE_FILES {\n    publishDir \"results/\", mode: \"copy\"\n\n    input:\n    path(files)\n\n    output:\n    path(\"*.txt\")\n\n    script:\n    \"\"\"\n    cat ${files} &gt; concat.txt\n    \"\"\"\n}\n</code></pre> <p>Run your workflow again and check the <code>results/</code> directory. At this point you should have three files there: <code>a.upper.txt</code>, <code>b.upper.txt</code> and <code>concat.txt</code>.</p> <ul> <li>Inspect the contents of <code>concat.txt</code> - do you see everything as you expected?</li> </ul> <p>Note the use of <code>path(files)</code> as input. Although we pass a list of files as input, the list is considered a single object, and so the <code>files</code> variable references a list. Each file in that list can be individually accessed using an index e.g. <code>${files[0]}</code>, or as we do here, use the variable without an index to list all the input files.</p>"},{"location":"pages/nextflow/nextflow-2-the-basics/#viewing-channel-contents","title":"Viewing channel contents","text":"<p>As our channels become more complicated it is useful to actually check out what's inside them: you can do this using the <code>.view()</code> operator.</p> <ul> <li>Add the following to your workflow definition (on a new line) and execute the   workflow: <code>ch_input.view()</code>. What do you see?</li> </ul> <p>It can be quite useful to inspect channel contents like this when you are developing workflows, especially if you are working with tuples, maps and any transforming operators in general.</p> <ul> <li>Check the channel contents of the (1) raw and (2) collected output of the   <code>CONVERT_TO_UPPER_CASE</code> process. How are they different?</li> </ul> <p>Quick recap</p> <p>In this section we've learnt:</p> <ul> <li>How to create and extend simple Nextflow workflows</li> <li>How to create channels for input data</li> <li>How to execute workflows</li> <li>How to explore Nextflow's <code>work</code> directory</li> <li>How to generalize workflows</li> <li>How to view channel contents</li> </ul>"},{"location":"pages/nextflow/nextflow-3-executing-workflows/","title":"Executing workflows","text":"<p>It's time to start working with a more realistic workflow using the MRSA case study of this course! We've created a bare-bones version of this pipeline for you, but we'll work our way through it as we go along and learn more about Nextflow's features and functionality. The MRSA workflow looks like this:</p> <pre><code>workflow {\n\n    // Workflow for generating count data for the MRSA case study\n\n    // Define SRA input data channel\n    ch_sra_ids = Channel.fromList( [\"SRR935090\", \"SRR935091\", \"SRR935092\"] )\n\n    // Define the workflow\n    GET_SRA_BY_ACCESSION (\n        ch_sra_ids\n    )\n    RUN_FASTQC (\n        GET_SRA_BY_ACCESSION.out\n    )\n    RUN_MULTIQC (\n        RUN_FASTQC.out[1].collect()\n    )\n    GET_GENOME_FASTA ()\n    INDEX_GENOME (\n        GET_GENOME_FASTA.out.fasta\n    )\n    ALIGN_TO_GENOME (\n        GET_SRA_BY_ACCESSION.out,\n        INDEX_GENOME.out.index\n    )\n    SORT_BAM (\n        ALIGN_TO_GENOME.out.bam\n    )\n    GET_GENOME_GFF3 ()\n    GENERATE_COUNTS_TABLE (\n        SORT_BAM.out.bam.collect(),\n        GET_GENOME_GFF3.out.gff\n    )\n}\n</code></pre> <p>The workflow has one input channel named <code>ch_sra_ids</code>, which is a list of SRA IDs (i.e. a list of strings). We then define the processes to be executed by this workflow, nine in total. The first process (<code>GET_SRA_BY_ACCESSION</code>) takes the <code>ch_sra_ids</code> channel as input, while the rest of the processes takes the output of previous processes as input. Before we go into more detail regarding the ins-and-outs of this workflow, let's start with some specifics of how workflows are executed and what you can get from them.</p>"},{"location":"pages/nextflow/nextflow-3-executing-workflows/#reports-and-visualisations","title":"Reports and visualisations","text":"<p>Let's start with running the workflow plus getting some reports and visualisation while we're at it!</p> <ul> <li>Run the workflow using the following command: <code>nextflow run main_mrsa.nf   -with-report -with-timeline -with-dag dag.png</code>.</li> </ul> <p>After successful executing, you will find three more files in your current directory: <code>report.html</code>, <code>timeline.html</code> and <code>dag.png</code>. The first file contains a workflow report, which includes various information regarding execution such as runtime, resource usage and details about the different processes. The second file contains a timeline for how long each individual process took to execute, while the last contains a visualisation of the workflow itself.</p> <p>Take a few minutes to browse these files for yourself! When running a workflow you can of course choose which of these additional files you want to include by picking which ones are important or interesting to you - or don\u2019t include any!</p>"},{"location":"pages/nextflow/nextflow-3-executing-workflows/#re-running-workflows","title":"Re-running workflows","text":"<p>Something you often want to do in Nextflow (or any WfMS for that matter) is to re-run the workflow when you changed some input files or some of the code for its analyses, but you don't want to re-run the entire workflow from start to finish. Let\u2019s find out how this works in Nextflow!</p> <ul> <li>Run the same <code>nextflow run main_mrsa.nf</code> command again.</li> </ul> <p>What happened here? Nextflow actually re-ran the entire workflow from scratch, even though we didn't change anything. This is the default behaviour of Nextflow.</p> <ul> <li>Let\u2019s try that again: <code>nextflow run main_mrsa.nf -resume</code> instead.</li> </ul> <p>Now you can see that Nextflow didn't actually re-run anything. The <code>-resume</code> flag instructed Nextflow to use the cached results from the previous run!</p> <p>Nextflow automatically keeps track of not only changes to input files, but also changes to code, process definitions and scripts. You can thus change anything relating to your workflow and just re-run with the <code>-resume</code> flag and be sure that only processes relevant to your changes are executed again!</p> <ul> <li>Use <code>tree work</code> to list the contents of the work directory.</li> </ul> <p>Because Nextflow keeps track of all the runs, we've now got two sets of files in the work directory. One set from the first run, and another from the second run. This can take up valuable space, so let's clean that up.</p> <ul> <li>Use <code>nextflow clean -n -before &lt;run_name&gt;</code> to show which work directories will be cleaned up. Then delete those directories by changing <code>-n</code> to <code>-f</code>.</li> </ul> <p>Nextflow's <code>clean</code> subcommand can be used to clean up failed tasks and unused processes. Use <code>nextflow help clean</code> to see other options for cleaning. This is the preferred way to clean up the working directory.</p> <ul> <li>Remove the <code>results</code> directory and re-run the workflow again using the   <code>-resume</code> flag.</li> </ul> <p>We removed all the results we used before, but we still managed to resume the workflow and use its cache - how come? Remember that Nextflow uses the <code>work</code> directory to run all of its tasks, while the <code>results</code> directory is just where we have chosen to publish our outputs. We can thus delete the <code>results</code> directory as often as we like (a necessity when output filenames are changed) and still get everything back without having to re-run anything. If we were to delete the <code>work</code> directory, however...</p> <ul> <li>Delete the <code>work</code> directory and re-run the workflow using the <code>-resume</code> flag.</li> </ul> <p>There is no longer any cache for Nextflow to use, so it re-runs from the start! This is good to keep in mind: you can always delete the output directories of your workflow, but if you mess with <code>work</code> you'll lose, well... work!</p>"},{"location":"pages/nextflow/nextflow-3-executing-workflows/#logs","title":"Logs","text":"<p>Nextflow keeps a log of all the workflows that have been executed. Let's check it out!</p> <ul> <li>Type <code>nextflow log</code> to get a list of all the executions.</li> </ul> <p>Here we get information about when the workflow was executed, how long it ran, its run name, whether it succeeded or not and what command was used to run it. You can also use <code>nextflow log &lt;run name&gt;</code> to show each task's directory that was executed for that run. You can also supply the <code>-f</code> (or <code>-fields</code>) flag along with additional fields to show.</p> <ul> <li>Run <code>nextflow log &lt;run name&gt; -f hash,name,exit,status</code></li> </ul> <p>This shows us not only the beginning of each task's working directory, but also its name, exit code and status (i.e. if it completed successfully or failed in some manner).</p> <p>Listing fields  If you want to see a complete list of all the fields you might explore using the log, just type <code>nextflow log -l</code> or <code>nextflow log -list-fields</code>. This is highly useful for debugging when there's some specific information about a run you're particularly interested in!</p> <p>We can also get even more detailed information about the latest run by looking into the <code>.nextflow.log</code> file!</p> <ul> <li>Look into the latest log by typing <code>less .nextflow.log</code>.</li> </ul> <p>You'll be greeted by a wealth of debugging information, which may even seem a bit overkill at this point! This level of detail is, however, quite useful both as a history of what you've attempted and as an additional help when you run into errors! Also, it helps with advanced debugging - which we'll get into later.</p> <p>Quick recap</p> <p>In this section we've learnt:</p> <ul> <li>How to get automatic reports and visualisations</li> <li>How to re-run workflows</li> <li>How to clean the Nextflow cache</li> <li>How to check the Nextflow logs</li> </ul>"},{"location":"pages/nextflow/nextflow-4-working-with-processes/","title":"Working with process","text":"<p>Now that we've gone through the specifics of executing workflows in a bit more detail, let's go through working with processes. While there are numerous process directives that can be used, we'll go through some of the more commonly used ones here.</p>"},{"location":"pages/nextflow/nextflow-4-working-with-processes/#tags","title":"Tags","text":"<p>Let's look at the command line output we got during the workflow's execution, which should look something like this:</p> <pre><code>N E X T F L O W  ~  version 21.04.0\nLaunching `./main.nf` [friendly_bhaskara] - revision: b4490b9201\nexecutor &gt;  local (17)\n[c9/e5f818] process &gt; GET_SRA_BY_ACCESSION (SRR935092) [100%] 3 of 3 \u2714\n[d5/b5f24e] process &gt; RUN_FASTQC (SRR935092)           [100%] 3 of 3 \u2714\n[91/2cea54] process &gt; RUN_MULTIQC                      [100%] 1 of 1 \u2714\n[e0/b4fd37] process &gt; GET_GENOME_FASTA                 [100%] 1 of 1 \u2714\n[87/32ce10] process &gt; INDEX_GENOME                     [100%] 1 of 1 \u2714\n[56/e9a460] process &gt; ALIGN_TO_GENOME (SRR935092)      [100%] 3 of 3 \u2714\n[ed/d8c223] process &gt; SORT_BAM (SRR935092)             [100%] 3 of 3 \u2714\n[e7/4a6bda] process &gt; GET_GENOME_GFF3                  [100%] 1 of 1 \u2714\n[e9/84f093] process &gt; GENERATE_COUNTS_TABLE            [100%] 1 of 1 \u2714\n</code></pre> <p>Have you noticed that there are SRA IDs after some of the processes? Well, if you look at which processes show these SRA IDs you might see that it's only those processes that are executed three times, i.e. once per SRA ID. This doesn't happen automatically, however, and comes from something called tags. Let's look at the <code>GET_SRA_BY_ACCESSION</code> process:</p> <pre><code>process GET_SRA_BY_ACCESSION {\n\n    // Retrieve a single-read FASTQ file from SRA (Sequence Read Archive) by run\n    // accession number.\n\n    tag \"${sra_id}\"\n    publishDir \"results/data/raw_internal\",\n        mode: \"copy\"\n\n    input:\n    val(sra_id)\n\n    output:\n    tuple val(sra_id), path(\"*.fastq.gz\")\n\n    script:\n    \"\"\"\n    fastq-dump ${sra_id} \\\n        -X 25000 \\\n        --readids \\\n        --dumpbase \\\n        --skip-technical \\\n        --gzip \\\n        -Z \\\n        &gt; ${sra_id}.fastq.gz\n    \"\"\"\n}\n</code></pre> <p>You can see the <code>tag</code> directive at the very top of the process definition. Tags can be used to e.g. show information about the sample currently being analysed by the process. This is useful both during run-time (allowing you to see which sample is being processed) but also for debugging or finding problematic samples in case of errors or odd output. There is, naturally, no need to use tags for processes which are only run once.</p> <ul> <li> <p>Comment out (prefix with <code>//</code>) the <code>tag</code> directive from the <code>GET_SRA_BY_ACCESSION</code> process and run the   workflow again. No more SRA IDs!</p> </li> <li> <p>Uncomment the <code>tag</code> directive before you move on.</p> </li> </ul>"},{"location":"pages/nextflow/nextflow-4-working-with-processes/#named-outputs","title":"Named outputs","text":"<p>Let's move on to the next process! It looks like this:</p> <pre><code>process RUN_FASTQC {\n\n    // Run FastQC on a FASTQ file.\n\n    tag \"${sample}\"\n    publishDir \"results/\", mode: \"copy\"\n\n    input:\n    tuple val(sample), path(fastq)\n\n    output:\n    path(\"*.html\")\n    path(\"*.zip\")\n\n    script:\n    \"\"\"\n    # Run FastQC\n    fastqc ${fastq} -q\n    \"\"\"\n}\n</code></pre> <p>Here is a process with two output channels! One contains all the <code>.html</code> files, while the other contains all the <code>.zip</code> files. How is this handled in the workflow definition of downstream processes that use the outputs? The <code>RUN_MULTIQC</code> process uses this output, and its part in the workflow definition looks like this:</p> <pre><code>RUN_MULTIQC (\n    RUN_FASTQC.out[1].collect()\n)\n</code></pre> <p>We already know about <code>.out</code> and <code>.collect()</code>, but we have something new here: the <code>RUN_MULTIQC</code> process is taking the second channel of the output from the <code>RUN_FASTQC</code> process - <code>[1]</code> is the index for the second channel, as Groovy is zero-based (the first channel is indexed by <code>[0]</code>).</p> <p>This comes with some issues, however. What if we accidentally changed the order of the outputs in the rule, or added a new one? Using positions like this is easy to mess up, but there is a better solution: named outputs! This can be achieved by adding the <code>emit</code> option for some or all of the outputs, like so:</p> <pre><code>output:\npath(*.txt), emit: text\n</code></pre> <p>Instead of referring to the output by its position in an array as previously, we refer to the channel with a label we choose (<code>.out.text</code>) instead. This benefits us in that we can infer more information about channel contents called <code>text</code> rather than <code>[1]</code>, and it is also allows us to be less error-prone when rewriting parts of a workflow.</p> <p>Your turn! Add named outputs to the <code>RUN_FASTQC</code> process and make <code>RUN_MULTIQC</code> use those outputs. You'll have to change both the output section of the <code>RUN_FASTQC</code> process, and the workflow definition section for <code>RUN_MULTIQC</code>. If you need help, see the hint below.</p> Click to show the solution <pre><code>    // Workflow definition for RUN_MULTIQC\n    RUN_MULTIQC (\n        RUN_FASTQC.out.zip.collect()\n\n    // Output section of RUN_FASTC\n    output:\n        path(\"*.html\"), emit: html\n        path(\"*.zip\"),  emit: zip\n</code></pre> <p>Check if it works by executing the workflow.</p>"},{"location":"pages/nextflow/nextflow-4-working-with-processes/#advanced-publishing","title":"Advanced publishing","text":"<p>So far we've only used the <code>publishDir</code> directive in a very simple way: specifying a directory and the <code>mode</code> to use when publishing (to copy the files rather than symbolically link them). There are more things you can do, however, especially for processes with more than one output. For example, we can publish outputs in separate directories, like so:</p> <pre><code>publishDir \"results/tables/\",\n    pattern: \"*.tsv\",\n    mode: \"copy\"\npublishDir \"results/logs\",\n    pattern: \"*.log\",\n    mode: \"copy\"\n</code></pre> <p>In this example, <code>*.tsv</code> files are copied to the folder <code>results/tables/</code>, while <code>*.log</code> files are copied to the folder <code>results/logs</code>. The <code>publishDir</code> directive can be used multiple times in a single process, allowing one to separate output as above, or publish the same output to multiple folders.</p> <ul> <li>Edit the <code>RUN_FASTQC</code> process to place the HTML and compressed files in   separate directories. Remove the <code>results</code> directory and re-run the workflow   to check that it worked.</li> </ul> <p>Note that an output and a published output are different things: something can be an output of a process without being published. In fact, the <code>RUN_FASTQC</code> process is a prime example of this! Think about the compressed output: this output is only used by the downstream process <code>RUN_MULTIQC</code> and is never meant to be viewed by a human or used by a human in some downstream task not part of the pipeline itself. We would thus like to keep the compressed files as an output, but not publish said output. How do we do this? Just remove the corresponding <code>publishDir</code> directive!</p> <p>The MRSA workflow we've made here was refactored directly from its original version in the Snakemake tutorial of this course, which means that its output structure is not fully taking advantage of some of Nextflow's functionality. The compressed output we've already talked about above is, for example, put in the <code>results/intermediate/</code> directory. This is required for Snakemake, but not so for Nextflow.</p> <ul> <li>See if you can find any other processes in the current implementation of the   MRSA workflow that you could optimise like this!</li> </ul> <p>Think about whether all processes actually need to have published outputs. Make sure you test executing the workflow after you've made any changes.</p>"},{"location":"pages/nextflow/nextflow-4-working-with-processes/#debugging","title":"Debugging","text":"<p>It is, sadly, inevitable that we all make mistakes while coding - nobody's perfect! Nextflow helps you quite a bit when this happens, not just with its logs but also with informative error messages. Let's introduce an error and look at what we get:</p> <ul> <li>Change the name of the <code>multiqc_general_stats.txt</code> output in the <code>RUN_MULTIQC</code>   process to something different and re-run the workflow.</li> </ul> <p>We got an error! We get a number of things, actually, including (in order from the top) the name of the process that gave the error, the likely cause, the command that was executed, along with its exit status, output, error and the work directory that the task was run in. Let's focus on the <code>Caused by:</code> part, which should look something like this:</p> <pre><code>Caused by:\n  Missing output file(s) `multiqc_general_stats.text` expected by process `RUN_MULTIQC`\n</code></pre> <p>We can also see that the command's exit status is <code>0</code>, which means that the command was successful; any exit status other than <code>0</code> means there was an error of some kind. We can thus infer that the command (1) worked, (2) failed to give us the output expected by Nextflow. Thankfully, Nextflow graciously prints the work directory for us so that we may check out what happened in more detail.</p> <ul> <li>Copy the working directory path, <code>cd</code> into it and list its contents using   <code>ls</code>.</li> </ul> <p>You might already have spotted the error in the message above! The error we introduced here was that the expected output file has a <code>.text</code> extension, rather than the correct <code>.txt</code>. Nextflow is expecting the <code>.text</code> output, but the process <code>script</code> directive is (correctly) giving us the <code>.txt</code> file.</p> <ul> <li>Go back to the root directory, revert the error you introduced and re-run the   workflow to make sure it works again.</li> </ul> <p>This might have seemed like a trivial error, but a lot of errors in Nextflow can be solved in the same manner, i.e. by just following the debugging output reported by Nextflow and inspecting the specific subdirectory in question.</p> <p>A note about Bash</p> <p>If you are using Bash variables inside the <code>script</code> directive you have to be careful to prepend it with a backslash, e.g. <code>\\${BASH_VARIABLE}</code>. This is because the dollar-sign is used by Nextflow, so you have to tell Nextflow explicitly when you're using a Bash variable. This is a common source of errors when using Bash variables, so keeping it in mind can save you some debugging time!</p> <p>Quick recap</p> <p>In this section we've learnt:</p> <ul> <li>How to use the <code>tag</code> directive</li> <li>How to use named output</li> <li>How to separate process outputs into different directories</li> <li>How to debug errors and mistakes</li> </ul>"},{"location":"pages/nextflow/nextflow-5-workflow-configuration/","title":"Workflow configuration","text":"<p>We've so far been working with a relatively non-generalised workflow: it's got hard-coded inputs, paths and genome references. This is perfectly fine for a project that is purely aimed at getting reproducible results (which is the full extent of what you want in a lot of cases), but it can be made a lot more generalisable. Let's go through the MRSA workflow and see what can be improved!</p>"},{"location":"pages/nextflow/nextflow-5-workflow-configuration/#configuration-basics","title":"Configuration basics","text":"<p>One of the things that allow generalisability of Nextflow workflows is parameters, which hold information and values that can be changed directly on the command-line at the time of execution. One use of parameters in our MRSA workflow is to remove the hard-coded <code>results</code> output directory, for example. Parameters can be written in the following form:</p> <pre><code>params {\n    parameter_1 = \"some/data/path\"      // A string parameter\n    parameter_2 = 42                    // A value parameter\n    parameter_3 = [\"a\", \"b\", \"c\", \"d\"]  // A list parameter\n}\n</code></pre> <p>You would then refer to these parameters using e.g. <code>params.parameter_1</code> anywhere you need to in the workflow. Although parameters can be defined in <code>main_mrsa.nf</code>, it is preferable to define them in a separate configuration file. The default name of this file is <code>nextflow.config</code> and if such a file is present it will be used automatically by Nextflow (to supply a config file with another name use <code>nextflow -c &lt;path-to-config-file&gt; run main_mrsa.nf</code>)</p> <ul> <li> <p>Create a configuration file and add a parameter for the <code>results</code> output   directory.</p> </li> <li> <p>Use your newly created parameter in the <code>publishDir</code> directory of a process   (it'll be in the form of <code>${params.resultsdir}/some/other/path</code>, for example).   Run your workflow to see if it worked.</p> </li> </ul> <p>Tip</p> <p>Instead of manually changing all the hard-coded directories in your workflow you can use the following little <code>sed</code> command, which will do it for you: <code>sed 's/\\\"results\\//\\\"${params.resultsdir}\\//g' main_mrsa.nf &gt; tmp; mv tmp main_mrsa.nf</code>. In case you used a parameter name other than <code>resultsdir</code> update the command accordingly.</p>"},{"location":"pages/nextflow/nextflow-5-workflow-configuration/#command-line-parameters","title":"Command line parameters","text":"<p>Workflow parameters can be assigned on the command-line by executing workflows like so: <code>nextflow run main_mrsa.nf --parameter_name 'some_value'</code>. The workflow parameter <code>parameter_name</code>, is prefixed by a double dash <code>--</code> to tell Nextflow this is a parameter to the workflow (a single dash is a parameter to Nextflow, e.g. <code>-resume</code>). The value is also quoted (this is important for parameters that take file paths as values).</p> <ul> <li>Run your workflow using the parameter you previously created, but pick   something other than the default value!</li> </ul> <p>You should now have a new directory containing all the results! This is highly useful if you want to keep track of separate runs of a workflow with different software parameters, for example: <code>nextflow run main.nf --important_param 'value1' --resultsdir 'value1'</code>, or simply want to keep the results of separate versions of the same workflow. You can also change parameters by using the <code>-params-file</code> option or by using another configuration file (and using <code>-c</code>), rather than on the command line!</p>"},{"location":"pages/nextflow/nextflow-5-workflow-configuration/#configuring-inputs","title":"Configuring inputs","text":"<p>Remember the input for the MRSA workflow, the <code>ch_sra_ids</code> channel? This input is also hard-coded inside the <code>main_mrsa.nf</code> file. This could also be made into a parameter!</p> <ul> <li>Add another parameter for the input SRA IDs and execute your workflow to check   that it worked.</li> </ul> <p>Using lists for parameters has its problems though, as you won't be able to change it on the command line, since the command line doesn't know about Groovy lists. There are several other ways of specifying inputs in a command line-friendly way, one of which is to use sample sheets. Instead of specifying samples directly in the command line, you specify a file that lists the input samples instead; this is the standard that is used in e.g. nf-core pipelines. Such a sample sheet for the MRSA workflow might be stored in e.g. <code>input.csv</code> and look like this:</p> <pre><code>sra_id\nSRR935090\nSRR935091\nSRR935092\n</code></pre> <p>Reading input from a CSV file can be done by combining the <code>.splitCsv</code> channel factory (splitting the rows to read each entry) and the <code>.map</code> operator (defining which columns should be used). Let's see if we can make it work!</p> <ul> <li> <p>Create the <code>input.csv</code> file with the above shown content.</p> </li> <li> <p>Change the definition of the <code>ch_sra_ids</code> channel to take the value of a new   parameter of your choice, defined in the configuration file.</p> </li> <li> <p>Add the <code>.splitCsv(header: true)</code> operator to the end of the channel   definition, so that the input is read from the file contents.</p> </li> <li> <p>Add the <code>.map{row -&gt; row.sra_id}</code> operator, which specifies that each row   should contain the <code>sra_id</code> column, but no other columns.</p> </li> </ul> <p>You should now have a more generalised input to your workflow! Try to run it to make sure it works - look below if you need some help.</p> Click to show the solution <pre><code>// Channel definition\nch_sra_ids = Channel\n    .fromPath ( params.sra_ids )\n    .splitCsv ( header: true )\n    .map      { row -&gt; row.sra_id }\n\n// Configuration file\nsra_ids = \"input.csv\"\n</code></pre> <p>By specifying inputs from sample sheets like this we can change inputs of a workflow execution by creating another sample sheet and specifying e.g., <code>--sra_ids input-2.csv</code> on the command line. This is highly useful when you want to run a single sample e.g., when testing a workflow, or when you want to keep track of all the different inputs you've used historically. Sample sheets are also useful for keeping other metadata, such as custom sample names, sample groups, location of files, etc. For example:</p> <pre><code>sample-1,case,data/sample-1.fastq.gz\nsample-2,ctrl,data/sample-2.fastq.gz\nsample-3,case,data/sample-3.fastq.gz\nsample-4,ctrl,data/sample-4.fastq.gz\n</code></pre> <p>Here we have not only names and file paths but also to which group each sample belongs, i.e. case or control. Such metadata can be highly useful for more advanced workflows to use in downstream analyses, such as differential gene expression! We could create a tuple based on this metadata like so:</p> <pre><code>ch_input = Channel\n    .fromPath(\"metadata.csv\")\n    .splitCsv(header: ['id', 'group', 'fastq'])\n    .view()\n</code></pre> <p>Input file formatting  The input file may also have headers, in which case you can use <code>header: true</code> to use the column headers defined in the file. You can also read e.g. tab-separated files by using the <code>sep</code> field: <code>.splitCsv(sep: \"\\t\")</code>.</p>"},{"location":"pages/nextflow/nextflow-5-workflow-configuration/#other-configuration-scopes","title":"Other configuration scopes","text":"<p>There are lots of things that you might want to add to your configuration, not just parameters! The workflow manifest, for example, which might look like this:</p> <pre><code>manifest {\n    name        = \"My Workflow\"\n    description = \"My workflow, created by me\"\n    author      = \"Me\"\n    mainScript  = \"main.nf\"\n    version     = \"1.0.0\"\n}\n</code></pre> <ul> <li>Go ahead and add a workflow manifest to your <code>nextflow.config</code> file!</li> </ul> <p>The manifest is useful when you're publishing or sharing the workflow through e.g. GitHub or similar. There are many more such configuration scopes that you might want to use - read more about them in the documentation.</p> <p>Quick recap</p> <p>In this section we learnt:</p> <ul> <li>How to create parameters in a configuration file</li> <li>How to specify parameters on the command line</li> <li>How to add workflow manifest and other configuration scopes</li> </ul>"},{"location":"pages/nextflow/nextflow-6-optimising-the-mrsa-workflow/","title":"Optmising the MRSA workflow","text":"<p>We just added several parameters and configurations to our MRSA workflow, but we didn't do anything about the reference genomes: those are still hard-coded. How come? Well, the current MRSA workflow is, in fact, not very well-optimised for Nextflow at all, being a refactor from the Snakemake tutorial of this course.</p> <p>All of the processes are basically unchanged, excluding some minor alterations. For example, the <code>run_fastqc</code> rule in Snakemake used the <code>-o</code> flag to specify that the results should be in the current directory, followed by moving the output files to their respective output directory. The first part is not needed in Nextflow (as everything is run in its own subdirectory), and the second part is done by the <code>publishDir</code> directive. These are just minor alterations, though, but we can do much more if we fully utilise Nextflow's features!</p>"},{"location":"pages/nextflow/nextflow-6-optimising-the-mrsa-workflow/#remote-files","title":"Remote files","text":"<p>One of these features is the ability to automatically download remote files, without needing to explicitly do so! The <code>path</code> input type can handle either file paths (like we've done so far) or a URI-supported protocol (such as <code>http://</code>, <code>s3://</code>, <code>ftp://</code>, etc.). This would be highly useful for e.g. the <code>GET_GENOME_FASTA</code> process - in fact, we don't even need that process at all! All we need to do is to change the input to the <code>INDEX_GENOME</code> and <code>ALIGN_TO_GENOME</code> processes.</p> <ul> <li> <p>Create a new input channel using the <code>fromPath()</code> channel factory and the   absolute path to the genome FASTA.</p> </li> <li> <p>Make the <code>INDEX_GENOME</code> process use that input channel instead of the   previously used output of the <code>GET_GENOME_FASTA</code> process.</p> </li> <li> <p>Remove the <code>GET_GENOME_FASTA</code> process, as it is not needed anymore.</p> </li> </ul> <p>Re-run the workflow to see if it worked. Check the code below for an example if you're stuck:</p> Click to show the solution <pre><code># Channel creation\n\nch_genome_fasta = Channel.fromPath( \"ftp://ftp.ensemblgenomes.org/pub/bacteria/release-37/fasta/bacteria_18_collection/staphylococcus_aureus_subsp_aureus_nctc_8325/dna/Staphylococcus_aureus_subsp_aureus_nctc_8325.ASM1342v1.dna_rm.toplevel.fa.gz\" )\n\n# Workflow definition\nINDEX_GENOME (\n    ch_genome_fasta\n)\n</code></pre> <p>If we want to get detailed we can also change the hard-coded \"NCT8325\" naming in e.g. the <code>INDEX_GENOME</code> process and put that in another parameter, or grab the <code>baseName()</code> from the channel and make a <code>[prefix, file]</code> tuple using the <code>map{}</code> operator like we did previously; check below if you're curious of how this could be done.</p> Click to show the solution <pre><code>// Channel definition\nch_genome_fasta = Channel\n    .fromPath( \"ftp://ftp.ensemblgenomes.org/pub/bacteria/release-37/fasta/bacteria_18_collection/staphylococcus_aureus_subsp_aureus_nctc_8325/dna/Staphylococcus_aureus_subsp_aureus_nctc_8325.ASM1342v1.dna_rm.toplevel.fa.gz\" )\n    .map{ file -&gt; tuple(file.getBaseName(), file) }\n\n// INDEX_GENOME process definition\nprocess INDEX_GENOME {\n\n    publishDir \"results/intermediate/\",\n        mode: \"copy\"\n\n    input:\n    tuple val(fasta_name), path(fasta)\n\n    output:\n    path(\"*.b2t\"), emit: index\n\n    script:\n    \"\"\"\n    # Bowtie2 cannot use .gz, so unzip to a temporary file first\n    gunzip -c ${fasta} &gt; tempfile\n    bowtie2-build tempfile ${fasta_name}\n    \"\"\"\n}\n</code></pre>"},{"location":"pages/nextflow/nextflow-6-optimising-the-mrsa-workflow/#subworkflows","title":"Subworkflows","text":"<p>The DSL2 allows highly modular workflow design, where a workflow may contain multiple subworkflows. A subworkflow is just like a normal workflow, but it can be called inside other workflows, similar to a process. There is thus no special difference between a subworkflow and a workflow; the only difference is how you use them in practice. Let's take a look at a toy example:</p> <pre><code>workflow {\n    ch_input = Channel.fromPath( params.input )\n    SUBWORKFLOW( ch_input )\n}\n\nworkflow SUBWORKFLOW {\n\n    take:\n    input_file\n\n    main:\n    ALIGN_READS( input_file )\n\n    emit:\n    bam = ALIGN_READS.out.bam\n}\n</code></pre> <p>Here we have an unnamed, main workflow like before, plus a named subworkflow. A workflow can have inputs specified by the <code>take</code> directive, which is the equivalent of process <code>input</code> for workflows. The <code>main</code> part is the workflow body, which contains how to run which processes in which order. The last part, <code>emit</code>, also works the same as for processes, in that we name the different outputs of the workflow so that we may use them in other workflows or processes. Nextflow will run the unnamed workflow by default, unless the <code>-entry</code> flag is specified, like so:</p> <pre><code>nextflow run main.nf -entry SUBWORKFLOW\n</code></pre> <p>This will run the workflow named <code>SUBWORKFLOW</code>, but nothing else. You can also store subworkflows in separate files, so that everything doesn't have to be crammed into a single <code>main.nf</code> file. A subworkflow named <code>SUBWORKFLOW</code> contained in the file <code>subworkflow.nf</code> can be loaded into a <code>main.nf</code> file like so:</p> <pre><code>include { SUBWORKFLOW } from \"./subworkflow.nf\"\n</code></pre> <p>If you have a complex workflow with several subworkflows you might thus store them in a separate directory, e.g. <code>subworkflows</code>. This allows you to have fine-grained control over the general architecture of your Nextflow workflows, organising them in a manner that is easy to code and maintain. A <code>process</code> can also be treated in the same manner, and defined separately in another file.</p> <ul> <li>Now it's your turn! Separate the <code>RUN_FASTQC</code> and <code>RUN_MULTIQC</code> processes out   of the main workflow and into a subworkflow. Check below if you're having   trouble.</li> </ul> Click to show the solution <pre><code>// In the main workflow:\nRUN_QC (\n    GET_SRA_BY_ACCESSION.out\n)\n\n// A new subworkflow\nworkflow RUN_QC {\n\n    take:\n    fastq\n\n    main:\n    RUN_FASTQC (\n        fastq\n    )\n    RUN_MULTIQC (\n        RUN_FASTQC.out.zip.collect()\n    )\n}\n</code></pre> <p>If you want to challenge yourself, try to do the same with the <code>INDEX_GENOME</code>, <code>ALIGN_TO_GENOME</code> and <code>SORT_BAM</code> processes! Be careful of where you get your inputs and outputs.</p> <p>Quick recap</p> <p>In this section we learnt:</p> <ul> <li>How to automatically download remote files</li> <li>How to create and work with subworkflows</li> </ul>"},{"location":"pages/nextflow/nextflow-7-extra-material/","title":"Extra material","text":"<p>There are many more things you can do with Nextflow than covered here. If you are interested to learn more details about Nextflow, we will briefly show some of its advanced features in this section. But first, here are some links to additional resources on Nextflow:</p> <ul> <li>Nextflow patterns that    can help with common operations and concepts</li> <li>The Nextflow documentation</li> <li>Learning Nextflow in 2020</li> <li>Nextflow training at Seqera</li> <li>A work-in-progress Nextflow Carpentry course</li> <li>Community help from Nextflow's Slack channel</li> </ul>"},{"location":"pages/nextflow/nextflow-7-extra-material/#using-containers-in-nextflow","title":"Using containers in Nextflow","text":"<p>Nextflow has built-in support for using both Docker and Singularity containers (and others too), either with a single container for the workflow as a whole or separate containers for each individual process. The simplest way to do it is to have a single container for your entire workflow, in which case you simply run the workflow and specify the image you want to use, like so:</p> <pre><code># Run with docker\nnextflow run main.nf -with-docker [image]\n\n# Run with Singularity\nnextflow run main.nf -with-singularity [image].sif\n</code></pre> <p>If you don't want to supply this at every execution, you can also add it directly to your configuration file:</p> <pre><code># Docker configuration\nprocess.container = 'image'\ndocker.enabled = true\n\n# Singularity configuration\nprocess.container = 'path/to/image.sif'\nsingularity.enabled = true\n</code></pre> <p>If you instead would like to have each process use a different container you can use the <code>container</code> directive in your processes:</p> <pre><code>process PROCESS_01 {\n(...)\ncontainer: 'image_01'\n(...)\n}\n\nprocess PROCESS_02 {\n(...)\ncontainer: 'image_02'\n(...)\n}\n</code></pre> <p>Regardless of which solution you go for, Nextflow will execute all the processes inside the specified container. In practice, this means that Nextflow will automatically wrap your processes and run them by executing the Docker or Singularity command with the image you have provided.</p>"},{"location":"pages/nextflow/nextflow-7-extra-material/#using-conda-in-nextflow","title":"Using Conda in Nextflow","text":"<p>While you can execute Nextflow inside Conda environments just like you would any other type of software, you can also use Conda with Nextflow in the same way as for Docker and Singularity above. You can either supply an <code>environment.yml</code> file, the path to an existing environment or the packages and their versions directly in the <code>conda</code> directive, like so:</p> <pre><code>process PROCESS_01 {\n(...)\nconda: 'mrsa-environment.yml'\n(...)\n}\nprocess PROCESS_02 {\n(...)\nconda: 'path/to/mrsa-env'\n(...)\n}\nprocess PROCESS_03 {\n(...)\nconda: 'bioconda::bwa=0.7.17 bioconda::samtools=1.13'\n(...)\n}\n</code></pre> <p>You can use either of the methods described above with your configuration file as well, here exemplified using an <code>environment.yml</code> file:</p> <pre><code>process.conda = 'mrsa-environment.yml'\n</code></pre>"},{"location":"pages/nextflow/nextflow-7-extra-material/#running-nextflow-on-hpc","title":"Running Nextflow on HPC","text":"<p>A lot of researchers use HPC cluster, which is easily handled by Nextflow.  What you need to do is to add an appropriate profile to your <code>nextflow.config</code> file.  Example for iTrop:</p> <pre><code>profiles {\n\n    // Itrop general profile\n    itrop {\n        process {\n            executor       = 'slurm'\n            clusterOptions = \"-A '${params.account}'\"\n            memory         = { 6.GB * task.attempt }\n            cpus           = { 1 * task.attempt }\n            time           = { 10.h * task.attempt }\n            scratch        = '/scracth/$USER'\n            errorStrategy  = 'retry'\n            maxRetries     = 1\n        }\n    }\n}\n</code></pre> <p>This will add a profile to your workflow, which you can access by running the workflow with <code>-profile itrop</code>. You can leave the profile as-is, unless you want to tinker with e.g. compute resource specifications. That's all you need! Nextflow will take care of communications with SLURM (the system used by iTrop, specified by the <code>executor</code> line) and will send off jobs to the cluster for you, and everything will look exactly the same way as if you were executing the pipeline locally.</p> <p>The <code>memory</code>, <code>cpus</code> and <code>time</code> lines define the various resources Nextflow will use as well as how much to automatically increase them by if re-trying failed tasks; this, in turn, is specified by the <code>errorStrategy</code> and <code>maxRetries</code> variables. The <code>scratch</code> defines where each node's local storage is situated, which gives Nextflow the most optimal access to the cluster file system for temporary files.</p>"},{"location":"pages/nextflow/nextflow-7-extra-material/#advanced-channel-creation","title":"Advanced channel creation","text":"<p>The input data shown in the MRSA example workflow is not that complex, but Nextflow channels can do much more than that. A common scenario in high-throughput sequencing is that you have pairs of reads for each sample. Nextflow has a special, built-in way to create channels for this data type: the <code>fromFilePairs</code> channel factory:</p> <pre><code>Channel\n.fromFilePairs ( \"data/*_R{1,2}.fastq.gz\" )\n.set           { ch_raw_reads }\n</code></pre> <p>This will create a channel containing all the reads in the <code>data/</code> directory in the format <code>&lt;sample&gt;_R1.fastq.gz</code> and <code>&lt;sample&gt;_R2.fastq.gz</code> and will pair them together into a nested tuple looking like this:</p> <pre><code>[sample, [data/sample_R1.fastq.gz, data/sample_R2.fastq.gz]]\n</code></pre> <p>The first element of the tuple (<code>[0]</code>) thus contains the value <code>sample</code>, while the second element (<code>[1]</code>) contains another tuple with paths to both read files. This nested tuple can be passed into processes for e.g. read alignment, and it makes the entire procedure of going from read pairs (i.e. two separate files, one sample) into a single alignment file (one file, one sample) very simple. For more methods of reading in data see the Nextflow documentation on Channel Factories.</p> <p>We can also do quite advanced things to manipuate data in channels, such as this:</p> <pre><code>Channel\n.fromPath ( params.metadata )\n.splitCsv ( sep: \"\\t\", header: true )\n.map      { row -&gt; tuple(\"${row.sample_id}\", \"${row.treatment}\") }\n.filter   { id, treatment -&gt; treatment != \"DMSO\" }\n.unique   (  )\n.set      { samples_and_treatments }\n</code></pre> <p>That's a bit of a handful! But what does it do? The first line specifies that we want to read some data from a file specified by the <code>metadata</code> parameter, and the second line actually reads that data using tab as delimiter, including a header. The <code>map</code> operator takes each entire row and subsets it to only two columns: the <code>sample_id</code> and <code>treatment</code> columns. This subset is stored as a tuple. The <code>filter</code> operator is then used to remove any tuples where the second entry, <code>treatment</code>, is not equal to the string <code>\"DMSO\"</code> (i.e. untreated cells, in this example). We then only take the unique tuples and set the results as the new channel <code>samples_and_treatments</code>. Let's say that this is the metadata we're reading:</p> <pre><code>sample_id     dose    group     treatment\nsample_1      0.1     control   DMSO\nsample_1      1.0     control   DMSO\nsample_1      2.0     control   DMSO\nsample_2      0.1     case      vorinostat\nsample_2      1.0     case      vorinostat\nsample_2      2.0     case      vorinostat \nsample_3      0.1     case      fulvestrant\nsample_3      1.0     case      fulvestrant\nsample_3      2.0     case      fulvestrant\n</code></pre> <p>Given the channel creation strategy above, we would get the following result:</p> <pre><code>[sample_2, vorinostat]\n[sample_3, fulvestrant]\n</code></pre> <p>In this way, you can perform complex operations on input files or input metadata and send the resulting content to your downstream processes in a simple way. Composing data manipuations in Nextflow like this can be half  the fun of writing the workflow. Check out Nextflow's documentation on  Channel operators to see the full list of channel operations at your disposal. </p>"},{"location":"pages/nextflow/nextflow-7-extra-material/#using-groovy-in-processes","title":"Using Groovy in processes","text":"<p>You don't have to use bash or external scripts inside your processes all the time unless you want to: Nextflow is based on Groovy, which allows you to use both Groovy and Bash in the same process. For example, have a look at this:</p> <pre><code>process index_fasta {\ntag \"${fasta_name}\"\n\ninput:\ntuple val(fasta), path(fasta_file)\n\noutput:\npath(\"${fasta_name}.idx\"), emit: fasta\n\nscript:\nfasta_name = fasta.substring(0, fasta.lastIndexOf(\".\"))\n\"\"\"\n    index --ref ${fasta_file},${fasta_name}\n    \"\"\"\n}\n</code></pre> <p>Here we have some command <code>index</code> that, for whatever reason, requires both the path to a FASTA file and the name of that file without the <code>.fasta</code> extension. We can use Groovy in the <code>script</code> directive together with normal Bash, mixing and matching as we like. The first line of the <code>script</code> directive gets the name of the FASTA file without the extension by removing anything after the dot, while the second calls the <code>index</code> command like normal using bash.</p>"},{"location":"pages/nextflow/nextflow-7-extra-material/#the-nf-core-pipeline-collection","title":"The nf-core pipeline collection","text":"<p>You may have heard of the nf-core pipeline collection previously, which is a large, collaborative bioinformatics community dedicated to building, developing and maintaining Nextflow workflows. In fact, if you have sequenced data at e.g. the National Genomics Infrastructure (NGI), you can be sure that the data processing has been run using one of the nf-core pipelines! While the community only started in 2018 (with a Nature Biotechnology paper in 2020), it already has over 30 production-ready pipelines with everything from genomics, transcriptomics, proteomics and metagenomics - and more being developed all the time.</p> <p>The nf-core pipelines all work in the same way, in that they have the same exact base for inputs, parameters and arguments, making them all highly similar to run. Since you've already learnt the basics of Nextflow in this course, you should now be able to also run the nf-core pipelines! It might be that you have a data type that you can analyse using one of the pipelines in nf-core, meaning you don't need to do anything other than find out what parameters you should run it with.</p> <p>Each pipeline comes with extensive documentation, test datasets that you can use to practice on, can be run on both HPCs like iTrop, cloud services like AWS or locally on your own computer. All pipelines support both Conda and Docker/Singularity, and you can additionally run specific versions of the pipelines, allowing for full reproducibility of your analyses. If you want to check nf-core out, simply head over to their list of pipelines and see what's available! Who knows, you might even write your own nf-core pipeline in the future?</p>"},{"location":"pages/nextflow/nextflow-installation/","title":"Setup Nextflow tutorial","text":""},{"location":"pages/nextflow/nextflow-installation/#setup-course-material","title":"Setup course material","text":"Follow this instructions only if you start the course at this stage! Otherwise skip this step! <pre><code>This tutorial depends on files from the course GitHub repo. Please follow these instructions \non how to set it up if you haven't done so already.  \nLet's create a directory and clone the course GitHub repo.\n\n```bash\nmkdir -p  ~/training-reproducible-research-area\ncd ~/training-reproducible-research-area\ngit clone https://github.com/SouthGreenPlatform/training_reproducible_research\n```\n</code></pre>"},{"location":"pages/nextflow/nextflow-installation/#setup-environment","title":"Setup environment","text":"<p>First let's create a dedicated folder for this tutorial:</p> <pre><code>mkdir -p  ~/training-reproducible-research-area/nextflow\ncd ~/training-reproducible-research-area/nextflow\n</code></pre>"},{"location":"pages/nextflow/nextflow-nfcore/","title":"Getting started with nf-core","text":"<p>title: Getting started with nf-core subtitle: Tutorial covering the basics of using nf-core pipelines.</p> <p>Material originally written for the Nextflow Camp 2019, Barcelona 2019-09-19: \"Getting started with nf-core\" (see programme).</p> <p>Duration: 1hr</p> <p>Updated for the nf-core Hackathon 2020, London 2020-03 (see event).</p> <p>Updated for the Elixir workshop on November 2021 (see event).</p> <p>Updated during the March 2022 hackathon.</p> <p>Click here to download the slides associated with this tutorial.</p>"},{"location":"pages/nextflow/nextflow-nfcore/#overview","title":"Overview","text":"<ul> <li>1 - Installation</li> <li>2 - Listing pipelines</li> <li>3 - Running pipelines</li> <li>4 - Troubleshooting pipelines</li> </ul>"},{"location":"pages/nextflow/nextflow-nfcore/#abstract","title":"Abstract","text":"<p>The nf-core community provides a range of tools to help new users get to grips with Nextflow - both by providing complete pipelines that can be used out of the box, and also by helping developers with best practices. Companion tools can create a bare-bones pipeline from a template scattered with <code>TODO</code> pointers and CI with linting tools check code quality. Guidelines and documentation help to get Nextflow newbies on their feet in no time. Best of all, the nf-core community is always on hand to help.</p> <p>In this tutorial, we discuss the best-practice guidelines developed by the nf-core community, why they're important and give insight into the best tips and tricks for budding Nextflow pipeline users. \u2728</p>"},{"location":"pages/nextflow/nextflow-nfcore/#introduction","title":"Introduction","text":""},{"location":"pages/nextflow/nextflow-nfcore/#what-is-nf-core","title":"What is nf-core","text":"<p>nf-core is a community-led project to develop a set of best-practice pipelines built using Nextflow. Pipelines are governed by a set of guidelines, enforced by community code reviews and automatic linting (code testing). A suite of helper tools aim to help people run and develop pipelines.</p>"},{"location":"pages/nextflow/nextflow-nfcore/#what-this-tutorial-will-cover","title":"What this tutorial will cover","text":"<p>This tutorial attempts to give an overview of how nf-core works:</p> <ul> <li>What are the most commonly used nf-core tools.</li> <li>Listing pipelines available in the nf-core project.</li> <li>How to run nf-core pipelines.</li> <li>How to troubleshoot nf-core pipelines.</li> </ul>"},{"location":"pages/nextflow/nextflow-nfcore/#where-to-get-help","title":"Where to get help","text":"<p>The beauty of nf-core is that there is lots of help on offer! The main place for this is Slack - an instant messaging service.</p> <p>The nf-core Slack can be found at https://nfcore.slack.com (NB: no hyphen in <code>nfcore</code>!). To join you will need an invite, which you can get at https://nf-co.re/join/slack.</p> <p>The nf-core Slack organisation has channels dedicated for each pipeline, as well as specific topics (eg. #help, #pipelines, #tools, #configs and much more).</p> <p>One additional tool which we like a lot is TLDR - it gives concise command line reference through example commands for most linux tools, including <code>nextflow</code>, <code>docker</code>, <code>singularity</code>, <code>conda</code>, <code>git</code> and more. There are many clients, but raylee/tldr is arguably the simplest - just a single bash script.</p>"},{"location":"pages/nextflow/nextflow-nfcore/#installing-the-nf-core-helper-tools","title":"Installing the nf-core helper tools","text":"<p>Much of this tutorial will make use of the <code>nf-core</code> command line tool. This has been developed to provide a range of additional functionality for the project such as pipeline creation, testing and more.</p> <p>The <code>nf-core</code> tool is written in Python and is available from the Python Package Index and Bioconda. You can install the latest released version from PyPI as follows:</p> <pre><code>pip install nf-core\n</code></pre> <p>Or this command to install the <code>dev</code> version:</p> <pre><code>pip install --upgrade --force-reinstall git+https://github.com/nf-core/tools.git@dev\n</code></pre> <p>If using conda, first set up Bioconda as described in the bioconda docs (especially setting the channel order), create and activate an environment and then install nf-core:</p> <pre><code>conda install nf-core\n</code></pre> <p>To update the package you can run the following command</p> <pre><code>conda update nf-core\n</code></pre> <p>The nf-core/tools source code is available at https://github.com/nf-core/tools - if you prefer, you can clone this repository and install the code locally:</p> <pre><code>git clone https://github.com/nf-core/tools.git nf-core-tools\ncd nf-core-tools\npython setup.py install\n</code></pre> <p>Once installed, you can check that everything is working by printing the help:</p> <pre><code>nf-core --help\n</code></pre> <p>You will also need to install Prettier for formatting your code. To do so, you can either use the following command with conda:</p> <pre><code>conda install prettier\n</code></pre> <p>Or use the Visual Studio Code extension Prettier also available in the pack of useful extension NF-core.</p> <p>Besides, you can also add a comment with <code>@nf-core-bot fix linting</code> in your Pull Request and prettier will be used to apply the required fixes to your code.</p>"},{"location":"pages/nextflow/nextflow-nfcore/#exercise-1-installation","title":"Exercise 1 (installation)","text":"<ul> <li>Install nf-core/tools</li> <li>Use the help flag to list the available commands</li> </ul>"},{"location":"pages/nextflow/nextflow-nfcore/#listing-available-nf-core-pipelines","title":"Listing available nf-core pipelines","text":"<p>As you saw from the <code>--help</code> output, the tool has a range of sub-commands. The simplest is <code>nf-core list</code>, which lists all available nf-core pipelines. The output shows the latest version number, when that was released. If the pipeline has been pulled locally using Nextflow, it tells you when that was and whether you have the latest version.</p> <p>If you supply additional keywords after the command, the listed pipelines will be filtered. Note that this searches more than just the displayed output, including keywords and description text. The <code>--sort</code> flag allows you to sort the list (default is by most recently released) and <code>--json</code> returns the complete list, without any filtering, in JSON output for programmatic use.</p> <p>The nf-core pipelines currently available and under development are also listed on the nf-core website, in the pipelines page.</p>"},{"location":"pages/nextflow/nextflow-nfcore/#exercise-2-listing-pipelines","title":"Exercise 2 (listing pipelines)","text":"<ul> <li>Use the help flag to print the list command usage</li> <li>List all available nf-core pipelines</li> <li>Sort pipelines alphabetically, then by popularity (stars)</li> <li>Fetch one of the pipelines using <code>nextflow pull</code></li> <li>Use <code>nf-core list</code> to see if the pipeline you pulled is up to date</li> <li>Filter pipelines for those that work with RNA</li> <li>Save these pipeline details to a JSON file</li> </ul>"},{"location":"pages/nextflow/nextflow-nfcore/#running-nf-core-pipelines","title":"Running nf-core pipelines","text":""},{"location":"pages/nextflow/nextflow-nfcore/#software-requirements-for-nf-core-pipelines","title":"Software requirements for nf-core pipelines","text":"<p>In order to run nf-core pipelines, you will need to have Nextflow installed (https://www.nextflow.io). The only other requirement is a software packaging tool: Conda, Docker or Singularity. In theory it is possible to run the pipelines with software installed by other methods (e.g. environment modules, or manual installation), but this is not recommended. Most people find either Docker or Singularity containers the best options, as conda environments cannot guarantee 100% reproducibility.</p>"},{"location":"pages/nextflow/nextflow-nfcore/#fetching-pipeline-code","title":"Fetching pipeline code","text":"<p>Unless you are actively developing pipeline code, we recommend using the Nextflow built-in functionality to fetch nf-core pipelines. Nextflow will automatically fetch the pipeline code when you run <code>nextflow run nf-core/PIPELINE</code>. For the best reproducibility, it is good to explicitly reference the pipeline version number that you wish to use with the <code>-revision</code>/<code>-r</code> flag. For example:</p> <pre><code>nextflow run nf-core/rnaseq -revision 3.4\n</code></pre> <p>If not specified, Nextflow will fetch the default branch. For pipelines with a stable release this the default branch is <code>master</code> - this branch contains code from the latest release. For pipelines in early development that don't have any releases, the default branch is <code>dev</code>.</p> <p>If you would like to run the latest development code, use <code>-r dev</code>.</p> <p>Note that once pulled, Nextflow will use the local cached version for subsequent runs. Use the <code>-latest</code> flag when running the pipeline to always fetch the latest version. Alternatively, you can force Nextflow to pull a pipeline again using the <code>nextflow pull</code> command:</p> <pre><code>nextflow pull nf-core/rnaseq -revision 3.4\n</code></pre>"},{"location":"pages/nextflow/nextflow-nfcore/#usage-instructions-and-documentation","title":"Usage instructions and documentation","text":"<p>You can find general documentation and instructions for Nextflow and nf-core on the nf-core website: https://nf-co.re/. Pipeline-specific documentation is bundled with each pipeline in the <code>/docs</code> folder. This can be read either locally, on GitHub, or on the nf-core website. Each pipeline has its own webpage at <code>https://nf-co.re/&lt;pipeline_name&gt;</code> (e.g. nf-co.re/rnaseq), including <code>Usage</code> documentation, <code>Output</code> documentation and <code>Parameter</code> documentation.</p> <p>In addition to this documentation, each pipeline comes with basic command line reference. This can be seen by running the pipeline with the <code>--help</code> flag, for example:</p> <pre><code>nextflow run nf-core/rnaseq --help\n</code></pre> <p>Example results of a pipeline run on full-sized test data can be browsed on the pipeline page, under the <code>aws</code> results tab.</p>"},{"location":"pages/nextflow/nextflow-nfcore/#config-profiles","title":"Config profiles","text":"<p>Nextflow can load pipeline configurations from multiple locations. To make it easy to apply a group of options on the command line, Nextflow uses the concept of config profiles. nf-core pipelines load configuration in the following order:</p> <ol> <li>Pipeline: Default 'base' config</li> <li>Always loaded. Contains pipeline-specific parameters and \"sensible defaults\" for things like computational requirements</li> <li>Does not specify any method for software packaging. If nothing else is specified, Nextflow will expect all software to be available on the command line.</li> <li>Pipeline: Core config profiles</li> <li>All nf-core pipelines come with some generic config profiles. The most commonly used ones are for software packaging: <code>docker</code>, <code>singularity</code> and <code>conda</code>. To ensure reproducibility across different compute infrastructures, it is recommended to use containers instead of conda environments.</li> <li>Other core profiles are <code>debug</code> and <code>test</code></li> <li>nf-core/configs: Server profiles</li> <li>At run time, nf-core pipelines fetch configuration profiles from the configs remote repository. The profiles here are specific to clusters at different institutions.</li> <li>Because this is loaded at run time, anyone can add a profile here for their system and it will be immediately available for all nf-core pipelines.</li> <li>Personal configuration under <code>~/.nextflow/config</code>.</li> <li>Local config files given to Nextflow with the <code>-c</code> flag.</li> <li>Command line configuration.</li> </ol> <p>Multiple comma-separate config profiles can be specified in one go, so the following commands are perfectly valid:</p> <pre><code>nextflow run nf-core/rnaseq -profile test,docker\nnextflow run nf-core/rnaseq -profile singularity,debug\n</code></pre> <p>Note that the order in which config profiles are specified matters. Their priority increases from left to right.</p> <p>Our tip: Be clever with multiple Nextflow configuration locations. For example, use <code>-profile</code> for your cluster configuration, <code>~/.nextflow/config</code> for your personal config such as <code>params.email</code> and a working directory <code>config</code> (e.g. <code>custom.config</code> provided to the run with <code>-c custom.config</code>) file for reproducible run-specific configuration.</p> <p>To know more about Nextflow configurations you can check the pipeline configuration tutorial.</p>"},{"location":"pages/nextflow/nextflow-nfcore/#running-pipelines-with-test-data","title":"Running pipelines with test data","text":"<p>The <code>test</code> config profile is a bit of a special case. Whereas all other config profiles tell Nextflow how to run on different computational systems, the <code>test</code> profile configures each <code>nf-core</code> pipeline to run without any other command line flags. It specifies URLs for test data and all required parameters. Because of this, you can test any nf-core pipeline with the following command:</p> <pre><code>nextflow run nf-core/&lt;pipeline_name&gt; -profile test --outdir &lt;OUTDIR&gt;\n</code></pre> <p>Note that you will typically still need to combine this with a configuration profile for your system - e.g. <code>-profile test,docker</code>. Running with the test profile is a great way to confirm that you have Nextflow configured properly for your system before attempting to run with real data.</p>"},{"location":"pages/nextflow/nextflow-nfcore/#the-nf-core-launch-command","title":"The nf-core launch command","text":"<p>Most nf-core pipelines have a number of flags that need to be passed on the command line: some mandatory, some optional. To make it easier to launch pipelines, these parameters are described in a JSON file bundled with the pipeline. The <code>nf-core launch</code> command uses this to build an interactive command-line wizard which walks through the different options with descriptions of each, showing the default value and prompting for values.</p> <p>Once all prompts have been answered, non-default values are saved to a <code>params.json</code> file which can be supplied to Nextflow to run the pipeline. Optionally, the Nextflow command can be launched there and then.</p> <p>To use the launch feature, just specify the pipeline name:</p> <pre><code>nf-core launch &lt;pipeline_name&gt;\n</code></pre>"},{"location":"pages/nextflow/nextflow-nfcore/#using-nf-core-pipelines-offline","title":"Using nf-core pipelines offline","text":"<p>Many of the techniques and resources described above require an active internet connection at run time - pipeline files, configuration profiles and software containers are all dynamically fetched when the pipeline is launched. This can be a problem for people using secure computing resources that do not have connections to the internet.</p> <p>To help with this, the <code>nf-core download</code> command automates the fetching of required files for running nf-core pipelines offline. The command can download a specific release of a pipeline with <code>-r</code>/<code>--release</code> and fetch the singularity container if <code>--singularity</code> is passed (this needs Singularity to be installed). All files are saved to a single directory, ready to be transferred to the cluster where the pipeline will be executed.</p> <p>To know more about running pipelines offline you can check the pipeline configuration tutorial.</p>"},{"location":"pages/nextflow/nextflow-nfcore/#exercise-3-using-pipelines","title":"Exercise 3 (using pipelines)","text":"<ul> <li>Install required dependencies (Nextflow, Docker)</li> <li>Print the command-line usage instructions for the nf-core/rnaseq pipeline</li> <li>In a new directory, run the nf-core/rnaseq pipeline with the provided test data</li> <li>Try launching the RNA pipeline using the <code>nf-core launch</code> command</li> <li>Download the nf-core/rnaseq pipeline for offline use using the <code>nf-core download</code> command</li> </ul>"},{"location":"pages/nextflow/nextflow-nfcore/#troubleshooting-nf-core-pipelines","title":"Troubleshooting nf-core pipelines","text":"<p>Not everything always runs smoothly and you might be getting some errors when running nf-core pipelines. Here are some step-by-step tips that can help you troubleshoot your errors.</p> <ol> <li>Start small: each nf-core pipeline comes with small test data that are checked by continuous integration and for each pipeline release.</li> <li>Start by running the pipeline tests as described above. If these tests fail, there is a good chance that you are missing some of the components needed to run Nextflow pipelines.</li> <li>Nextflow: check that you have the latest version installed.</li> <li>Check that you have docker/singularity/conda installed and that you are using the right docker/singularity/conda/custom profile.</li> <li>Check the troubleshooting docs.</li> <li>Categorize the type of error. Check the Nextflow low to figure out if the error occurs:</li> <li>Before the first process</li> <li>In the first process</li> <li>During the pipeline run</li> <li>Problems with the process output</li> <li>Read the Nextflow log. Check the work directory for the <code>.command.err</code> or <code>.command.log</code> files for more information.</li> <li>Search the nf-core slack, google. Ask for help in the corresponding nf-core slack channel.</li> <li>Report a pipeline bug on the nf-core GitHub if none of the above steps helps.</li> </ol> <p>Here is a bytesize talk with a step by step explanation on how to troubleshoot failing pipelines.</p>"},{"location":"pages/nextflow/nextflow-nfcore/#conclusion","title":"Conclusion","text":"<p>We hope that this nf-core tutorial has been helpful! Remember that there is more in-depth documentation on many of these topics available on the nf-core website. If in doubt, please ask for help on Slack.</p> <p>If you have any suggestions for how to improve this tutorial, or spot any mistakes, please create an issue or pull request on the nf-core/nf-co.re repository.</p> <p>Phil Ewels, Maxime Garcia, Gisela Gabernet, Friederike Hanssen for nf-core, March 2022</p>"},{"location":"pages/notebook/notebook-context/","title":"Context","text":"<p>There are typical guidelines for keeping a notebook of wet-lab work :</p> <ol> <li>Record everything you do in the lab, even if you are following a published procedure.</li> <li>If you make a mistake, put a line through the mistake and write the new information next to it.</li> <li>Use a ball point pen so that marks will not smear nor will they be erasable.</li> <li>Use a bound notebook so that tear-out would be visible.</li> <li>When you finish a page, put a corner-to corner line through any blank parts that could still be used for data entry.</li> <li>All pages must be pre-numbered.</li> <li>Write a title for each and every new set of entries.</li> <li>It is critical that you enter all procedures and data directly into your notebook in a timely manner.</li> <li>Properly introduce and summarize each experiment.</li> <li>The investigator and supervisor must sign each page.</li> </ol> <p>But for dry-lab work... </p> <p>The concept of the notebook is not new, as these quotes show.</p> <p>Literate programming</p> <p>Instead of imagining that our main task is to instruct a computer what to do, let us concentrate rather on explaining to human beings what we want a computer to do. - Donald Knuth (1984)</p> <p>Literate computing</p> <p>A literate computing environment is one that allows users not only to execute commands interactively, but also to store in a literate document the results of these commands along with figures and free-form text.   - Millman KJ and Perez F (2014)</p> <p>And the concept was already in place in the 1980s</p> <p> </p> Wolfram Mathematica notebook (1987) <p>This notebook concept has evolved since then and in this course we're going to introduce you to three different notebooks: </p> <ul> <li>Rmarkdown</li> <li>Jupyter</li> <li>Quarto</li> </ul> <p> </p> Appearance of packages allowing the creation of notebooks (source)"},{"location":"pages/notebook/notebook-markdown/","title":"Markdown","text":""},{"location":"pages/notebook/notebook-markdown/#a-markup-language","title":"A markup languageSub-heading","text":"<p>A markup language is a system for annotating text documents in order to e.g. define formatting. HTML, if you are familiar with that, is an example of a markup language. HTML uses tags, such as:</p> Example in HTML<pre><code>&lt;h1&gt;Heading&lt;/h1&gt;\n&lt;h2&gt;Sub-heading&lt;/h2&gt;\n&lt;a href=\"www.webpage.com\"&gt;Link&lt;/a&gt;\n&lt;ul&gt;\n  &lt;li&gt;List-item1&lt;/li&gt;\n  &lt;li&gt;List-item2&lt;/li&gt;\n  &lt;li&gt;List-item3&lt;/li&gt;\n&lt;/ul&gt;\n</code></pre> <p>and this is the result :</p> Heading <p>Link</p> <ul> <li>List-item1</li> <li>List-item2</li> <li>List-item3</li> </ul>"},{"location":"pages/notebook/notebook-markdown/#markdown","title":"Markdown","text":"<p>Markdown is a lightweight markup language which uses plain-text syntax in order to be as unobtrusive as possible, so that a human can easily read it. The code below gives the same result as the HTML code shown above : </p> Example in markdown<pre><code># Heading\n\n## Sub-heading\n\n### Another deeper heading\n\nA [link](http://example.com).\n\nText attributes _italic_, *italic*, **bold**, `monospace`.\n\nBullet list:\n\n  * apples\n  * oranges\n  * pears\n</code></pre> <p>A markdown document can be converted to other formats, such as HTML or PDF, for viewing in a browser or a PDF reader. For example, the page you are reading right now is written in markdown. Markdown is somewhat ill-defined, and as a consequence of that there exist many implementations and extensions, although they share most of the syntax. R Markdown is one such implementation/extension.</p> <p>A large number of sites give you the full syntax of Markdonw. Below is a cheat sheet  proposed by GitHUb : </p>"},{"location":"pages/project/project-1-review/","title":"Review","text":"By working reproducibly you will also make your life a lot easier!"},{"location":"pages/project/project-1-review/#what-have-we-learned","title":"What have we learned?","text":"<ul> <li>How to use the version control system Git to track changes to code</li> <li>How to use the package and environment manager Conda</li> <li>How to use the workflow managers Snakemake and Nextflow</li> <li>How to use R Markdown and Jupyter to generate automated reports and to document your analyses</li> <li>How to use Docker and Singularity to distribute containerized computational environments</li> </ul>"},{"location":"pages/project/project-2-good_praticies/","title":"Good praticies","text":""},{"location":"pages/project/project-2-good_praticies/#divide-your-work-into-distinct-projects","title":"Divide your work into distinct projects","text":"<ul> <li>Keep all files needed to go from raw data to final results in a dedicated directory</li> <li>Use relevant subdirectories</li> <li>Many software support the \u201cproject way of working\u201d, e.g. Rstudio and the text editors Sublime Text and Atom</li> <li>Use Git to create structured and version controlled project repositories</li> </ul> <p>Everything can be a project ! For example, you can find a project directory tempalte proposed by NBIS (repo):</p> <pre><code>project\n|- doc/                documentation for the study\n|\n|- data/               raw and primary data, essentially all input files, never edit!\n|  |- raw_external/\n|  |- raw_internal/\n|  |- meta/\n|\n|- code/               all code needed to go from input files to final results\n|- notebooks/\n|\n|- intermediate/       output files from different analysis steps, can be deleted\n|- scratch/            temporary files that can be safely deleted or lost\n|- logs/               logs from the different analysis steps\n|\n|- results/            output from workflows and analyses\n|  |- figures/\n|  |- tables/\n|  |- reports/\n|\n|- .gitignore          sets which parts of the repository that should be git tracked\n|- Snakefile           project workflow, carries out analysis contained in code/\n|- config.yml          configuration of the project workflow\n|- environment.yml     software dependencies list, used to create a project environment\n|- Dockerfile          recipe to create a project container\n</code></pre> <p>An other proposition: </p> <p> </p> Data management tips by Kira H\u00f6ffler (source)"},{"location":"pages/project/project-2-good_praticies/#files-naming-and-format","title":"Files: naming and format","text":""},{"location":"pages/project/project-2-good_praticies/#naming","title":"Naming","text":"<ul> <li>Brief and to the point</li> <li>No spaces or special characters</li> <li>With the right date format</li> <li>With the most important element first</li> <li>With the document version</li> </ul> Date format (source)"},{"location":"pages/project/project-2-good_praticies/#format","title":"Format","text":"<ul> <li>Non-proprietary if possible</li> <li>Formats that lose the least data on conversion</li> <li>The format used by the community</li> </ul>"},{"location":"pages/project/project-2-good_praticies/#write-a-good-read-me","title":"Write a good READ ME","text":"<p>An other example from Recheche data gouv (source)</p> <pre><code>&lt;Help text is included in angle brackets and should be deleted before saving.&gt; \n\n&lt;A README: Why ?\n\n***The documentation of a dataset should be sufficient to enable any reuser to understand and evaluate its quality. The README provides complementary and accessible information that is not provided through the dataset\u2019s metatada, and its file\u2019s metadata, and/or associated files, or files that are publicly accessible on a long term hosting service (file storage or publication). In the latter case, please include the documents\u2019 persistent URLs or their references***.&gt;\n\n&lt;Prefer text document (.txt,) or markdown (.md) as file format&gt;\n\nRDG README File Template --- General --- Version: 0.1 (2022-10-04) \n\nThis README file was generated on [YYYY-MM-DD] by [NAME].\nLast updated: [YYYY-MM-DD].\n\n# GENERAL INFORMATION\n\n## Dataset title:\n\n## DOI:\n\n## Contact email:\n\n&lt;Here is a list of suggested items to help you enrich your documentation if necessary. Some may not be applicable, depending on the dataset\u2019s discipline or context of production.&gt;\n\n&lt;***Remove or add any section if applicable***&gt;\n\n# METHODOLOGICAL INFORMATION \n\n## Environmental/experimental conditions: \n\n## Description of sources and methods used to collect and generate data:\n&lt;If applicable, describe standards, calibration information, facility instruments, etc. &gt; \n\n## Methods for processing the data: \n&lt; If applicable, describe how submitted data were processed and include details that may be important for data reuse or replication. Add comments to explain each step taken.\nFor example, include data cleaning and analysis methods; code and/or algorithms, de-identification procedures for sensitive data human subjects or endangered species data.&gt; \n\n## Quality -assurance procedures performed on the data: \n\n## Other contextual information:\n&lt;Any information that you consider important for the evaluation of the dataset\u2019s quality and reuse thereof: for example, information about the software required to interpret the data. \nIf applicable and not covered above, include full name and version of software, and any necessary packages or libraries needed to read and interpret the data, *e.g.* to run scripts.&gt;\n\n\n# DATA &amp; FILE OVERVIEW\n\n\n## File naming convention:\n\n## File hierarchy convention:\n\n\n# DATA-SPECIFIC INFORMATION FOR: [FILENAME]\n\n&lt;Repeat this section for each folder or file, as appropriate. Recurring items may also be explained in a common initial section.&gt;\n\n&lt;For tabular data, provide a data dictionary/code book containing the following information:&gt;\n## Variable/Column List:\nFor each variable or column name, provide: \n\n    -- full \u201chuman readable\u201d name of the variable, \n    -- description of the variable, \n    -- unit of measurement if applicable, \n    -- decimal separator *i.e.* comma or point if applicable\n    -- allowed values : list of values or range, or domain\n    -- format if applicable, e.g. date&gt;\n\n## Missing data codes: \n&lt;Define codes or symbols used to indicate missing data.&gt;\n\n## Additional information: \n&lt;Any relevant information you consider useful to better understand the file&gt;\n</code></pre>"},{"location":"pages/project/project-2-good_praticies/#what-to-do-with-the-different-data","title":"What to do with the different data ?","text":"<p>You must keep your input data read-only - consider it static. Don't create different versions of the input data - write a script, R Markdown document, Jupyter notebook or a Snakemake / Nextflow workflow if you need to preprocess your input data so that the steps can be recreated. Of course, BACKUP your data! Keep redundant copies in different physical locations (3-2-1 backup rule).</p> <p> </p> 3-2-1 Backup rule (source) <p>And of course in an open science &amp; FAIR approach, upload your raw data as soon as possible to a public data repository</p>"},{"location":"pages/project/project-2-good_praticies/#what-is-reasonable-for-your-project","title":"What is reasonable for your project?","text":"<p>The minimal level is to write code in a reproductible way. You can connect your results with the code (R markdown, Juyter notebooks, ...) and why not convert your code into a Snakemake / Nexflow workflow.</p> <p>The good level is to add the code versionning. You can use Git for versionning and collaboration and publish your code along with your results on GitHub, GitLab,...</p> <p>A better level would be to add the organisation of software dependencies. You can use conda  to install software in environments that can be easily exported and installed on a different system.</p> <p>The best level would be to add an export of everything. You can completly recreate he compute system with docker or singularity container.</p>"},{"location":"pages/project/project-2-good_praticies/#alternatives","title":"Alternatives","text":"<p>There are of course alternatives to all the tools presented in this course such as: </p> Presented Alternative Git Mecurial (Distributed model just like Git, close to sourceforge.), Subversion (Centralized model unlike git/mercurial; no local repository on your computer and somewhat easier to use) Conda Pip (Package manager for Python, has a large repository at pypi.), Apt/yum/brew (Native package managers for different OS. Integrated in OS and might deal with e.g. update notifications better), virtualenv (Environment manager used to set up semi-isolated python environments) Conda Make (Used in software development and has been around since the 70s. Flexible but notoriously obscure syntax.), Galaxy (attempts to make computational biology accessible to researchers without programming experience by using a GUI.) Jupyter, R markdown, Quarto Zeppelin (Developed by Apache. Closely integrated with Spark for distributed computing and Big Data applications.), Beaker (Newcomer based on Ipython, just as Jupyter. Has a focus on integrating multiple languages in the same notebook.) Docker, Sigularity Shifter (Similar ambition as Singularity, but less focus on mobility and more on resource management), VirtualBox/VMWare (Virtualization rather than containerization. Less lightweight, but no reliance on host kernel.)"},{"location":"pages/project/project-2-good_praticies/#in-conclusion-whats-in-it-for-me","title":"In conclusion, what's in it for me ?","text":""},{"location":"pages/project/project-3-and_now/","title":"And now ?","text":""},{"location":"pages/project/project-3-and_now/#create-a-project-from-scratch","title":"Create a project from scratch !","text":"<p>It is time to try to set up a project from scratch and use the different tools that we have covered during the course together! This exercise is very open-ended and you have free hands to try out a bit of what you want. But you should aim to use what you've learned to do the following:</p> <ol> <li> <p>Create a new git repository for the project (either on BitBucket or GitHub)</p> </li> <li> <p>Add a README file which should contain the required information on how to    run the project</p> </li> <li> <p>Create a Conda <code>environment.yml</code> file with the required dependencies</p> </li> <li> <p>Create a R Markdown or Jupyter notebook to run your code</p> </li> <li> <p>Alternatively, create a <code>Snakefile</code> to run your code as a workflow and use a <code>config.yml</code> file to add settings to the workflow</p> </li> <li> <p>Use git to continuously commit changes to the repository</p> </li> <li> <p>Possibly make a Docker or Singularity image for your project</p> </li> </ol> <p>This is not a small task and may seem overwhelming! Don't worry if you feel lost or if the task seems daunting. To get the most out of the exercise, take one step at a time and go back to the previous tutorials for help and inspiration. The goal is not necessarily for you to finish the whole exercise, but to really think about each step and how it all fits together in practice.</p> <p>Recommendation</p> <p>We recommend to start with git, Conda and a notebook, as we would see these as the core tools to make a research project reproducible. We suggest to keep the analysis for this exercise short so that you have time to try out the different tools together while you have the opportunity to ask for help.</p>"},{"location":"pages/project/project-3-and_now/#your-own-project","title":"Your own project","text":"<p>This is a great opportunity for you to try to implement these methods on one of your current research projects. It is of course up to you which tools to include in making your research project reproducible, but we suggest to aim for at least git and Conda.</p> <p>Tip</p> <p>If your analysis project contains computationally intense steps it may be good to scale them down for the sake of the exercise. You might, for example, subset your raw data to only contain a minuscule part of its original size. You can then test your implementation on the subset and only run it on the whole dataset once everything works to your satisfaction.</p>"},{"location":"pages/project/project-3-and_now/#alternative-student-experience-project","title":"Alternative: student experience project","text":"<p>If you don't want to use a project you're currently working on we have a suggestion for a small-scale project for you. The idea is to analyze students' experiences at this Reproducible Research course. For this you will use responses from students to the registration form for the course. Below you'll find links to files in <code>*.csv</code> format with answers from 3 course instances:</p> <p>2018-11 https://docs.google.com/spreadsheets/d/1yLcJL-rIAO51wWCPrAdSqZvCJswTqTSt4cFFe_eTjlQ/export?format=csv 2019-05 https://docs.google.com/spreadsheets/d/1mBp857raqQk32xGnQHd6Ys8oZALgf6KaFehfdwqM53s/export?format=csv  2019-11 https://docs.google.com/spreadsheets/d/1aLGpS9WKvmYRnsdmvvgX_4j9hyjzJdJCkkQdqWq-uvw/export?format=csv </p> <p>The goal here is to create a Snakemake workflow, which contains the following:</p> <ol> <li> <p>Has a rule that downloads the <code>csv</code> files (making use of a <code>config.yml</code> file    to pass the URLs and file names)</p> </li> <li> <p>Has a rule that cleans the files (making use of <code>wildcards</code> so that the same    rule can be run on each file)</p> </li> <li> <p>The final step is to plot the student experience in some way</p> </li> </ol> <p>Remember to</p> <ul> <li>Keep everything versioned controlled with <code>git</code></li> <li>Add information to the <code>README</code> file so others know how to re-run   the project</li> <li>Add required software to the Conda <code>environment.yml</code> file</li> </ul>"},{"location":"pages/project/project-3-and_now/#inspiration-and-tips-for-the-student-experience-workflow","title":"Inspiration and tips for the student experience workflow","text":"<p>The first two steps should be part of the Snakemake workflow. If you need some help with the cleaning step, see below for a Python script that you can save to a file and run in the second Snakemake rule.</p> Click to show a script for cleaning column names <p>The script (e.g. <code>clean_csv.py</code>):</p> <pre><code>#!/usr/bin/env python\nimport pandas as pd\nfrom argparse import ArgumentParser\n\ndef main(args):\n    df = pd.read_csv(args.input, header=0)\n    df.rename(columns=lambda x: x.split(\"[\")[-1].rstrip(\"]\"), inplace=True)\n    df.rename(columns={'R Markdown': 'RMarkdown'}, inplace=True)\n    df.to_csv(args.output, index=False)\n\nif __name__ == '__main__':\n    parser = ArgumentParser()\n    parser.add_argument(\"input\", type=str,\n                        help=\"Input csv file\")\n    parser.add_argument(\"output\", type=str,\n                        help=\"Output csv file cleaned\")\n    args = parser.parse_args()\n    main(args)\n</code></pre> <p>Command to execute the script:</p> <pre><code>python clean_csv.py input_file.csv output_file.csv\n</code></pre> <p>The third step is really up to you how to implement. You could:</p> <ul> <li>Include the plotting in the workflow using an RMarkdown document that   gets rendered into a report</li> <li>Have a script that produces separate figures (e.g. <code>png</code> files)</li> <li>Create a jupyter notebook that reads the cleaned output from the workflow   and generates some plot or does other additional analyses</li> </ul> <p>If you need some help/inspiration with plotting the results, click below to see an example Python script that you can save to file and run with the cleaned files as input.</p> Click to show a script for plotting the student experience <p>The script (e.g. <code>plot.py</code>):</p> <pre><code>#!/usr/bin/env python\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nmpl.use('agg')\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nfrom argparse import ArgumentParser\n\ndef read_files(files):\n\"\"\"Reads experience counts and concatenates into one dataframe\"\"\"\n    df = pd.DataFrame()\n    for i, f in enumerate(files):\n        # Extract date\n        d = f.split(\".\")[0]\n        _df = pd.read_csv(f, sep=\",\", header=0)\n        # Assign date\n        _df = _df.assign(Date=pd.Series([d]*len(_df), index=_df.index))\n        if i==0:\n            df = _df.copy()\n        else:\n            df = pd.concat([df,_df], sort=True)\n    return df.reset_index().drop(\"index\",axis=1).fillna(0)\n\ndef count_experience(df, normalize=False):\n\"\"\"Generates long format dataframe of counts\"\"\"\n    df_l = pd.DataFrame()\n    for software in df.columns:\n        if software==\"Date\":\n            continue\n        # Groupby software and count\n        _df = df.groupby([\"Date\",software]).count().iloc[:,0].reset_index()\n        _df.columns = [\"Date\",\"Experience\",\"Count\"]\n        _df = _df.assign(Software=pd.Series([software]*len(_df),\n            index=_df.index))\n        if normalize:\n            _df = pd.merge(_df.groupby(\"Date\").sum().rename(columns={'Count':'Tot'}),_df, left_index=True, right_on=\"Date\")\n            _df.Count = _df.Count.div(_df.Tot)*100\n            _df.rename(columns={'Count': '%'}, inplace=True)\n        df_l = pd.concat([df_l, _df], sort=True)\n    df_l.loc[df_l.Experience==0,\"Experience\"] = np.nan\n    return df_l\n\n\ndef plot_catplot(df, outdir, figname, y, palette=\"Blues\"):\n\"\"\"Plot barplots of user experience per software\"\"\"\n    ax = sns.catplot(data=df, x=\"Date\", col=\"Software\", col_wrap=3, y=y,\n        hue=\"Experience\", height=2.8,\n                     kind=\"bar\",\n                     hue_order=[\"Never heard of it\",\n                                \"Heard of it but haven't used it\",\n                                \"Tried it once or twice\", \"Use it\"],\n                     col_order=[\"Conda\", \"Git\", \"Snakemake\", \"Jupyter\",\n                                \"RMarkdown\", \"Docker\", \"Singularity\"],\n                     palette=palette)\n    ax.set_titles(\"{col_name}\")\n    plt.savefig(\"{}/{}\".format(outdir, figname), bbox_to_inches=\"tight\",\n        dpi=300)\n    plt.close()\n\ndef plot_barplot(df, outdir, figname, x):\n\"\"\"Plot a barplot summarizing user experience over all software\"\"\"\n    ax = sns.barplot(data=df, hue=\"Date\", y=\"Experience\", x=x, errwidth=.5,\n                order=[\"Never heard of it\",\n                       \"Heard of it but haven't used it\",\n                       \"Tried it once or twice\", \"Use it\"])\n    plt.savefig(\"{}/{}\".format(outdir, figname), bbox_inches=\"tight\",\n        dpi=300)\n    plt.close()\n\ndef main(args):\n    # Read all csv files\n    df = read_files(args.files)\n    # Count experience\n    df_l = count_experience(df)\n    # Count and normalize experience\n    df_lp = count_experience(df, normalize=True)\n    # Plot catplot of student experience\n    plot_catplot(df_l, args.outdir, \"exp_counts.png\", y=\"Count\")\n    # Plot catplot of student experience in %\n    plot_catplot(df_lp, args.outdir, \"exp_percent.png\", y=\"%\",\n                 palette=\"Reds\")\n    # Plot barplot of experience\n    plot_barplot(df_lp, args.outdir, \"exp_barplot.png\", x=\"%\")\n\nif __name__ == '__main__':\n    parser = ArgumentParser()\n    parser.add_argument(\"files\", nargs=\"+\",\n        help=\"CSV files with student experience to produce plots for\")\n    parser.add_argument(\"--outdir\", type=str, default=\".\",\n        help=\"Output directory for plots (defaults to current directory)\")\n    args = parser.parse_args()\n    main(args)\n</code></pre> <p>Command to execute the script:</p> <pre><code>python plot.py file1.csv file2.csv file3.csv --outdir results/\n</code></pre>"},{"location":"pages/project/project-4-demo/","title":"To end on a high note !","text":"<p>To conclude this session, we offer you a demo to go further into reproducibility. We will : </p> <ol> <li>Create a repo on GitHub</li> <li>Create a README and set a license</li> <li>Add a conda file and our Jupyter notebook file</li> <li>Update the README to explain how to edit it and use quarto to generate the page in HTML</li> <li>Publish this page on GitHub and put it online</li> <li>Make a release with Zenodo to have a DOI of your code</li> <li>Set up a contributors file</li> <li>See how to archive the repository on Software heritage</li> </ol> <p>You can find a support with different screenshots if you want try it yourself (here)</p> <p>Et voil\u00e0 !</p>"},{"location":"pages/quarto/quarto-1-introduction/","title":"Introduction","text":"<p>Another notebook? But why? We have already seen R markdown and Jupyter. That's true, but Quarto is the new kid on the block and it's already getting a lot of attention!</p> <p> </p> Appearance of packages allowing the creation of notebooks (source) <p>Quarto is an open-source scientific and technical publishing system where authors :</p> <ul> <li>Can use Jupyter notebooks or with plain text markdown in your favorite editor.</li> <li>Create dynamic content with Python, R, Julia, and Observable.</li> <li>Publish reproducible, production quality articles, presentations, websites, blogs, and books in HTML, PDF, MS Word, ePub, and more.</li> <li>Share results in a lot of publishing systems like GitHub.</li> </ul> <p>His goal ? Unify and extend the R Markdown ecosystem by</p> <ol> <li>unifing R Markdown fans</li> <li>extending the ecosystem to those who don't know R Markdown</li> </ol> <p>In addition, Quarto presents itself as the new open-source system for publishing scientific and technical articles with the aim of making the process of creating and collaboration process radically simpler</p>"},{"location":"pages/quarto/quarto-1-introduction/#quarto-highlights","title":"Quarto highlights","text":"<p>During the Rstudio Conf 2022, Quarto was presented with the following 4 highlights: </p> <ol> <li>Consistent implementation of attractive and handy features across outputs: tabsets, code-folding, syntax highlighting, etc.</li> <li>More accessible defaults as well as better support for accessibility</li> <li>Guardrails, particularly helpful for new learners: YAML completion, informative syntax errors, etc.</li> <li>Support for other languages like Python, Julia, Observable, and more via Jupyter engine for executable code chunks.</li> </ol> <p>The original presentation is here</p>"},{"location":"pages/quarto/quarto-1-introduction/#one-to-rule-them-all","title":"One to rule them all","text":"<p>Quarto CLI orchestrates each step of rendering</p> <p> </p> Source <p>This tutorial depends on files from the course GitHub repo. Take a look at the setup for instructions on how to set it up if you haven't done so already. As a reminder, go here  and download the last version of Quarto.</p>"},{"location":"pages/quarto/quarto-2-the-basics/","title":"The basics","text":""},{"location":"pages/quarto/quarto-2-the-basics/#quarto-rstudio","title":"Quarto &amp; Rstudio","text":"<p>In this part, we will use Rstudio and create from scratch a new quarto document.</p> <p>Warning</p> <p>It is necessary to have a recent version of the RStudio IDE (&gt;=v2022.07).</p> <p>Let's begin with starting RStudio and opening a new file (<code>File</code> --&gt; <code>New File</code> --&gt; <code>Quarto document</code>).</p> <p>You get this :</p> <p></p> <p>But it's like R mardown?! Yes exactly, since one of the objectives of quarto is to replace R markdown !</p> <p>And like with R markdown, you can click Render button to obtain your output !</p> <p></p> <p>A small novelty: the appearance of the visual button to have a more user friendly editor.</p> <p></p> <p>Want to know more ? You can go here to follow the Posit tutorial. </p>"},{"location":"pages/quarto/quarto-2-the-basics/#quarto-jupyter","title":"Quarto &amp; Jupyter","text":"<p>One great thing about quarto is that you don't have to learn anything new! There is nothing new about Jupyter to know. Just how to convert it to HTML or whatever with Quarto. Let's play !</p> <p>Open up a terminal and run these commands to render Jupyter notebook with quarto: </p> <pre><code>quarto render supplementary_material.ipynb --to html\nquarto render supplementary_material.ipynb --to docx\n</code></pre> <p>Et voil\u00e0 !</p> <p>Want to know more ? You can go here to follow the Posit tutorial.</p> <p>Quick recap</p> <p>In this section you learned to: </p> <ul> <li>create a quarto document with Rstudio</li> <li>convert a Jupyter Notebook with Quarto</li> </ul>"},{"location":"pages/quarto/quarto-3-the-rendering/","title":"Rendering","text":"<p>The great strength of Quarto is its diversity of rendering modes.</p>"},{"location":"pages/quarto/quarto-3-the-rendering/#presentation","title":"Presentation","text":"<p>Quarto supports a variety of formats for creating presentations, including:</p> <ul> <li>revealjs \u2014 reveal.js (HTML)</li> <li>pptx \u2014 PowerPoint (MS Office)</li> <li>beamer \u2014 Beamer (LaTeX/PDF)</li> </ul> <p>The most capable format by far is revealjs so is highly recommended. For example, you can write this and render :</p> <pre><code>---\ntitle: \"Habits\"\nauthor: \"John Doe\"\nformat: revealjs\n---\n\n## Getting up\n\n- Turn off alarm\n- Get out of bed\n\n## Going to sleep\n\n- Get in bed\n- Count sheep\n</code></pre> <p>You can find here a complete tutorial and below a demo :</p> <p>An other presentation made in Quarto with revealjs and presented in the real life :</p>"},{"location":"pages/quarto/quarto-3-the-rendering/#websites","title":"Websites","text":"<p>Quarto Websites are a convenient way to publish groups of documents. Documents published as part of a website share navigational elements, rendering options, and visual style. For example, the Quarto website is generated by... quarto !</p> <p>You can find here a complete tutorial</p>"},{"location":"pages/quarto/quarto-3-the-rendering/#books","title":"Books","text":"<p>Quarto Books are combinations of multiple documents (chapters) into a single manuscript. Books can be created in a variety of formats:</p> <ul> <li>HTML</li> <li>PDF</li> <li>MS Word</li> <li>EPUB</li> <li>AsciiDoc</li> </ul> <p>You can find here a complete tutorial</p>"},{"location":"pages/quarto/quarto-3-the-rendering/#interactivity","title":"Interactivity","text":"<p>Quarto can be connected with for example Shiny to make its content dynamic. Thus, it will be possible to have sliders, choice zones,...</p> <p>You can find here a complete tutorial.</p> <p>Quick recap</p> <p>In this section, you had an overview of the different rendering methods from Quarto.</p>"},{"location":"pages/quarto/quarto-4-the-extensions/","title":"Extensions","text":"<p>Extensions are a powerful way to modify and extend the behavior of Quarto. You can find the complete list here. Below, a small selection of extensions that impressed us</p>"},{"location":"pages/quarto/quarto-4-the-extensions/#scientific-paper","title":"Scientific Paper","text":"<p>The quarto community offers templates for famous scientific journals such as : - ACM - ACS - Biophysical - Elsevier - PLOS - ...</p> <p>Below, the visualisation of PLOS template :</p>"},{"location":"pages/quarto/quarto-4-the-extensions/#molstar","title":"Molstar","text":"<p>This extension provides shortcodes for molstar in quarto. Molstar can display macro-molecules such as proteins as well as molcular dynamics trajectories in an interactive viewer. You can see it in action e.g. in the RCSB Protein Data Base: https://www.rcsb.org/, where it provides the 3d view for entries. Follow me, if you want this right in your quarto reports (html only).</p>"},{"location":"pages/quarto/quarto-4-the-extensions/#and-also","title":"And also","text":"<ul> <li>Quarto extension for INRAE : https://github.com/davidcarayon/quarto-inrae-extension</li> <li>Filter to include code from source file : https://github.com/quarto-ext/include-code-files</li> <li>Embed WebR in HTML documents to enable interactive R code cells without the need for an R server : https://github.com/coatless/quarto-webr</li> <li>... </li> </ul> <p>Quick recap</p> <p>In this section, you discover the extent of the quarto extensions.</p>"},{"location":"pages/quarto/quarto-5-the-mrsa-case-study/","title":"The MRSA case study","text":"<p>All the work of writing notebooks has already been done in the  previous sequences: Rmardown and Jupyter</p> <p>Here, we're going to see how to use quarto to convert them. </p>"},{"location":"pages/quarto/quarto-5-the-mrsa-case-study/#rmarkdown","title":"Rmarkdown","text":"<pre><code>quarto render code/supplementary_material.Rmd --to html\nquarto render code/supplementary_material.Rmd --to docx\n</code></pre> <p>Warning</p> <p>As indicated in the setup section, the package versions available on conda cause conflicts with Posit. It is possible to run these commands in the conda environment but we had to mount the versions of certain packages in relation to the rmardown TP because of conflicts. </p>"},{"location":"pages/quarto/quarto-5-the-mrsa-case-study/#jupyter","title":"Jupyter","text":"<pre><code>quarto render code/supplementary_material.ipynb --to html\nquarto render code/supplementary_material.ipynb --to docx\n</code></pre> <p>You're looking at quarto's greatest strength! It can convert Rmd and jupyter notebooks in exactly the same way!</p> <p>Quick recap</p> <p>In this section you learned how to render R Markdown documents and Jupyter document using quarto.</p>"},{"location":"pages/quarto/quarto-6-comparison/","title":"Comparison","text":"Method Jupyter Rmarkdown Quarto IDE JupyterHub, JupyterLab R, Rstudio VS Code, Jupyter, Rstudio, Neovim, TextEditor Code mixing Limited Yes Yes Format ipynb Rmd Qmd, Rmd, ipynb Output Asciidoc, HTML, LaTeX, MD, PDF, RST, Slide (Reveal) HTML, PDF, Docx, ODT, RTF, MD, Slides (Powerpoint, Reveal,...), Dashboard, ... HTML, PDF, Docx, ODT, Epub,  RTF, MD, Slides (Powerpoint, Reveal,...), Wiki (MediaWiki, ...), Book, and more ! Reproductibility Easy Easy Yes (if done from the start) <p>And the winner is...</p>"},{"location":"pages/quarto/quarto-7-extra-material/","title":"Extra material","text":""},{"location":"pages/quarto/quarto-7-extra-material/#a-gallery-full-of-examples","title":"A gallery full of examples","text":"<p>If we haven't convinced you, maybe a trip to the Quarto Gallery will help!  To go there, click here.</p>"},{"location":"pages/quarto/quarto-7-extra-material/#publishing","title":"Publishing","text":"<p>Quarto allows you to generate static web pages. It is therefore possible to host these web pages in tools like GitHub pages. This way you can easily share your code and results for free.</p> <p>You can find here a complete tutorial.</p>"},{"location":"pages/quarto/quarto-installation/","title":"Setup Quarto tutorial","text":""},{"location":"pages/quarto/quarto-installation/#setup-course-material","title":"Setup course material","text":"Follow this instructions only if you start the course at this stage! Otherwise skip this step! <pre><code>This tutorial depends on files from the course GitHub repo. Please follow these instructions \non how to set it up if you haven't done so already.  \nLet's create a directory and clone the course GitHub repo.\n\n```bash\nmkdir -p  ~/training-reproducible-research-area\ncd ~/training-reproducible-research-area\ngit clone https://github.com/SouthGreenPlatform/training_reproducible_research\n```\n</code></pre>"},{"location":"pages/quarto/quarto-installation/#setup-environment","title":"Setup environment","text":"<p>First let's create a dedicated folder for this tutorial:</p> <pre><code>mkdir -p  ~/training-reproducible-research-area/quarto\ncd ~/training-reproducible-research-area/quarto\ncp -r ~/training-reproducible-research-area/training_reproducible_research/tutorials/quarto/* . </code></pre> <p>We will use Conda environments for the set up of this tutorial. So, now we will install all the tools via conda:</p> <pre><code>conda env create -f ~/training-reproducible-research-area/training_reproducible_research/tutorials/quarto/environment.yml -n quarto-env\n</code></pre> <p>You can then activate the environment followed by running RStudio in the background from the command lines:</p> <pre><code>conda activate quarto-env\n</code></pre>"},{"location":"pages/quarto/quarto-installation/#rstudio-with-quarto","title":"Rstudio with quarto","text":"<p>Warning<p>Quarto is still in its infancy on conda and has some conflicts with certain R packages on conda. What's more, the Rstudio package on conda is not the latest version (does not integrate quarto natively). If you wish to use the graphical interface to generate Quarto files, we recommend the following installations</p> </p>"},{"location":"pages/quarto/quarto-installation/#docker-installation-reproductible","title":"Docker installation (Reproductible)","text":"<p>The documentation is available here from the Rocker project.</p> <pre><code>docker run --rm -ti -p 8787:8787 rocker/rstudio\n</code></pre>"},{"location":"pages/quarto/quarto-installation/#classic-installation","title":"Classic installation","text":"<ol> <li>Install Rstudio : https://posit.co/download/rstudio-desktop/</li> <li>Install quarto : https://quarto.org/docs/get-started/</li> </ol> <p>Et voil\u00e0 !</p> <p>conda env update -f environment.yml -n quartp-env</p>"},{"location":"pages/rmarkdown/r-markdown-1-introduction/","title":"Introduction","text":"<p>R Markdown documents can be used both to save and execute code and to generate reports in various formats. This is done by mixing markdown (as in the example above), and so-called code chunks in the same document. The code itself, as well as the output it generates, can be included in the final report.</p> <p>R Markdown makes your analysis more reproducible by connecting your code, figures and descriptive text. You can use it to make reproducible reports, rather than e.g. copy-pasting figures into a Word document. You can also use it as a notebook, in the same way as lab notebooks are used in a wet lab setting (or as we utilise Jupyter notebooks in the tutorial after this one).</p> <p>An example before we start? A screenshot of an R markdown file...</p> <p></p> <p>and on the right its result in HTML.</p> <p></p>"},{"location":"pages/rmarkdown/r-markdown-2-the-basics/","title":"The basics","text":"<p>Let's begin with starting RStudio and opening a new file (File --&gt; New File --&gt; R Markdown). If you're using Conda you should have all the packages needed, but install anything that RStudio prompts you to. In the window that opens, select Document and HTML (which should be the default), and click Ok. This will open a template R Markdown document for you. On the top is a so called YAML header:</p> <pre><code>---\ntitle: \"Untitled\"\noutput:\nhtml_document:\ntoc: true\n---\n</code></pre> <p>Warning</p> <p>The header might look slightly different depending on your version of RStudio. If so, replace the default with the header above.</p> <p>Here we can specify settings for the document, like the title and the output format.</p> <ul> <li>Change the title to <code>My first R Markdown document</code></li> </ul> <p>Now, read through the rest of the template R Markdown document to get a feeling for the format. As you can see, there are essentially three types of components in an R Markdown document:</p> <ol> <li>Text (written in R Markdown)</li> <li>Code chunks (written in R or another supported language)</li> <li>The YAML header</li> </ol> <p>Let's dig deeper into each of these in the following sections! But first, just to get the flavor for things to come: press the little Knit-button located at the top of the text editor panel in RStudio. This will prompt you to save the Rmd file (do that), and generate the output file (an HTML file in this case). It will also open up a preview of this file for you.</p> <p>Quick recap</p> <p>In this section, you learned how to create a R mardown file and generate an  output file in HTML.</p>"},{"location":"pages/rmarkdown/r-markdown-3-text/","title":"Text","text":"<p>Some commonly used formatting written in markdown is shown below, which you may recognize from the Git tutorial:</p> <pre><code># This is a heading\n\nThis is a paragraph.\nThis line-break will not appear in the output file.\\\nBut this will (since the previous line ends with a backslash).\n\nThis is a new paragraph.\n\n## This is a sub-heading\n\nThis is **bold text**, this is *italic text*, this is `monospaced text`,\nand this is [a link](http://rmarkdown.rstudio.com/lesson-1.html).\n\nAn important feature of R Markdown is that you are allowed to use R code\ninline to generate text by enclosing it with `r `. As an example: 112/67 is\nequal to `r round(112/67, 2)`. You can also use multiple commands like this:\nI like `r fruits &lt;- c(\"apples\",\"bananas\"); paste(fruits, collapse = \" and \")`!\n</code></pre> <p>The above markdown would generate something like this:</p> <p></p> <p>Instead of reiterating information here, take a look on the first page (only the first page!) of this reference. This will show you how to write more stuff in markdown and how it will be displayed once the markdown document is converted to an output file (e.g. HTML or PDF). An even more complete guide is available here.</p> <ul> <li>Try out some of the markdown described above (and in the links) in your   template R Markdown document! Press Knit to see the effect of your changes.   Don't worry about the code chunks just yet, we'll come to that in a second.</li> </ul> <p>Quick recap</p> <p>In this section you learned and tried out some of the basic markdown syntax.</p>"},{"location":"pages/rmarkdown/r-markdown-4-code-chunks/","title":"Code chunks","text":"<p>Enough about markdown, let's get to the fun part and include some code! Look at the last code chunk in the template R Markdown document that you just created, as an example:</p> <pre><code>```{r pressure, echo = FALSE}\nplot(pressure)\n```\n</code></pre> <p>The R code is surrounded by: <code>```{r}</code> and <code>```</code>. The <code>r</code> indicates that the code chunk contains R code (it is possible to add code chunks using other languages, e.g. Python). After that comes an optional chunk name, <code>pressure</code> in this case (this can be used to reference the code chunk as well as alleviate debugging). Last comes chunk options, separated by commas (in this case there is only one option: <code>echo = FALSE</code>).</p> <p>Note</p> <p>The code chunk name pressure has nothing to do with the code <code>plot(pressure)</code>. In the latter case, pressure is a default R dataframe that is used in examples. The chunk name happened to be set to the string pressure as well, but could just as well have been called something else, e.g. \"Plot pressure data\".</p> <p>Below are listed some useful chunk options related to evaluating and displaying code chunks in the final file:</p> Chunk option Effect <code>echo = FALSE</code> Prevents code, but not the results, from appearing in the finished file. This is a useful way to embed figures. <code>include = FALSE</code> Prevents both code and results from appearing in the finished file. R Markdown still runs the code in the chunk, and the results can be used by other chunks. <code>eval = FALSE</code> The code in the code chunk will not be run (but the code can be displayed in the finished file). Since the code is not evaluated, no results can be shown. <code>results = \"hide\"</code> Evaluate (and display) the code, but don't show the results. <code>message = FALSE</code> Prevents messages that are generated by code from appearing in the finished file. <code>warning = FALSE</code> Prevents warnings that are generated by code from appearing in the finished file. <ul> <li>Go back to your template R Markdown document in RStudio and locate the <code>cars</code>   code chunk.</li> <li>Add the option <code>echo = FALSE</code>:</li> </ul> <pre><code>```{r cars, echo = FALSE}\nsummary(cars)\n```\n</code></pre> <ul> <li>How do you think this will affect the rendered file? Press Knit and check if   you were right.</li> <li>Remove the <code>echo = FALSE</code> option and add <code>eval = FALSE</code> instead:</li> </ul> <pre><code>```{r cars, eval = FALSE}\nsummary(cars)\n```\n</code></pre> <ul> <li>How do you think this will affect the rendered file? Press Knit and check if   you were right.</li> <li>Remove the <code>eval = FALSE</code> option and add <code>include = FALSE</code> instead:</li> </ul> <pre><code>```{r cars, include = FALSE}\nsummary(cars)\n```\n</code></pre> <p>There are also some chunk options related to plots:</p> Chunk option Effect <code>fig.height = 9, fig.width = 6</code> Set plot dimensions to 9x6 inches (the default is 7x7.) <code>out.height = \"10cm\", out.width = \"8cm\"</code> Scale plot to 10x8 cm in the final output file. <code>fig.cap = \"This is a plot.\"</code> Add a figure caption. <ul> <li>Go back to your template R Markdown document in RStudio and locate the   <code>pressure</code> code chunk.</li> <li>Add the <code>fig.width</code> and <code>fig.height</code> options as below:</li> </ul> <pre><code>```{r pressure, echo = FALSE, fig.width = 6, fig.height = 4}\nplot(pressure)\n```\n</code></pre> <ul> <li>Press Knit and look at the output. Can you see any differences?</li> <li>Now add a whole new code chunk to the end of the document. Give it the name   <code>pressure 2</code> (code chunks have to have unique names, or no   name). Add the <code>fig.width</code> and <code>out.width</code> options like this:</li> </ul> <pre><code>```{r pressure 2, echo = FALSE, fig.width = 9, out.width = \"560px\"}\nplot(pressure)\n```\n</code></pre> <ul> <li>Press Knit and look at the output. Notice the difference between the two   plots? In the second chunk we have first plotted a figure that is a fair bit   larger (9 inches wide) than that in the first chunk. Next we have down-sized   it in the final output, using the <code>out.width</code> option (where we need to use   a size metric recognized by the output format, in this case \"560px\" which   works for HTML).</li> </ul> <p>Have you noticed the first chunk?</p> <pre><code>```{r setup, include = FALSE}\nknitr::opts_chunk$set(echo = TRUE)\n```\n</code></pre> <p>In this way we can set global chunk options, i.e. defaults for all chunks. In this example, <code>echo</code> will always be set to <code>TRUE</code>, unless otherwise specified in individual chunks.</p> <p>Tip</p> <p>For more chunk options, have a look at page 2-3 of this reference.</p> <p>It is also possible to create different types of interactive plots using R Markdown. You can see some examples of this here. If you want to try it out you can add the following code chunk to your document:</p> <pre><code>```{r}\nlibrary(networkD3)\ndata(MisLinks, MisNodes)\nforceNetwork(Links = MisLinks, Nodes = MisNodes, Source = \"source\",\n             Target = \"target\", Value = \"value\", NodeID = \"name\",\n             Group = \"group\", opacity = 0.4)\n```\n</code></pre> <p>Quick recap</p> <p>In this section you learned how to include code chunks and how to use chunk options to control how the output (code, results and figures) is displayed.</p>"},{"location":"pages/rmarkdown/r-markdown-5-the-yaml-header/","title":"The YAML header","text":"<p>Last but not least, we have the YAML header. Here is where you configure general settings for the final output file, and a few other things.</p> <p>The settings are written in YAML format in the form key: value. Nested settings or sub-settings are indented with spaces. In the template R Markdown document you can see that <code>html_document</code> is nested under <code>output</code>, and in turn, <code>toc</code> is nested under <code>html_document</code> since it is a setting for the HTML output. The table of contents (TOC) is automatically compiled from the section headers (marked by #).</p> <ul> <li>Add a subsection header somewhere in your document using three <code>###</code>. Knit   and look at how the table of contents is structured.</li> <li>Now set <code>toc: false</code> and knit again. What happened?</li> <li>A setting that works for HTML output is <code>toc_float: true</code>. Add that to your   document (same indentation level as <code>toc: true</code>) and knit. What happened?</li> <li>In the same way, add the option <code>number_sections: true</code>. What happened?</li> <li>Do you think it looks weird with sections numbered with 0, e.g. 0.1? That is   because the document does not contain any level-1-header. Add a header using   only one <code>#</code> at the top of the document, just after the <code>setup</code> chunk. Knit   and see what happens!</li> </ul> <p>We can also set parameters in the YAML header. These are either character strings, numerical values, or logicals, and they can be used in the R code in the code chunks. Let's try it out:</p> <ul> <li>Add two parameters, <code>data</code> and <code>color</code>, to the YAML header. It should now   look something like this:</li> </ul> <pre><code>---\ntitle: \"Untitled\"\noutput:\nhtml_document:\ntoc: true\ntoc_float: true\nnumber_sections: true\nparams:\ndata: cars\ncolor: blue\n---\n</code></pre> <ul> <li>So now we have two parameters that we can use in the code! Modify the   <code>pressure</code> code chunk so that it looks like this:</li> </ul> <pre><code>```{r pressure, fig.width = 6, fig.height = 4}\nplot(get(params$data), col = params$color)\n```\n</code></pre> <p>This will plot the dataset <code>cars</code> using the color <code>blue</code>, based on the parameters we set in the YAML header.</p> <ul> <li>Knit and see what happens!</li> </ul> <p>Later, we will learn how to set parameters using an external command.</p> <p>We have up until now mainly been using <code>html_document</code> as an output format. There are however a range of different available formats to choose between. What is important to know, is that not all chunk settings work for all output formats (this mainly regards settings related to rendering plots and figures), and some YAML settings are specific for the given output format chosen.</p> <ul> <li>Take a look at this gallery of   R Markdown documents to see what different kinds of output formats are   possible to generate.</li> <li>Take a look at the last page of this reference   for a list of YAML header options, and what output formats they are available   for.</li> </ul> <p>Quick recap</p> <p>In this section you learned how to set document-wide settings in the YAML header, including document output type and user defined parameters.</p>"},{"location":"pages/rmarkdown/r-markdown-6-rendering/","title":"Rendering","text":"<p>You can render (sometimes called \"knitting\") reports in several different ways:</p> <ul> <li>Pressing the Knit button in RStudio (as we have done this far)</li> <li>Running the R command <code>render</code>: to Knit the file <code>my_file.Rmd</code> run   <code>rmarkdown::render(\"my_file.Rmd\")</code> in the R console.</li> <li>Running from the command line: <code>R -e 'rmarkdown::render(\"my_file.Rmd\")'</code></li> </ul> <p>Using the <code>render</code> command, we can also set YAML header options and change defaults (i.e. override those specified in the R Markdown document itself). Here are a few useful arguments (see <code>?rmarkdown::render</code> for a full list):</p> <ul> <li><code>output_format</code>: change output format, e.g. <code>html_document</code> or   <code>pdf_document</code></li> <li><code>output_file</code> and <code>output_dir</code>: change directory and file name of the   generated report file (defaults to the same name and directory as the .Rmd   file)</li> <li><code>params</code>: change parameter defaults. Note that only parameters already listed   in the YAML header can be set, no new parameters can be defined</li> </ul> <p>Try to use the <code>render</code> command to knit your template R Markdown document and set the two parameters <code>data</code> and <code>color</code>. Hint: the <code>params</code> argument should be a list, e.g.:</p> <pre><code>rmarkdown::render(\"my_file.Rmd\", params = list(data = \"cars\", color = \"green\"))\n</code></pre> <p>You might already have noticed the various ways in which you can run code chunks directly in RStudio:</p> <ul> <li>Place the cursor on an R command and press <code>CTRL + Enter</code> (Windows) or   <code>Cmd + Enter</code> (Mac) to run that line in R.</li> <li>Select several R command lines and use the same keyboard shortcut as above to   run those lines.</li> <li>To the right in each chunk there are two buttons; one for running the code in   all chunks above the current chunk and one for running the code in the current   chunk (depending on your layout, otherwise you can find the options in the   Run drop-down).</li> <li>You can easily insert an empty chunk in your Rmd document in RStudio by   pressing Code --&gt; Insert Chunk in the menu.</li> </ul> <p>Depending on your settings, the output of the chunk code will be displayed inline in the Rmd document, or in RStudio's Console and Plot panels. To customize this setting, press the cog-wheel next to the Knit button and select either \"Chunk Output Inline\" or \"Chunk Output in Console\". Additionally, in the top right in the editor panel in RStudio there is a button to toggle the document outline. By making that visible you can click and jump between sections (headers and named code chunks) in your R Markdown document.</p> <p>Quick recap</p> <p>In this section you learned how to render R Markdown documents into HTML documents using several different methods.</p>"},{"location":"pages/rmarkdown/r-markdown-7-the-mrsa-case-study/","title":"The MRSA case study","text":"<p>As you might remember from the intro, we are attempting to understand how lytic bacteriophages can be used as a future therapy for the multiresistant bacteria MRSA (methicillin-resistant Staphylococcus aureus). In this exercise, we will use R Markdown to make a report in form of a Supplementary Material HTML based on the outputs from the Snakemake tutorial. Among the benefits of having the supplementary material (or even the full manuscript) in R Markdown format are:</p> <ul> <li>It is fully transparent how the text, tables and figures were produced.</li> <li>If you get reviewer comments, or realize you've made a mistake somewhere, you   can easily update your code and regenerate the document with the push of   a button.</li> <li>By making report generation part of your workflow early on in a project, much   of the eventual manuscript \"writes itself\". You no longer first have to   finish the research part and then start creating the tables and figures for   the paper.</li> </ul> <p>Before you start:</p> <ul> <li>Make sure that your working directory in R is <code>training-reproducible-research-area/rmarkdown</code>   in the course directory (Session &gt; Set Working Directory).</li> <li>Open the file <code>code/supplementary_material.Rmd</code>.</li> </ul> <p>Note</p> <p>In this tutorial we have used Conda to install all the R packages we need, so that you get to practice how you can actually do this in projects of your own. You can, however, install things using <code>install.packages()</code> or <code>BiocManager::install()</code> as well, even though this makes it both less reproducible and more complicated in most cases.</p>"},{"location":"pages/rmarkdown/r-markdown-7-the-mrsa-case-study/#overview","title":"Overview","text":"<p>Let's start by taking a look at the YAML header at the top of the file. The parameters correspond to files (and sample IDs) that are generated by the MRSA analysis workflow (see the Snakemake tutorial) and contain results that we want to include in the supplementary material document. We've also specified that we want to render to HTML.</p> <pre><code>---\ntitle: \"Supplementary Materials\"\noutput: html_document\nparams:\ncounts_file: \"results/tables/counts.tsv\"\nmultiqc_file: \"intermediate/multiqc_general_stats.txt\"\nsummary_file: \"results/tables/counts.tsv.summary\"\nrulegraph_file: \"results/rulegraph.png\"\nSRR_IDs: \"SRR935090 SRR935091 SRR935092\"\nGSM_IDs: \"GSM1186459 GSM1186460 GSM1186461\"\n---\n</code></pre> <ul> <li>From a reproducibility perspective it definitely makes sense to include   information about who authored the document and the date it was generated.   Add the two lines below to the YAML header. Note that we can include inline   R code by using <code>`r some_code`</code>.</li> </ul> <pre><code>author: John Doe, Joan Dough, Jan Doh, Dyon Do\ndate: \"`r format(Sys.time(), '%d %B, %Y')`\"\n</code></pre> <p>Tip</p> <p>Make it a practice to keep track of all input files and add them as parameters rather than hard-coding them later in the R code.</p> <p>Next, take a look at the <code>dependencies</code>, <code>read_params</code>, and <code>read_data</code> chunks. They </p> <ol> <li>load the required packages, </li> <li>read the parameters and store them in R objects to be used later in the code, and </li> <li>read the data in the counts file, the multiqc file, as well as fetch meta data from GEO. These chunks are provided as is, and you do not need to edit them.</li> </ol> <p>Below these chunks there is some markdown text that contains the Supplementary Methods section. Note the use of section headers using <code>#</code> and <code>##</code>. Then there is a Supplementary Tables and Figures section. This contains four code chunks, each for a specific table or figure. Have a quick look at the code and see if you can figure out what it does, but don't worry if you can't understand everything.</p> <p>Finally, there is a Reproducibility section which describes how the results in the report can be reproduced. The <code>session_info</code> chunk prints information regarding R version and which packages and versions that are used. We highly encourage you to include this chunk in all your R Markdown reports: it's an effortless way to increase reproducibility.</p>"},{"location":"pages/rmarkdown/r-markdown-7-the-mrsa-case-study/#rendering-options-and-paths","title":"Rendering options and paths","text":"<p>Now that you have had a look at the R Markdown document, it is time to Knit! We will do this from the R terminal (rather than pressing Knit).</p> <pre><code>rmarkdown::render(\"code/supplementary_material.Rmd\", output_dir = \"results\")\n</code></pre> <p>The reason for this is that we can then redirect the output html file to be saved in the <code>results/</code> directory.</p> <p>Normally, while rendering, R code in the Rmd file will be executed using the directory of the Rmd file as working directory (<code>rmarkdown/code</code> in this case). However, it is good practice to write all code as if it would be executed from the project root directory (<code>rmarkdown/</code> in this case). For instance, you can see that we have specified the files in <code>params</code> with relative paths from the project root directory. To set a different directory as working directory for all chunks one modifies the knit options like this:</p> <pre><code>knitr::opts_knit$set(root.dir = '../')\n</code></pre> <p>Here we set the working directory to the parent directory of the Rmd file (<code>../</code>), in other words, the project root. Use this rather than <code>setwd()</code> while working with Rmd files.</p> <ul> <li>Take a look at the output. You should find the html file in the <code>results</code>   directory.</li> </ul>"},{"location":"pages/rmarkdown/r-markdown-7-the-mrsa-case-study/#formatting-tables-and-figures","title":"Formatting tables and figures","text":"<p>You will probably get a good idea of the contents of the file, but the tables look weird and the figures could be better formatted. Let's start by adjusting the figures!</p> <ul> <li> <p>Locate the <code>Setup</code> chunk. Here, we have already set <code>echo = FALSE</code>. Let's add   some default figure options: <code>fig.height = 6, fig.width = 6, fig.align   = 'center'</code>. This will make the figures slightly smaller than default and   center them.</p> </li> <li> <p>Knit again, using the same R command as above. Do you notice any difference?   Better, but still not perfect!</p> </li> </ul> <p>Let's improve the tables! We have not talked about tables before. There are several options to print tables, here we will use the <code>kable</code> function which is part of the <code>knitr</code> package.</p> <ul> <li>Go to the <code>Sample info</code> chunk. Replace the last line, <code>sample_info</code>, with:</li> </ul> <pre><code>knitr::kable(sample_info)\n</code></pre> <ul> <li>Knit again and look at the result. You should see a formatted table.</li> <li>The column names can be improved, and we could use a table legend. Change to   use the following:</li> </ul> <pre><code>knitr::kable(sample_info, caption = \"Sample info\",\ncol.names = c(\"SRR\", \"GEO\", \"Strain\", \"Treatment\"))\n</code></pre> <ul> <li>Knit and check the result.</li> <li>Try to fix the table in the <code>QC statistics</code> chunk in the same manner. The   column names are fine here so no need to change them, but add a table legend:   \"QC stats from FastQC\". Knit and check your results.</li> </ul> <p>Let's move on to the figures!</p> <ul> <li>Go to the <code>Counts barplot</code> chunk. To add a figure legend we have to use   a chunk option (so not in the same way as for tables). Add the chunk option:</li> </ul> <pre><code>fig.cap = \"Counting statistics per sample, in terms of read counts for genes\n           and reads not counted for various reasons.\"\n</code></pre> <ul> <li>Knit and check the outcome!</li> <li>Next, add a figure legend to the figure in the <code>gene-heatmap</code> chunk. Here we   can try out the possibility to add R code to generate the legend:</li> </ul> <pre><code>fig.cap = paste0(\"Expression (log-10 counts) of genes with at least \",\nmax_cutoff, \" counts in one sample and a CV&gt;\", cv_cutoff, \".\")\n</code></pre> <p>This will use the <code>cv_cutoff</code> and <code>max_cutoff</code> variables to ensure that the figure legend gives the same information as was used to generate the plot. Note that figure legends are generated after the corresponding code chunk is evaluated. This means we can use objects defined in the code chunk in the legend.</p> <ul> <li>Knit and have a look at the results.</li> </ul> <p>The heatmap still looks a bit odd. Let's play with the <code>fig.height</code> and <code>out.height</code> options, like we did above, to scale the figure in a more appropriate way. Add this to the chunk options: <code>fig.height = 10, out.height = \"22cm\"</code>. Knit and check the results. Does it look better now?</p> <ul> <li>Now let's add a third figure! This time we will not plot a figure in R, but   use an available image file showing the structure of the Snakemake workflow   used to generate the inputs for this report. Add a new chunk at the end of   the Supplementary Tables and Figures section containing this code:</li> </ul> <pre><code>knitr::include_graphics(normalizePath(rulegraph_file))\n</code></pre> <p>Knitr paths  Just like for R Markdown paths in general, Knitr needs everything to be relative to the directory in which the <code>.Rmd</code> file is situated. Just like setting the <code>root.dir</code> chunk option can help with this for the overall rendering, the <code>normalizePath()</code> function is needed when using <code>include_graphics()</code> for adding images with Knitr.</p> <ul> <li>Also, add the chunk options:</li> </ul> <pre><code>fig.cap = \"A rule graph showing the different steps of the bioinformatic\n           analysis that is included in the Snakemake workflow.\"\n</code></pre> <p>and:</p> <pre><code>out.height = \"11cm\"\n</code></pre> <ul> <li>Knit and check the results.</li> </ul> <p>Note</p> <p>It is definitely possible to render R Markdown documents as part of a Snakemake or Nextflow workflow. This is something we do for the final version of the MRSA project (in the Containers tutorial). In such cases it is advisable to manage the installation of R and required R packages through your conda environment file and use the <code>rmarkdown::render()</code> command from the shell section of your Snakemake rule or Nexflow process.</p> <p>Quick recap</p> <p>In this section you learned some additional details for making nice R Markdown reports in a reproducible research project setting, including setting the root directory, adding tables as well as setting figure and table captions.</p>"},{"location":"pages/rmarkdown/r-markdown-8-extra-material/","title":"Extra materiel","text":"<p>While the tutorial teaches you all the basics of using R Markdown, there is much more you can do with it, if you want to! Here we cover some extra material if you're curious to learn more, but we don't consider this to be a main part of the course.</p> <p>If you want to read more about R Markdown in general, here are some useful resources:</p> <ul> <li>A nice \"Get Started\" section, as a complement to this tutorial, is available   at RStudio.com.</li> <li>R Markdown cheat sheet   (also available from Help --&gt; Cheatsheets in RStudio)</li> <li>R Markdown reference guide   (also available from Help --&gt; Cheatsheets in RStudio)</li> </ul>"},{"location":"pages/rmarkdown/r-markdown-8-extra-material/#a-nicer-session-info","title":"A nicer session info","text":"<p>While the default <code>sessionInfo()</code> command is highly useful for ending your report with all the packages and their respective versions, it can be a bit hard to read. There is, however, another version of the same command, which you can find in the <code>devtools</code> package. By combining this command with the <code>markup</code> result format you can get a more nicely formatted session information:</p> <pre><code>```{r Session info, echo = FALSE, results = \"markup\"}\ndevtools::session_info()\n```\n</code></pre>"},{"location":"pages/rmarkdown/r-markdown-8-extra-material/#r-markdown-and-workflows","title":"R Markdown and workflows","text":"<p>Working with R Markdown in the context of a Snakemake or Nextflow workflow is something that is highly useful for reproducibility and quite easy to get going with. An important thing that you'll have to manage a bit more than usual is, however, the working directory of the R Markdown document, which is something you can do with parameters, the <code>root_directory</code>, <code>output_dir</code> and <code>output_file</code> special variables. The following is a simple example of how you can write a Snakemake rule for R Markdown:</p> <pre><code>rule report:\n    input:\n        report = \"report.Rmd\"\n    output:\n        html = \"results/report.html\"\n    params:\n        outdir = \"results\"\n    shell:\n\"\"\"\n        Rscript -e 'parameters &lt;- list(root_directory = getwd(),\n                                       parameter_a    = \"first\",\n                                       parameter_b    = 10);\n                    rmarkdown::render(\"{input.report}\",\n                                      params      = parameters,\n                                      output_dir  = \"{params.outdir}\",\n                                      output_file = \"{output.html}\")'\n        \"\"\"\n</code></pre> <p>Doing it for Nextflow would look almost the same, except using Nextflow syntax and variables.</p>"},{"location":"pages/rmarkdown/r-markdown-8-extra-material/#r-markdown-and-other-languages","title":"R Markdown and other languages","text":"<p>While R is the default and original language for any R Markdown document it supports several others, including Python, bash, SQL, JavaScript, to name a few. This means that you can actually get the reproducibility of R Markdown documents when coding in languages other than just R, if your language is one of the supported ones.</p> <p>Two of the most important languages are Python and bash. While bash is supported out-of-the-box directly and only requires you to specify <code>bash</code> instead of <code>r</code> in the start of the code chunk, Python will additionally require you to have installed the <code>reticulate</code> package. Not only does this allow you to code in Python directly in your in R Markdown document, but the objects and variables you use in one language/chunk will actually be available for the other language! You can read more about the R Markdown Python engine here.</p>"},{"location":"pages/rmarkdown/r-markdown-8-extra-material/#r-markdown-and-latex","title":"R Markdown and LaTeX","text":"<p>This tutorial has been using HTML as the output format, as this is the most common format that many data analyses are using. Some reasons for this include not having to think about a page-layout (which is especially useful for documents with many figures/plots, which is common for data analysis documents), simplified creation (i.e. not having to think about special LaTeX commands for PDF output) and fewer dependencies.</p> <p>The PDF format has a lot going for it as well, however, such as being an end-point for journal articles, books and similar documents, as well as being much more powerful (meaning a steeper learning curve) than just HTML rendering: you can customise almost anything you can think of. Not that HTML output is lacking in options, it's just that LaTeX is more feature-rich.</p> <p>Let's take an example: font sizes. This is something that is quite hard to do on a per-chunk basis in HTML, but easy in LaTeX. You can change the font size of all HTML chunks by using a custom CSS template, for instance, but in LaTeX you can just set the font size to something before and after a chunk, like so:</p> <pre><code>\\footnotesize\n```{r Count to 3}\nseq_len(3)\n```\n\\normalsize\n</code></pre> <p>You could also do automatic figure captions using the LaTeX engine, meaning you won't have to add \"Figure X\" to each caption manually. You can even have separate groups of figure captions, such as one group for main article figures and one group for supplementary figures - the same goes for tables, of course.</p> <p>R Markdown uses LaTeX behind the scenes to render PDF documents, but you miss some of the features that are inherent in LaTeX by going this route. There is, thankfully, a different file format that you can use that more explicitly merges R Markdown and all the functionality of LaTeX, called Sweave.</p> <p>Sweave allows you to use any LaTeX command you want outside of R code chunks, which is awesome for those of you who are already using LaTeX and want to combine it with R Markdown. These files use the <code>.Rnw</code> extension rather than <code>.Rmd</code>, with some additional changes, such as code chunks starting with <code>&lt;&lt;&gt;&gt;=</code> and ending with <code>@</code>.</p> <p>There is simply a plethora of tweaks, variables and parameters that you can use for PDF rendering, but it can be quite overwhelming if you're just starting out. We recommend using HTML for most things, especially data analysis, but PDF certainly has its strengths - you could, for example, write a whole paper (and its supplementary material) in R Markdown with LaTeX only, without using Microsoft Word or anything else - it doesn't get much more reproducible than that! In fact, there are several publication-ready templates on a per-journal basis in the package <code>rticles</code>, which can greatly facilitate this process!</p>"},{"location":"pages/rmarkdown/r-markdown-8-extra-material/#presentations-in-r-markdown","title":"Presentations in R Markdown","text":"<p>R Markdown is not only useful for data analysis reports and papers, but you can also create presentations with it! In fact, most of the presentations created for this course were done using R Markdown.</p> <p>A major difference between presentations in R Markdown and e.g. Microsoft PowerPoint is the same as between any Markdown document (or LaTeX, for that matter) and the more common Microsoft Word: the more usual Microsoft software is \"what you see is what you get\", while Markdown/LaTeX doesn't show you the actual output until you've rendered it. This difference is more pronounced when it comes to presentations, as they are more visually heavy.</p> <p>In essence, a R Markdown presentation works the same way as for a R Markdown report, except some different formatting and output specifications. There are a number of output formats you can use, but the one we've used for this course (for no other reason than that we like it) is Xaringan. You can install it from Conda (<code>r-xaringan</code>) like any other package and then specify the output format as <code>xaringan::moon_reader</code> in your YAML header. Slides are separated using three dashes (<code>---</code>) while two dashes (<code>--</code>) signify slide elements that should appear on-click.</p> <p>Here is a bare-bones example of a R Markdown presentation using Xaringan:</p> <pre><code>---\ntitle: \"A R Markdown presentation\"\noutput: xaringan::moon_reader\n---\n\n# R Markdown presentations\n\nYou can write text like normal, including all the normal Markdown formatting\nsuch as *italics* or **bold**.\n\n--\n\nAnything can be separated into on-click appearance using double dashes.\n\n## Sub-headers work fine as well\n\nJust remember to separate your slides with three dashes!\n\n---\n\n# Use code chunks just like normal\n\nThis is especially useful for presentations of data analyses, since you don't\nhave to have a separate R Markdown or script to create the tables/figures and\nthen copy/paste them into a PowerPoint!\n\n```{r, fig.height = 5, fig.width = 5, fig.align = \"center\"}\ndata(cars)\nplot(cars)\n```\n</code></pre> <p>Having said that, presentations is R Markdown can do most things that PowerPoint can do, but it'll take some more effort. Getting something to look like you want in a WYSIWYG-editor like PowerPoint is easier, since you're seeing the output as you're making it, but it'll take more experimentation in R Markdown. You can, however, automate a lot of things, such as by using CSS templates that apply to each slide (including things such as font styles, header, footers, and more) or like the above mentioned benefit of having both code and its results already in your presentation without having to muck about with copying and pasting figures and tables to a separate presentation.</p> <p>For inspiration, we suggest you go to the <code>lectures/</code> directory of the course Git repository. You should also have a look at the official documentation of Xaringan (which is itself a R Markdown-based presentation), as well as its several alternatives. We find that using R Markdown for presentations does take about the same time or slightly more compared to PowerPoint once you're used to it, but there's a learning curve - as with everything else. Anything related to actual code and presenting results can be much quicker, however! A good exercise is to take one of the presentations you have given in the past (such as for a lab meeting, a journal club, etc.) and try to recreate that with R Markdown. Which method of creating presentations you prefer is, ultimately, up to you and what the your current end-goal is for the presentation.</p>"},{"location":"pages/rmarkdown/r-markdown-8-extra-material/#and-much-much-more","title":"And much, much more !","text":"<p>Rmarkdown is available in many other output formats such as book format, website, ... You'll find them all here with examples.</p>"},{"location":"pages/rmarkdown/r-markdown-installation/","title":"Setup R-markdown tutorial","text":""},{"location":"pages/rmarkdown/r-markdown-installation/#setup-course-material","title":"Setup course material","text":"Follow this instructions only if you start the course at this stage! Otherwise skip this step! <pre><code>This tutorial depends on files from the course GitHub repo. Please follow these instructions \non how to set it up if you haven't done so already.  \nLet's create a directory and clone the course GitHub repo.\n\n```bash\nmkdir -p  ~/training-reproducible-research-area\ncd ~/training-reproducible-research-area\ngit clone https://github.com/SouthGreenPlatform/training_reproducible_research\n```\n</code></pre>"},{"location":"pages/rmarkdown/r-markdown-installation/#setup-environment","title":"Setup environment","text":"<p>First let's create a dedicated folder for this tutorial:</p> <pre><code>mkdir -p  ~/training-reproducible-research-area/rmarkdown\ncd ~/training-reproducible-research-area/rmarkdown\ncp -r ~/training-reproducible-research-area/training_reproducible_research/tutorials/rmarkdown/* . </code></pre> <p>We will use Conda environments for the set up of this tutorial. So, now we will install all the tools via conda:</p> <pre><code>conda env create -f ~/training-reproducible-research-area/training_reproducible_research/tutorials/rmarkdown/environment.yml -n rmarkdown-env\n</code></pre> <p>You can then activate the environment followed by running RStudio in the background from the command lines:</p> <pre><code>conda activate rmarkdown-env\n</code></pre> <p>and after :</p> <pre><code>rstudio &amp;\n</code></pre> <p>The sluggishness of Conda<p>Some environments are inherently quite complicated in that they have many and varied dependencies, meaning that the search space for the entire dependency hierarchy becomes huge - leading to slow and sluggish installations. This is often the case for R environments. This can be improved by using Mamba, a faster wrapper around Conda. Simply run conda install -n base mamba to install Mamba in your base environment, and replace any conda command with mamba - except activating and deactivating environments, which still needs to be done using Conda.</p> </p> <p>RStudio and Conda<p>In some cases RStudio doesn't play well with Conda due to differing libpaths. The first and simplest thing to try is to always start RStudio from the command line (rstudio &amp;). If you're still having issues, check the available library path by .libPaths() to make sure that it points to a path within your Conda environment. It might be that .libPaths() shows multiple library paths, in which case R packages will be searched for by R in all these locations. This means that your R session will not be completely isolated in your Conda environment and that something that works for you might not work for someone else using the same Conda environment, simply because you had additional packages installed in the second library location. One way to force R to just use the conda library path is to add a .Renviron file to the directory where you start R with these lines: <pre><code>R_LIBS_USER=\"\"\nR_LIBS=\"\"\n</code></pre> ... and restart RStudio. The rmarkdown/ directory in the course materials already contains this file, so you shouldn't have to add this yourself, but we mention it here for your future projects.</p> </p>"},{"location":"pages/snakemake/snakemake-1-introduction/","title":"Introduction","text":"<p>A workflow management system (WfMS) is a piece of software that sets up, performs and monitors a defined sequence of computational tasks (i.e. \"a workflow\"). Snakemake is a WfMS that was developed in the bioinformatics community, and as such it has a number of features that make it particularly well-suited for creating reproducible and scalable data analyses.</p> <p>First of all the language you use to formulate your workflows is based on Python, which is a language with strong standing in academia. However, users are not required to know how to code in Python to work efficiently with Snakemake. Workflows can easily be scaled from your desktop to server, cluster, grid or cloud environments. This makes it possible to develop a workflow on your laptop, maybe using only a small subset of your data, and then run the real analysis on a cluster. Snakemake also has several features for defining the environment with which each task is carried out. This is important in bioinformatics, where workflows often involve running a large number of small third-party tools.</p> <p>Snakemake is primarily intended to work on files (rather than for example streams, reading/writing from databases or passing variables in memory). This fits well with many fields of bioinformatics, notably next-generation sequencing, that often involve computationally expensive operations on large files. It's also a good fit for a scientific research setting, where the exact specifications of the final workflow aren't always known at the beginning of a project.</p> <p>Lastly, a WfMS is a very important tool for making your analyses reproducible. By keeping track of when each file was generated, and by which operation, it is possible to ensure that there is a consistent \"paper trail\" from raw data to final results. Snakemake also has features that allow you to package and distribute the workflow, and any files it involves, once it's done.</p>"},{"location":"pages/snakemake/snakemake-10-generalizing-workflows/","title":"Generalizing workflows","text":"<p>It's generally a good idea to separate project-specific parameters from the actual implementation of the workflow. If we want to move all project-specific information to <code>config.yml</code>, and let the Snakefile be a more general RNA-seq analysis workflow, we need the config file to:</p> <ul> <li>Specify which samples to run.</li> <li>Specify which genome to align to and where to download its sequence and   annotation files.</li> <li>(Any other parameters we might need to make it into a general workflow,   e.g. to support both paired-end and single-read sequencing)</li> </ul> <p>Note</p> <p>Putting all configuration in <code>config.yml</code> will break the <code>generate_rulegraph</code> rule. You can fix it either by replacing <code>--config max_reads=0</code> with <code>--configfile=config.yml</code> in the shell command of that rule in the Snakefile, or by adding <code>configfile: \"config.yml\"</code> to the top of the Snakefile (as mentioned in a previous tip).</p> <p>The first point is straightforward; rather than using <code>SAMPLES = [\"...\"]</code> in the Snakefile we define it as a parameter in <code>config.yml</code>. You can either add it as a list similar to the way it was expressed before by adding  <code>SAMPLES: [\"...\"]</code> to <code>config.yml</code>, or you can use this yaml notation:</p> <pre><code>sample_ids:\n- SRR935090\n- SRR935091\n- SRR935092\n</code></pre> <p>You also have to change the workflow to reference <code>config[\"sample_ids\"]</code> (if using the latter example) instead of <code>SAMPLES</code>, as in:</p> <pre><code>expand(\"intermediate/{sample_id}_fastqc.zip\",\n            sample_id = config[\"sample_ids\"])\n</code></pre> <p>Do a dry-run afterwards to make sure that everything works as expected.</p> <p>The second point is trickier. Writing workflows in Snakemake is quite straightforward when the logic of the workflow is reflected in the file names, i.e. <code>my_sample.trimmed.deduplicated.sorted.fastq</code>, but that isn't always the case. In our case we have the FTP paths to the genome sequence and annotation where the naming doesn't quite fit with the rest of the workflow. The easiest solution is probably to make three parameters to hold these values, say <code>genome_id</code>, <code>genome_fasta_path</code> and <code>genome_gff_path</code>, but we will go for a somewhat more complex but very useful alternative. We want to construct a dictionary where something that will be a wildcard in the workflow is the key and the troublesome name is the value. An example might make this clearer (this is also in <code>config.yml</code> in the finished version of the workflow under <code>tutorials/git/</code>). This is a nested dictionary where \"genomes\" is a key with another dictionary as value, which in turn has genome ids as keys and so on. The idea is that we have a wildcard in the workflow that takes the id of a genome as value (either \"NCTC8325\" or \"ST398\" in this case). The fasta and gff3 paths can then be retrieved based on the value of the wildcard.</p> <pre><code>genomes:\nNCTC8325:\nfasta: ftp://ftp.ensemblgenomes.org/pub/bacteria/release-37/fasta/bacteria_18_collection/staphylococcus_aureus_subsp_aureus_nctc_8325/dna//Staphylococcus_aureus_subsp_aureus_nctc_8325.ASM1342v1.dna_rm.toplevel.fa.gz\ngff3: ftp://ftp.ensemblgenomes.org/pub/bacteria/release-37/gff3/bacteria_18_collection/staphylococcus_aureus_subsp_aureus_nctc_8325//Staphylococcus_aureus_subsp_aureus_nctc_8325.ASM1342v1.37.gff3.gz\nST398:\nfasta: ftp://ftp.ensemblgenomes.org/pub/bacteria/release-37/fasta/bacteria_18_collection//staphylococcus_aureus_subsp_aureus_st398/dna/Staphylococcus_aureus_subsp_aureus_st398.ASM958v1.dna.toplevel.fa.gz\ngff3: ftp://ftp.ensemblgenomes.org/pub/bacteria/release-37/gff3/bacteria_18_collection/staphylococcus_aureus_subsp_aureus_st398//Staphylococcus_aureus_subsp_aureus_st398.ASM958v1.37.gff3.gz\n</code></pre> <p>Go ahead and add the section above to <code>config.yml</code>.</p> <p>Let's now look at how to do the mapping from genome id to fasta path in the rule <code>get_genome_fasta</code>. This is how the rule currently looks (if you have added the log section as previously described).</p> <pre><code>rule get_genome_fasta:\n\"\"\"\n    Retrieve the sequence in fasta format for a genome.\n    \"\"\"\n    output:\n        \"data/raw_external/NCTC8325.fa.gz\"\n    log:\n        \"results/logs/get_genome_fasta/NCTC8325.log\"\n    shell:\n\"\"\"\n        wget ftp://ftp.ensemblgenomes.org/pub/bacteria/release-37/fasta/bacteria_18_collection/staphylococcus_aureus_subsp_aureus_nctc_8325/dna//Staphylococcus_aureus_subsp_aureus_nctc_8325.ASM1342v1.dna_rm.toplevel.fa.gz -O {output} -o {log}\n        \"\"\"\n</code></pre> <p>We don't want the hardcoded genome id <code>NCTC8325</code>, so replace that with a wildcard, say <code>{genome_id}</code> (remember to add the wildcard to the <code>log:</code> directive as well).</p> <p>Also change in <code>index_genome</code> to use a wildcard rather than a hardcoded genome id. Here you will run into a complication if you have followed the previous instructions and use the <code>expand()</code> expression. We want the list to expand to <code>[\"intermediate/{genome_id}.1.bt2\", \"intermediate/{genome_id}.2.bt2\", ...]</code>, i.e. only expanding the wildcard referring to the bowtie2 index. To keep the <code>genome_id</code> wildcard from being expanded we have to \"mask\" it with double curly brackets: <code>`. In addition, we need to replace the hardcoded</code>intermediate/NCTC8325<code>in the shell directive of the rule with the genome id wildcard. Inside the shell directive the wildcard object is accessed with this syntax:</code>`, so the bowtie2-build command should be:</p> <pre><code>bowtie2-build tempfile intermediate/{wildcards.genome_id} &gt; {log}\n</code></pre> <p>We now need to supply the remote paths to the fasta and gff files for a given genome id. Because we've added this information to the config file we just need to pass it to the rule in some way.</p> <p>Take a look at the code and <code>get_genome_fasta</code> rule below. Here we have defined a function called <code>get_fasta_path</code> which takes the <code>wildcards</code> object as its only argument. This object allows access to the wildcards values via attributes (here <code>wildcards.genome_id</code>). The function will then look in the nested <code>config</code> dictionary and return the value of the fasta path for the key <code>wildcards.genome_id</code>. In the rule this path is stored in the <code>fasta_path</code> param value and is made available to <code>wget</code> in the shell directive.</p> <pre><code>def get_fasta_path(wildcards):\n    return config[\"genomes\"][wildcards.genome_id][\"fasta\"]\n\nrule get_genome_fasta:\n\"\"\"\n    Retrieve the sequence in fasta format for a genome.\n    \"\"\"\n    output:\n        \"data/raw_external/{genome_id}.fa.gz\"\n    log:\n        \"results/logs/get_genome_fasta/{genome_id}.log\"\n    params:\n        fasta_path = get_fasta_path\n    shell:\n\"\"\"\n        wget {params.fasta_path} -O {output} -o {log}\n        \"\"\"\n</code></pre> <p>Note that this will only work if the <code>{genome_id}</code> wildcard can be resolved to something defined in the config (currently <code>NCTC8325</code> or <code>ST398</code>). If you try to generate a fasta file for a genome id not defined in the config Snakemake will complain, even at the dry-run stage.</p> <p>Now change the <code>get_genome_gff3</code> rule in a similar manner.</p> <p>The rules <code>get_genome_fasta</code>, <code>get_genome_gff3</code> and <code>index_genome</code> can now download and index any genome as long as we provide valid links in the config file.</p> <p>However, we need to define somewhere which genome id we actually want to use when running the workflow. This needs to be done both in <code>align_to_genome</code> and <code>generate_count_table</code>. Do this by introducing a parameter in <code>config.yml</code> called <code>\"genome_id\"</code> (you can set it to either <code>NCTC8325</code> or <code>ST398</code>).</p> <p>Now we can resolve the <code>genome_id</code> wildcard from the config. See below for an example for <code>align_to_genome</code>. Here the <code>substr</code> wildcard gets expanded from a list while <code>genome_id</code> gets expanded from the config file.</p> <pre><code>input:\n    index = expand(\"intermediate/{genome_id}.{substr}.bt2\",\n           genome_id = config[\"genome_id\"],\n           substr = [\"1\", \"2\", \"3\", \"4\", \"rev.1\", \"rev.2\"])\n</code></pre> <p>Also change the hardcoded genome id in the <code>generate_count_table</code> input in a similar manner.</p> <p>In general, we want the rules as far downstream as possible in the workflow to be the ones that determine what the wildcards should resolve to. In our case this is <code>align_to_genome</code> and <code>generate_count_table</code>. You can think of it like the rule that really \"needs\" the file asks for it, and then it's up to Snakemake to determine how it can use all the available rules to generate it. Here the <code>align_to_genome</code> rule says \"I need this genome index to align my sample to\" and then it's up to Snakemake to determine how to download and build the index.</p> <p>One last thing is to change the hardcoded <code>NCTC8325</code> in the <code>shell:</code> directive of <code>align_to_genome</code>. Bowtie2 expects the index name supplied with the <code>-x</code> flag to be without the \".*.bt2\" suffix so we can't use <code>-x {input.index}</code>. Instead we'll insert the genome_id directly from the config like this:</p> <pre><code>shell:\n    \"\"\"  \n    bowtie2 -x intermediate/{config[genome_id]} -U {input[0]} &gt; {output} 2&gt;{log}\n    \"\"\"\n</code></pre> <p>Quick recap</p> <p>In this section we've learned how to generalize a Snakemake workflow with a number of excellent features:</p> <ul> <li>A general RNA-seq pipeline which can easily be reused between projects, thanks to clear separation between code and settings.</li> <li>Great traceability due to logs and summary tables.</li> <li>Clearly defined the environment for the workflow using Conda.</li> <li>The workflow is neat and free from temporary files due to using <code>temp()</code> and <code>shadow</code>.</li> <li>A logical directory structure which makes it easy to separate raw data, intermediate files, and results.</li> <li>A project set up in a way that makes it very easy to distribute and reproduce either via Git, Snakemake's <code>--archive</code> option or a Docker image.</li> </ul>"},{"location":"pages/snakemake/snakemake-11-extra-material/","title":"Extra material","text":"<p>If you want to read more about Snakemake in general you can find several resources here:</p> <ul> <li>The Snakemake documentation is available on readthedocs.</li> <li>Here is another (quite in-depth) tutorial.</li> <li>If you have questions, check out stack overflow.</li> </ul>"},{"location":"pages/snakemake/snakemake-11-extra-material/#using-containers-in-snakemake","title":"Using containers in Snakemake","text":"<p>Snakemake also supports defining a Singularity or Docker container for each rule (you will have time to work on the Containers tutorial later during the course). Analogous to using a rule-specific Conda environment, specify <code>container: \"docker://some-account/rule-specific-image\"</code> in the rule definition. Instead of a link to a container image, it is also possible to provide the path to a <code>*.sif</code> file (= a Singularity file). When executing Snakemake, add the <code>--use-singularity</code> flag to the command line. For the given rule, a Singularity container will then be created from the image or Singularity file that is provided in the rule definition on the fly by Snakemake and the rule will be run in this container.</p> <p>You can find pre-made Singularity or Docker images for many tools on https://biocontainers.pro/ (bioinformatics-specific) or on https://hub.docker.com/.</p> <p>Here is an example for a rule and its execution:</p> <pre><code>rule align_to_genome:\n\"\"\"\n    Align a fastq file to a genome index using Bowtie 2.\n    \"\"\"\n    input:\n        \"data/raw_internal/{sample_id}.fastq.gz\",\n        \"intermediate/NCTC8325.1.bt2\",\n        \"intermediate/NCTC8325.2.bt2\",\n        \"intermediate/NCTC8325.3.bt2\",\n        \"intermediate/NCTC8325.4.bt2\",\n        \"intermediate/NCTC8325.rev.1.bt2\",\n        \"intermediate/NCTC8325.rev.2.bt2\"\n    output:\n        \"intermediate/{sample_id,\\w+}.bam\"\n    container: \"docker://quay.io/biocontainers/bowtie2:2.3.4.1--py35h2d50403_1\"\n    shell:\n\"\"\"\n        bowtie2 -x intermediate/NCTC8325 -U {input[0]} &gt; {output}\n        \"\"\"\n</code></pre> <p>Start your Snakemake workflow with the following command:</p> <pre><code>snakemake --use-singularity\n</code></pre> <p>Feel free to modify the MRSA workflow according to this example. As Singularity is a container software that was developed for HPC clusters, and for example the Mac version is still a beta version, it might not work to run your updated Snakemake workflow with Singularity locally on your computer. In the next section we explain how you can run Snakemake workflows on UPPMAX where Singularity is pre-installed.</p>"},{"location":"pages/snakemake/snakemake-11-extra-material/#running-snakemake-workflows-on-uppmax","title":"Running Snakemake workflows on UPPMAX","text":"<p>There are several options to execute Snakemake workflows on UPPMAX (a HPC cluster with the SLURM workload manager). In any case, we highly recommend to use a session manager like <code>tmux</code> or <code>screen</code> so that you can run your workflow in a session in the background while doing other things on the cluster or even logging out of the cluster.</p>"},{"location":"pages/snakemake/snakemake-11-extra-material/#run-your-workflow-in-an-interactive-job","title":"Run your workflow in an interactive job","text":"<p>For short workflows with only a few rules that need the same compute resources in terms of CPU (cores), you can start an interactive job (in your <code>tmux</code> or <code>screen</code> session) and run your Snakemake workflow as you would do that on your local machine. Make sure to give your interactive job enough time to finish running all rules of your Snakemake workflow.</p>"},{"location":"pages/snakemake/snakemake-11-extra-material/#cluster-configuration","title":"Cluster configuration","text":"<p>For workflows with long run times and/or where each rule requires different compute resources, Snakemake can be configured to automatically send each rule as a job to the SLURM queue and to track the status of each job.</p> <p>The relevant parameters for such a cluster configuration are <code>--cluster</code> and <code>--cluster-config</code>, in combination with a <code>cluster.yaml</code> file that specifies default and rule-specific compute resources and your compute account details.</p> <p>Here is an example for a <code>cluster.yaml</code> file:</p> <pre><code># cluster.yaml - cluster configuration file\n__default__:\naccount: # fill in your project compute account ID\npartition: core\ntime: 01:00:00\nntasks: 1\ncpus-per-task: 1\n### rule-specific resources\ntrimming:\ntime: 01-00:00:00\nmapping:\ntime: 01-00:00:00\ncpus-per-task: 16\n</code></pre> <p>Start your Snakemake workflow in a <code>tmux</code> or <code>screen</code> session with the following command:</p> <pre><code>snakemake\n    -j 10 \\\n--cluster-config cluster.yaml \\\n--cluster \"sbatch \\\n               -A {cluster.account} \\\n               -p {cluster.partition} \\\n               -t {cluster.time} \\\n               --ntasks {cluster.ntasks} \\\n               --cpus-per-task {cluster.cpus-per-task}\"\n</code></pre> <p>The additional parameter <code>-j</code> specifies the number of jobs that Snakemake is allowed to send to SLURM at the same time.</p>"},{"location":"pages/snakemake/snakemake-11-extra-material/#slurm-profile","title":"SLURM Profile","text":"<p>The cluster configuration is actually marked as \"deprecated\" but still exists side-by-side with the thought to be replacement: profiles. Snakemake profiles can be used to define several options, allowing you to quickly adapt a workflow to different use-cases or to different environments. One such convenient profile is the SLURM profile developed to make a workflow make efficient use of the SLURM workload manager that is used e.g. on Uppmax.</p> <p>The SLURM Profile needs to be set up with the software cookiecutter which you can install with conda: <code>conda install -c conda-forge cookiecutter</code>.</p> <p>During the setup of  the profile you will be asked for several values for your Profile. To configure the profile to use your account id see Example 1: project setup to use specific slurm account at the profile repository.</p> <p>Rule-specific resources can be defined in each rule via the <code>resources:</code> directive, for example:</p> <pre><code>rule align_to_genome:\n    input:\n        \"{genome_id}.bt2\",\n        \"{sample}.fastq.gz\"\n    output:\n        \"{sample}.bam\"\n    resources:\n        runtime = 360\n    threads: 10\n    shell:\n\"\"\"\n        aligner -t {threads} -i {input[1]} -x {input[0]} &gt; {output}\n        \"\"\"\n</code></pre> <p>Any rule for which runtime is specified in the <code>resources</code> directive will be submitted as one job to the SLURM queue with runtime as the allocated time. Similarly, the number specified in the <code>threads</code> directive will be used as the number of allocated cores.</p> <p>With this setup you can start the workflow with your SLURM Profile as follows from within a <code>tmux</code> or <code>screen</code> session:</p> <pre><code>snakemake -j 10 --profile your_profile_name\n</code></pre>"},{"location":"pages/snakemake/snakemake-2-the-basics/","title":"The basics","text":"<p>In this part of the tutorial we will create a very simple workflow from scratch, in order to show the fundamentals of how Snakemake works. The workflow will take two files as inputs, <code>a.txt</code> and <code>b.txt</code>, and the purpose is to convert the text in the files to upper case and then to concatenate them.</p> <p>Run the following shell commands. The first one will make an empty file named <code>Snakefile</code>, which will later contain the workflow. The second and third commands generate two files containing some arbitrary text.</p> <pre><code>touch Snakefile\necho \"This is a.txt\" &gt; a.txt\necho \"This is b.txt\" &gt; b.txt\n</code></pre> <p>Then open <code>Snakefile</code> in your favorite text editor. A Snakemake workflow is based on rules which take some file(s) as input, performs some type of operation on them, and generate some file(s) as outputs. Here is a very simple rule that produces <code>a.upper.txt</code> as an output, using <code>a.txt</code> as input. Copy this rule to your <code>Snakefile</code> and save it.</p> <pre><code>rule convert_to_upper_case:\n    output:\n        \"a.upper.txt\"\n    input:\n        \"a.txt\"\n    shell:\n\"\"\"\n        tr [a-z] [A-Z] &lt; {input} &gt; {output}\n        \"\"\"\n</code></pre> <p>Warning</p> <p>Indentation is important in Snakefiles, so make sure that you have the correct number of spaces before <code>input</code>/<code>output</code>/<code>shell</code> and their respective subsections. The number of spaces per level doesn't matter as long as you're consistent. Here we use four, but you could just as well use two for a more compact look. Don't use tabs (unless your editor automatically converts them to spaces).</p> <p>Rules can be given names, here it's <code>convert_to_upper_case</code>. While rule names are not strictly necessary we encourage you to use them and to make an effort to name your rules in a way that makes it easy to understand the purpose of the rule, as rule names are one of the main ways to interact with the workflow. The <code>shell</code> section (or directive) contains the shell commands that will convert the text in the input file to upper case and send it to the output file. In the shell command string, we can refer to elements of the rule via curly brackets. Here, we refer to the output file by specifying <code>{output}</code> and to the input file by specifying <code>{input}</code>. If you're not very familiar with Bash, this particular command can be read like \"send the contents of <code>a.txt</code> to the program <code>tr</code>, which will convert all characters in the set <code>[a-z]</code> to the corresponding character in the set <code>[A-Z]</code>, and then send the output to <code>a.upper.txt</code>\".</p> <p>Now let's run our first Snakemake workflow. When a workflow is executed Snakemake tries to generate a set of target files. Target files can be specified via the command line (or, as you will see later, in several other ways). Here we ask Snakemake to make the file <code>a.upper.txt</code>. It's good practice to first run with the flag <code>-n</code> (or <code>--dry-run</code>), which will show what Snakemake plans to do without actually running anything, and you also need to specify how many cores to be used for the workflow with <code>--cores</code> or <code>-c</code>. For now, you only need 1 so set <code>-c 1</code>. You can also use the flag <code>-p</code>, for showing the shell commands that it will execute, and the flag <code>-r</code> for showing the reason for running a specific rule. <code>snakemake --help</code> will show you all available flags.</p> <pre><code>$ snakemake -n -c 1 -r -p a.upper.txt\n\nBuilding DAG of jobs...\nJob stats:\njob                      count    min threads    max threads\n---------------------  -------  -------------  -------------\nconvert_to_upper_case        1              1              1\ntotal                        1              1              1\n\n\n[Mon Oct 25 16:48:43 2021]\nrule convert_to_upper_case:\n    input: a.txt\n    output: a.upper.txt\n    jobid: 0\n    reason: Missing output files: a.upper.txt\n    resources: tmpdir=/var/folders/p0/6z00kpv16qbf_bt52y4zz2kc0000gp/T\n\n\n        tr [a-z] [A-Z] &lt; a.txt &gt; a.upper.txt\n\nJob stats:\njob                      count    min threads    max threads\n---------------------  -------  -------------  -------------\nconvert_to_upper_case        1              1              1\ntotal                        1              1              1\n\nThis was a dry-run (flag -n). The order of jobs does not reflect the order of execution.\n</code></pre> <p>You can see that Snakemake plans to run one job: the rule <code>convert_to_upper_case</code> with <code>a.txt</code> as input and <code>a.upper.txt</code> as output. The reason for doing this is that it's missing the file <code>a.upper.txt</code>. Now execute the workflow without the <code>-n</code> flag and check that the contents of <code>a.upper.txt</code> is as expected. Then try running the same command again. What do you see? It turns out that Snakemake only reruns jobs if there have been changes to either the input files, or the workflow itself. This is how Snakemake ensures that everything in the workflow is up to date. We will get back to this shortly.</p> <p>What if we ask Snakemake to generate the file <code>b.upper.txt</code>?</p> <pre><code>$ snakemake -n -c 1 -r -p b.upper.txt\n\nBuilding DAG of jobs...\nMissingRuleException:\nNo rule to produce b.upper.txt (if you use input functions make sure that they don't raise unexpected exceptions).\n</code></pre> <p>That didn't work well. We could copy the rule to make a similar one for <code>b.txt</code>, but that would be a bit cumbersome. Here is where named wildcards come in; one of the most powerful features of Snakemake. Simply change the input from <code>input: \"a.txt\"</code> to <code>input: \"{some_name}.txt\"</code> and the output to <code>output: \"{some_name}.upper.txt\"</code>. Now try asking for <code>b.upper.txt</code> again.</p> <p>Tada! What happens here is that Snakemake looks at all the rules it has available (actually only one in this case) and tries to assign values to all wildcards so that the targeted files can be generated. In this case it was quite simple, you can see that it says that <code>wildcards: some_name=b</code>, but for large workflows and multiple wildcards it can get much more complex. Named wildcards is what enables a workflow (or single rules) to be efficiently generalized and reused between projects or shared between people.</p> <p>It seems we have the first part of our workflow working, now it's time to make the second rule for concatenating the outputs from <code>convert_to_upper_case</code>. The rule structure will be similar; the only difference is that here we have two inputs instead of one. This can be expressed in two ways, either with named inputs like this:</p> <pre><code>input:\n    firstFile=\"...\",\n    secondFile=\"...\"\nshell:\n\"\"\"\n    some_function {input.firstFile} {input.secondFile}\n    \"\"\"\n</code></pre> <p>Or with indexes like this:</p> <pre><code>input:\n    \"...\",\n    \"...\"\nshell:\n\"\"\"\n    some_function {input[0]} {input[1]}\n    \"\"\"\n</code></pre> <p>Warning</p> <p>If you have multiple inputs or outputs they need to be delimited with a comma (as seen above). This is a very common mistake when writing Snakemake workflows. The parser will complain, but sometimes the error message can be difficult to interpret.</p> <p>Now try to construct this rule yourself and name it <code>concatenate_a_and_b</code>. The syntax for concatenating two files in Bash is <code>cat first_file.txt second_file.txt &gt; output_file.txt</code>. Call the output <code>c.txt</code>. Run the workflow in Snakemake and validate that the output looks as expected.</p> <p>Wouldn't it be nice if our workflow could be used for any files, not just <code>a.txt</code> and <code>b.txt</code>? We can achieve this by using named wildcards (or in other ways as we will discuss later). As we've mentioned, Snakemake looks at all the rules it has available and tries to assign values to all wildcards so that the targeted files can be generated. We therefore have to name the output file in a way so that it also contains information about which input files it should be based on. Try to figure out how to do this yourself. If you're stuck you can look at the spoiler below, but spend some time on it before you look. Also rename the rule to <code>concatenate_files</code> to reflect its new more general use.</p> Click to show the solution <pre><code>rule concatenate_files:\n    output:\n        \"{first}_{second}.txt\"    \n    input:\n        \"{first}.upper.txt\",\n        \"{second}.upper.txt\"\n    shell:\n\"\"\"\n        cat {input[0]} {input[1]} &gt; {output}\n        \"\"\"\n</code></pre> <p>We can now control which input files to use by the name of the file we ask Snakemake to generate. Run the workflow without the flag <code>-n</code> (or <code>--dry-run</code>) to execute both rules, providing one core with <code>-c 1</code> (or <code>--cores 1</code>):</p> <pre><code>$ snakemake a_b.txt -c 1\n\nBuilding DAG of jobs...\nUsing shell: /bin/bash\nProvided cores: 1 (use --cores to define parallelism)\nRules claiming more threads will be scaled down.\nJob stats:\njob                      count    min threads    max threads\n---------------------  -------  -------------  -------------\nconcatenate_files            1              1              1\nconvert_to_upper_case        2              1              1\ntotal                        3              1              1\n\nSelect jobs to execute...\n\n[Mon Oct 25 16:51:52 2021]\nrule convert_to_upper_case:\n    input: b.txt\n    output: b.upper.txt\n    jobid: 2\n    wildcards: some_name=b\n    resources: tmpdir=/var/folders/p0/6z00kpv16qbf_bt52y4zz2kc0000gp/T\n\n[Mon Oct 25 16:51:53 2021]\nFinished job 2.\n1 of 3 steps (33%) done\nSelect jobs to execute...\n\n[Mon Oct 25 16:51:53 2021]\nrule convert_to_upper_case:\n    input: a.txt\n    output: a.upper.txt\n    jobid: 1\n    wildcards: some_name=a\n    resources: tmpdir=/var/folders/p0/6z00kpv16qbf_bt52y4zz2kc0000gp/T\n\n[Mon Oct 25 16:51:53 2021]\nFinished job 1.\n2 of 3 steps (67%) done\nSelect jobs to execute...\n\n[Mon Oct 25 16:51:53 2021]\nrule concatenate_files:\n    input: a.upper.txt, b.upper.txt\n    output: a_b.txt\n    jobid: 0\n    wildcards: first=a, second=b\n    resources: tmpdir=/var/folders/p0/6z00kpv16qbf_bt52y4zz2kc0000gp/T\n\n[Mon Oct 25 16:51:53 2021]\nFinished job 0.\n3 of 3 steps (100%) done\n</code></pre> <p>Neat!</p> <p>Tip</p> <p>You can name a file whatever you want in a Snakemake workflow, but you will find that everything falls into place much nicer if the filename reflects the file's path through the workflow, e.g. <code>sample_a.trimmed.deduplicated.sorted.bam</code>.</p> <p>Quick recap</p> <p>In this section we've learned:</p> <ul> <li>How a simple Snakemake rule looks.</li> <li>How to define target files when executing a workflow.</li> <li>How to use named wildcards for writing generic and flexible rules.</li> </ul>"},{"location":"pages/snakemake/snakemake-3-visualising-workflows/","title":"Visualising workflow","text":"<p>All that we've done so far could quite easily be done in a simple shell script that takes the input files as parameters. Let's now take a look at some of the features where a WfMS like Snakemake really adds value compared to a more straightforward approach. One such feature is the possibility to visualize your workflow. Snakemake can generate three types of graphs, one that shows how the rules are connected, one that shows how the jobs (i.e. an execution of a rule with some given inputs/outputs/settings) are connected, and finally one that shows rules with their respective input/output files.</p> <p>First we look at the rule graph. The following command will generate a rule graph in the dot language and pipe it to the program <code>dot</code>, which in turn will save a visualization of the graph as a PNG file (if you're having troubles displaying PNG files you could use SVG or JPG instead).</p> <pre><code>snakemake --rulegraph a_b.txt | dot -Tpng &gt; rulegraph.png\n</code></pre> <p></p> <p>This looks simple enough, the output from the rule <code>convert_to_upper_case</code> will be used as input to the rule <code>concatenate_files</code>.</p> <p>For a more typical bioinformatics project it can look something like this when you include all the rules from processing of the raw data to generating figures for the paper.</p> <p></p> <p>While saying that it's easy to read might be a bit of a stretch, it definitely gives you a better overview of the project than you would have without a WfMS.</p> <p>The second type of graph is based on the jobs, and looks like this for our little workflow (use <code>--dag</code> instead of <code>--rulegraph</code>).</p> <pre><code>snakemake --dag a_b.txt | dot -Tpng &gt; jobgraph.png\n</code></pre> <p></p> <p>The main difference here is that now each node is a job instead of a rule. You can see that the wildcards used in each job are also displayed. Another difference is the dotted lines around the nodes. A dotted line is Snakemake's way of indicating that this rule doesn't need to be rerun in order to generate <code>a_b.txt</code>. Validate this by running <code>snakemake -n -r a_b.txt</code> and it should say that there is nothing to be done.</p> <p>We've discussed before that one of the main purposes of using a WfMS is that it automatically makes sure that everything is up to date. This is done by recursively checking that outputs are always newer than inputs for all the rules involved in the generation of your target files. Now try to change the contents of <code>a.txt</code> to some other text and save it. What do you think will happen if you run <code>snakemake -n -r a_b.txt</code> again?</p> Click to show the solution <pre><code>$ snakemake -n -r a_b.txt\n\nBuilding DAG of jobs...\nJob stats:\njob                      count    min threads    max threads\n---------------------  -------  -------------  -------------\nconcatenate_files            1              1              1\nconvert_to_upper_case        1              1              1\ntotal                        2              1              1\n\n\n[Mon Oct 25 17:00:02 2021]\nrule convert_to_upper_case:\n    input: a.txt\n    output: a.upper.txt\n    jobid: 1\n    reason: Updated input files: a.txt\n    wildcards: some_name=a\n    resources: tmpdir=/var/folders/p0/6z00kpv16qbf_bt52y4zz2kc0000gp/T\n\n\n[Mon Oct 25 17:00:02 2021]\nrule concatenate_files:\n    input: a.upper.txt, b.upper.txt\n    output: a_b.txt\n    jobid: 0\n    reason: Input files updated by another job: a.upper.txt\n    wildcards: first=a, second=b\n    resources: tmpdir=/var/folders/p0/6z00kpv16qbf_bt52y4zz2kc0000gp/T\n\nJob stats:\njob                      count    min threads    max threads\n---------------------  -------  -------------  -------------\nconcatenate_files            1              1              1\nconvert_to_upper_case        1              1              1\ntotal                        2              1              1\n\nThis was a dry-run (flag -n). The order of jobs does not reflect the order of execution.\n</code></pre> <p>Were you correct? Also generate the job graph and compare to the one generated above. What's the difference? Now rerun without <code>-n</code> and validate that <code>a_b.txt</code> contains the new text (don't forget to specify <code>-c 1</code>). Note that Snakemake doesn't look at the contents of files when trying to determine what has changed, only at the timestamp for when they were last modified.</p> <p>We've seen that Snakemake keeps track of if files in the workflow have changed, and automatically makes sure that any results depending on such files are regenerated. What about if the rules themselves are changed? It turns out that since version 7.8.0 Snakemake keeps track of this automatically.</p> <p>Let's say that we want to modify the rule <code>concatenate_files</code> to also include which files were concatenated.</p> <pre><code>rule concatenate_files:\n    output:\n        \"{first}_{second}.txt\"\n    input:\n        \"{first}.upper.txt\",\n        \"{second}.upper.txt\"\n    shell:\n\"\"\"\n        echo 'Concatenating {input}' | cat - {input[0]} {input[1]} &gt; {output}\n        \"\"\"\n</code></pre> <p>Note</p> <p>It's not really important for the tutorial, but the shell command used here first outputs \"Concatenating \" followed by a space delimited list of the files in <code>input</code>. This string is then sent to the program <code>cat</code> where it's concatenated with <code>input[0]</code> and <code>input[1]</code> (the parameter <code>-</code> means that it should read from standard input). Lastly, the output from <code>cat</code> is sent to <code>{output}</code>.</p> <p>If you now run the workflow as before you should see: <pre><code>rule concatenate_files:\n    input: a.upper.txt, b.upper.txt\n    output: a_b.txt\n    jobid: 0\nreason: Code has changed since last execution\n    wildcards: first=a, second=b\n</code></pre></p> <p>because although no files involved in the workflow have been changed, Snakemake recognizes that the workflow code itself has been modified and this triggers a re-run.</p> <p>Snakemake is aware of changes to four categories of such \"rerun-triggers\": \"input\" (changes to rule input files), \"params\" (changes to the rule <code>params</code> section), \"software-env\" (changes to conda environment files specified by the <code>conda:</code> directive) and \"code\" (changes to code in the <code>shell:</code>, <code>run:</code>, <code>script:</code> and <code>notebook:</code> directives).</p> <p>Prior to version 7.8.0, only changes to the modification time of input files would trigger automatic re-runs. To run Snakemake with this previous behaviour you can use the setting <code>--rerun-triggers mtime</code> at the command line. Change the <code>shell:</code> section of the <code>concatenate_files</code> rule back to the previous version, then try running: <code>snakemake -n -r a_b.txt --rerun-triggers mtime</code> and you should again see <code>Nothing to be done (all requested files are present and up to date).</code></p> <p>You can also export information on how all files were generated (when, by which rule, which version of the rule, and by which commands) to a tab-delimited file like this:</p> <pre><code>snakemake a_b.txt -c 1 -D &gt; summary.tsv\n</code></pre> <p>The content of <code>summary.tsv</code> is shown in the table below:</p>  output_file   date   rule   version   log-file(s)   input-file(s)   shellcmd   status   plan   a_b.txt   Mon Oct 25 17:01:46 2021   concatenate_files   -   a.upper.txt,b.upper.txt   cat a.upper.txt b.upper.txt &gt; a_b.txt   rule implementation changed   update pending   a.upper.txt  Mon Oct 25 17:01:46 2021   convert_to_upper_case   -   a.txt   tr [a-z] [A-Z] &lt; a.txt &gt; a.upper.txt   ok   no update    b.upper.txt  Mon Oct 25 17:01:46 2021   convert_to_upper_case   -   b.txt   tr [a-z] [A-Z] &lt; b.txt &gt; b.upper.txt   ok   no update   <p>You can see in the second last column that the rule implementation for <code>a_b.txt</code> has changed. The last column shows if Snakemake plans to regenerate the files when it's next executed. You can see that for the <code>concatenate_files</code> the plan is <code>update pending</code> because we generated the summary with the default behaviour of using all rerun-triggers.</p> <p>You might wonder where Snakemake keeps track of all these things? It stores all information in a hidden subdirectory called <code>.snakemake</code>. This is convenient since it's easy to delete if you don't need it anymore and everything is contained in the project directory. Just be sure to add it to <code>.gitignore</code> so that you don't end up tracking it with git.</p> <p>By now you should be familiar with the basic functionality of Snakemake, and you can build advanced workflows with only the features we have discussed here. There's a lot we haven't covered though, in particular when it comes to making your workflow more reusable. In the following section we will start with a workflow that is fully functional but not very flexible. We will then gradually improve it, and at the same time showcase some Snakemake features we haven't discussed yet. Note that this can get a little complex at times, so if you felt that this section was a struggle then you could move on to one of the other tutorials instead.</p> <p>Quick recap</p> <p>In this section we've learned:</p> <ul> <li>How to use <code>--dag</code> and <code>--rulegraph</code> for visualizing the job and rule   graphs, respectively.</li> <li>How Snakemake reruns relevant parts of the workflow after   there have been changes.</li> <li>How Snakemake tracks changes to files and code in a workflow</li> </ul>"},{"location":"pages/snakemake/snakemake-4-the-mrsa-workflow/","title":"The MRSA Workflow","text":"<p>As you might remember from the intro, we are attempting to understand how lytic bacteriophages can be used as a future therapy for the multiresistant bacteria MRSA (methicillin-resistant Staphylococcus aureus). In order to do this we have performed RNA-seq of three strains, one test and two controls. We have already set up a draft Snakemake workflow for the RNA-seq analysis and it seems to be running nicely. It's now up to you to modify this workflow to make it more flexible and reproducible!</p> <p>Tip</p> <p>This section will leave a little more up to you compared to the previous one. If you get stuck at some point the final workflow after all the modifications is available in <code>~/training-reproducible-research-area/training_reproducible_research/tutorials/git/Snakefile</code>.</p> <p>You are probably already in your <code>snakemake-env</code> environment, otherwise activate it (use <code>conda info --envs</code> if you are unsure).</p> <p>Tip</p> <p>Here we have one Conda environment for executing the whole Snakemake workflow. Snakemake also supports using explicit Conda environments on a per-rule basis, by specifying something like <code>conda: rule-specific-env.yml</code> in the rule definition and running Snakemake with the <code>--use-conda</code> flag. The given rule will then be run in the Conda environment specified in <code>rule-specific-env.yml</code> that will be created and activated on the fly by Snakemake.</p> <p>Let's start by generating the rule graph so that we get an overview of the workflow.</p> <pre><code>snakemake -s snakefile_mrsa.smk --rulegraph | dot -T png &gt; rulegraph_mrsa.png\n</code></pre> <p>There are two differences in this command compared to the one we've used before. The first is that we're using the <code>-s</code> flag to specify which Snakemake workflow to run. We didn't need to do that before since <code>Snakefile</code> is the default name. The second is that we don't define a target. In the toy example we used <code>a_b.txt</code> as a target, and the wildcards were resolved based on that. How come that we don't need to do that here? It turns out that by default Snakemake targets the first rule in a workflow. By convention, we call this rule <code>all</code> and let it serve as a rule for aggregating the main outputs of the workflow.</p> <p></p> <p>Now take some time and look through the workflow file and try to understand how the rules fit together. Use the rule graph as aid. The rules represent a quite standard, although somewhat simplified, workflow for RNA-seq analysis. If you are unfamiliar with the purpose of the different operations (index genome, FastQC and so on), then take a look at the intro.</p> <p>Also generate the job graph in the same manner. Here you can see that three samples will be downloaded from SRA (Sequence Read Archive); SRR935090, SRR935091, and SRR935092. Those will then be quality controlled with FastQC and aligned to a genome. The QC output will be aggregated with MultiQC and the alignments will be used to generate a count table, i.e. a table that shows how many reads map to each gene for each sample. This count table is then what the downstream analysis will be based on.</p> <p></p> <p>Now try to run the whole workflow. Hopefully you see something like this.</p> <pre><code>Building DAG of jobs...\nUsing shell: /bin/bash\nProvided cores: 1 (use --cores to define parallelism)\nRules claiming more threads will be scaled down.\nJob stats:\njob                     count    min threads    max threads\n--------------------  -------  -------------  -------------\nalign_to_genome             3              1              1\nall                         1              1              1\nfastqc                      3              1              1\ngenerate_count_table        1              1              1\ngenerate_rulegraph          1              1              1\nget_SRA_by_accession        3              1              1\nget_genome_fasta            1              1              1\nget_genome_gff3             1              1              1\nindex_genome                1              1              1\nmultiqc                     1              1              1\nsort_bam                    3              1              1\ntotal                      19              1              1\n\nSelect jobs to execute...\n\n[Mon Oct 25 17:13:47 2021]\nrule get_genome_fasta:\n    output: data/raw_external/NCTC8325.fa.gz\n    jobid: 6\n    resources: tmpdir=/var/folders/p0/6z00kpv16qbf_bt52y4zz2kc0000gp/T\n\n--2021-10-25 17:13:48--  ftp://ftp.ensemblgenomes.org/pub/bacteria/release-37/fasta/bacteria_18_collection/staphylococcus_aureus_subsp_aureus_nctc_8325/dna//Staphylococcus_aureus_subsp_aureus_nctc_8325.ASM1342v1.dna_rm.toplevel.fa.gz\n           =&gt; \u2018data/raw_external/NCTC8325.fa.gz\u2019\nResolving ftp.ensemblgenomes.org (ftp.ensemblgenomes.org)... 193.62.197.75\nConnecting to ftp.ensemblgenomes.org (ftp.ensemblgenomes.org)|193.62.197.75|:21... connected.\nLogging in as anonymous ... Logged in!\n==&gt; SYST ... done.    ==&gt; PWD ... done.\n.\n.\n[lots of stuff]\n.\n.\nlocalrule all:\n    input: results/tables/counts.tsv, results/multiqc.html, results/rulegraph.png\n    jobid: 0\n    resources: tmpdir=/var/folders/p0/6z00kpv16qbf_bt52y4zz2kc0000gp/T\n\n[Mon Oct 25 17:14:38 2021]\nFinished job 0.\n19 of 19 steps (100%) done\n</code></pre> <p>After everything is done, the workflow will have resulted in a bunch of files in the directories <code>data</code>, <code>intermediate</code> and <code>results</code>. Take some time to look through the structure, in particular the quality control reports in <code>results</code> and the count table in <code>results/tables</code>.</p> <p>Quick recap</p> <p>In this section we've learned:</p> <ul> <li>How the MRSA workflow looks.</li> <li>How to run the MRSA workflow.</li> <li>Which output files the MRSA workflow produces.</li> </ul>"},{"location":"pages/snakemake/snakemake-5-parameters/","title":"Parameters","text":"<p>In a typical bioinformatics project, considerable efforts are spent on tweaking parameters for the various programs involved. It would be inconvenient if you had to change in the shell scripts themselves every time you wanted to run with a new setting. Luckily, there is a better option for this: the <code>params</code> keyword.</p> <pre><code>rule some_rule:\n    output:\n        \"...\"\n    input:\n        \"...\"\n    params:\n        cutoff=2.5\n    shell:\n\"\"\"\n        some_program --cutoff {params.cutoff} {input} {output}\n        \"\"\"\n</code></pre> <p>We run most of the programs with default settings in our workflow. However, there is one parameter in the rule <code>get_SRA_by_accession</code> that we use for determining how many reads we want to retrieve from SRA for each sample (<code>-X 25000</code>). Change in this rule to use the parameter <code>max_reads</code> instead and set the value to 20000. If you need help, click to show the solution below.</p> Click to show the solution <pre><code>rule get_SRA_by_accession:\n\"\"\"\n    Retrieve a single-read FASTQ file from SRA (Sequence Read Archive) by run accession number.\n    \"\"\"\n    output:\n        \"data/raw_internal/{sample_id}.fastq.gz\"\n    params:\n        max_reads = 20000\n    shell:\n\"\"\"\n        fastq-dump {wildcards.sample_id} -X {params.max_reads} --readids \\\n            --dumpbase --skip-technical --gzip -Z &gt; {output}\n        \"\"\"\n</code></pre> <p>Now run through the workflow. Because there's been changes to the <code>get_SRA_by_accession</code> rule this will trigger a re-run of the rule for all three accessions. In addition all downstream rules that depend on output from <code>get_SRA_by_accession</code> are re-run.</p> <p>The parameter values we set in the <code>params</code> section don't have to be static, they can be any Python expression. In particular, Snakemake provides a global dictionary of configuration parameters called <code>config</code>. Let's modify <code>get_SRA_by_accession</code> to look something like this in order to make use of this dictionary:</p> <pre><code>rule get_SRA_by_accession:\n\"\"\"\n    Retrieve a single-read FASTQ file from SRA (Sequence Read Archive) by run accession number.\n    \"\"\"\n    output:\n        \"data/raw_internal/{sample_id}.fastq.gz\"\n    params:\n        max_reads = config[\"max_reads\"]\n    shell:\n\"\"\"\n        fastq-dump {wildcards.sample_id} -X {params.max_reads} --readids \\\n            --dumpbase --skip-technical --gzip -Z &gt; {output}\n        \"\"\"\n</code></pre> <p>Note that Snakemake now expects there to be a key named <code>max_reads</code> in the config dictionary. If we don't populate the dictionary somehow the dictionary will be empty so if you were to run the workflow now it would trigger a <code>KeyError</code> (try running <code>snakemake -s snakefile_mrsa.smk -n</code> to see for yourself). In order to populate the config dictionary with data for the workflow we could use the <code>snakemake --config KEY=VALUE</code> syntax directly from the command line. However, from a reproducibility perspective, it's not optimal to set parameters from the command line, since it's difficult to keep track of which parameter values that were used.</p> <p>A much better alternative is to use the <code>--configfile FILE</code> option to supply a configuration file to Snakemake. In this file we can collect all the project-specific settings, sample ids and so on. This also enables us to write the Snakefile in a more general manner so that it can be better reused between projects. Like several other files used in these tutorials, this file should be in yaml format. Create the file below and save it as <code>config.yml</code>.</p> <pre><code>max_reads: 25000\n</code></pre> <p>If we now run Snakemake with <code>--configfile config.yml</code>, it will parse this file to form the <code>config</code> dictionary. If you want to overwrite a parameter value, e.g. for testing, you can still use the <code>--config KEY=VALUE</code> flag, as in <code>--config max_reads=1000</code>.</p> <p>Tip</p> <p>Rather than supplying the config file from the command line you could also add the line <code>configfile: \"config.yml\"</code> to the top of your Snakefile. Keep in mind that with such a setup Snakemake will complain if the file <code>config.yml</code> is not present.</p> <p>Quick recap</p> <p>In this section we've learned:</p> <ul> <li>How to set parameter values with the <code>params</code> directive.</li> <li>How to run Snakemake with the <code>config</code> variable and with a configuration file.</li> </ul>"},{"location":"pages/snakemake/snakemake-6-logs/","title":"Logs","text":"<p>As you probably noticed it was difficult to follow how the workflow progressed since some rules printed a lot of output to the terminal. In some cases this also contained important information, such as statistics on the sequence alignments or genome indexing. This could be valuable for example if you later in the project get weird results and want to debug. It's also important from a reproducibility perspective that the \"paper trail\" describing how the outputs were generated is saved. Luckily, Snakemake has a feature that can help with this. Just as we define <code>input</code> and <code>output</code> in a rule we can also define <code>log</code>.</p> <pre><code>rule some_rule:\n    output:\n        \"...\"\n    input:\n        \"...\"\n    log:\n        \"...\"\n    shell:\n\"\"\"\n        echo 'Converting {input} to {output}' &gt; {log}\n        \"\"\"\n</code></pre> <p>A log file is not different from any other output file, but it's dealt with a little differently by Snakemake. For example, it's shown in the file summary when using <code>-D</code>. It's also a good way to clarify the purpose of the file. We probably don't need to save logs for all the rules, only the ones with interesting output.</p> <ul> <li><code>get_genome_fasta</code> and <code>get_genome_gff3</code> would be good to log since they are   dependent on downloading files from an external server.</li> <li><code>multiqc</code> aggregates quality control data for all the samples into one html   report, and the log contains information about which samples were aggregated.</li> <li><code>index_genome</code> outputs some statistics about the genome indexing.</li> <li><code>align_to_genome</code> outputs important statistics about the alignments. This is   probably the most important log to save.</li> </ul> <p>Now add a log file to some or all of the rules above. A good place to save them to would be <code>results/logs/rule_name/</code>. In order to avoid that multiple jobs write to the same files Snakemake requires that all output and log files contain the same wildcards, so be sure to include any wildcards used in the rule in the log name as well, e.g. <code>{some_wildcard}.log</code>.</p> <p>You also have to specify in the <code>shell</code> section of each rule what you want the log to contain. Some of the programs we use send their log information to standard out, some to standard error and some let us specify a log file via a flag.</p> <p>For example, in the <code>align_to_genome</code> rule, it could look like this (bowtie2 writes log info to standard error):</p> <pre><code>rule align_to_genome:\n\"\"\"\n    Align a fastq file to a genome index using Bowtie 2.\n    \"\"\"\n    output:\n        \"intermediate/{sample_id,\\w+}.bam\"\n    input:\n        \"data/raw_internal/{sample_id}.fastq.gz\",\n        \"intermediate/NCTC8325.1.bt2\",\n        \"intermediate/NCTC8325.2.bt2\",\n        \"intermediate/NCTC8325.3.bt2\",\n        \"intermediate/NCTC8325.4.bt2\",\n        \"intermediate/NCTC8325.rev.1.bt2\",\n        \"intermediate/NCTC8325.rev.2.bt2\"\n    log:\n        \"results/logs/align_to_genome/{sample_id}.log\"\n    shell:\n\"\"\"\n        bowtie2 -x intermediate/NCTC8325 -U {input[0]} &gt; {output} 2&gt;{log}\n        \"\"\"\n</code></pre> <p>To save some time you can use the info below.</p> <pre><code># Wget has a -o flag for specifying the log file\nwget remote_file -O output_file -o {log}\n\n# MultiQC writes to standard error so we redirect with \"2&gt;\"\nmultiqc -n output_file input_files 2&gt; {log}\n\n# Bowtie2-build redirects to standard out so we use \"&gt;\"\nbowtie2-build input_file index_dir &gt; {log}\n</code></pre> <p>Now rerun the whole workflow. Do the logs contain what they should? Note how much easier it is to follow the progression of the workflow when the rules write to logs instead of to the terminal.</p> <p>Tip</p> <p>If you have a rule with a shell directive in which several commands are run and you want to save stdout and stderr for all commands into the same log file you can add <code>exec &amp;&gt;{log}</code> as the first line of the shell directive.</p> <p>If you run with <code>-D</code> (or <code>-S</code> for a simpler version) you will see that the summary table now also contains the log file for each of the files in the workflow.</p> <p>Quick recap</p> <p>In this section we've learned:</p> <ul> <li>How to redirect output to log files with the <code>log</code> directive.</li> </ul>"},{"location":"pages/snakemake/snakemake-7-temporary-files/","title":"Temporary files","text":"<p>It's not uncommon that workflows contain temporary files that should be kept for some time and then deleted once they are no longer needed. A typical case could be that some operation generates a file, which is then compressed to save space or indexed to make searching faster. There is then no need to save the original output file. Take a look at the job graph for our workflow again. The output from <code>align_to_genome</code> is a bam file, which contains information about all the reads for a sample and where they map in the genome. For downstream processing we need this file to be sorted by genome coordinates. This is what the rule <code>sort_bam</code> is for. We therefore end up with both <code>intermediate/{sample_id}.bam</code> and <code>intermediate/{sample_id}.sorted.bam</code>.</p> <p>In Snakemake we can mark an output file as temporary like this:</p> <pre><code>output: temp(\"...\")\n</code></pre> <p>The file will then be deleted as soon as all jobs where it's an input have finished. Now do this for the output of <code>align_to_genome</code>. We have to rerun the rule for it to trigger, so use <code>-R align_to_genome</code>. It should look something like this:</p> <pre><code>.\n.\nrule sort_bam:\n    input: intermediate/SRR935090.bam\n    output: intermediate/SRR935090.sorted.bam\n    jobid: 2\n    wildcards: sample_id=SRR935090\n\nRemoving temporary output file intermediate/SRR935090.bam.\nFinished job 2.\n.\n.\n</code></pre> <p>Tip</p> <p>Sometimes you may want to trigger removal of temporary files without actually rerunning the jobs. You can then use the <code>--delete-temp-output</code> flag. In some cases you may instead want to run only parts of a workflow and therefore want to prevent files marked as temporary from being deleted (because the files are needed for other parts of the workflow). In such cases you can use the <code>--notemp</code> flag.</p> <p>Snakemake has a number of options for marking files:</p> <ul> <li><code>temp(\"...\")</code>: The output file should be deleted once it's no longer needed   by any rules.</li> <li><code>protected(\"...\")</code>: The output file should be write-protected. Typically used   to protect files that require a huge amount of computational resources from   being accidentally deleted.</li> <li><code>ancient(\"...\")</code>: The timestamp of the input file is ignored and it's always   assumed to be older than any of the output files.</li> <li><code>touch(\"...\")</code>: The output file should be \"touched\", i.e. created or   updated, when the rule has finished. Typically used as \"flag files\" to   enforce some rule execution order without real file dependencies.</li> <li><code>directory(\"...\")</code>: The output is a directory rather than a file.</li> </ul> <p>Success \"Quick recap\"     In this section we've learned:</p> <pre><code>- How to mark an output file as temporary for automatic removal.\n</code></pre>"},{"location":"pages/snakemake/snakemake-8-targets/","title":"Targets","text":"<p>So far we have only defined the inputs/outputs of a rule as strings, or in some case a list of strings, but Snakemake allows us to be much more flexible than that. Actually, we can use any Python expression or even functions, as long as they return a string or list of strings. Consider the rule <code>align_to_genome</code> below.</p> <pre><code>rule align_to_genome:\n\"\"\"\n    Align a fastq file to a genome index using Bowtie 2.\n    \"\"\"\n    output:\n        \"intermediate/{sample_id,\\w+}.bam\"\n    input:\n        \"data/raw_internal/{sample_id}.fastq.gz\",\n        \"intermediate/NCTC8325.1.bt2\",\n        \"intermediate/NCTC8325.2.bt2\",\n        \"intermediate/NCTC8325.3.bt2\",\n        \"intermediate/NCTC8325.4.bt2\",\n        \"intermediate/NCTC8325.rev.1.bt2\",\n        \"intermediate/NCTC8325.rev.2.bt2\"\n    shell:\n\"\"\"\n        bowtie2 -x intermediate/NCTC8325 -U {input[0]} &gt; {output}\n        \"\"\"\n</code></pre> <p>Here we have seven inputs; the fastq file with the reads and six files with similar file names from the Bowtie 2 genome indexing. We can try to tidy this up by using a Python expression to generate a list of these files instead. If you're familiar with Python you could do this with list comprehensions like this:</p> <pre><code>input:\n    fastq = \"data/raw_internal/{sample_id}.fastq.gz\",\n    index = [f\"intermediate/NCTC8325.{substr}.bt2\" for\n        substr in [\"1\", \"2\", \"3\", \"4\", \"rev.1\", \"rev.2\"]]\n</code></pre> <p>This will take the elements of the list of substrings one by one, and insert that element in the place of <code>{substr}</code>. Since this type of aggregating rules are quite common, Snakemake also has a more compact way of achieving the same thing.</p> <pre><code>input:\n    fastq = \"data/raw_internal/{sample_id}.fastq.gz\",\n    index = expand(\"intermediate/NCTC8325.{substr}.bt2\",\n           substr = [\"1\", \"2\", \"3\", \"4\", \"rev.1\", \"rev.2\"])\n</code></pre> <p>Now change in the rules <code>index_genome</code> and <code>align_to_genome</code> to use the <code>expand()</code> expression.</p> <p>In the workflow we decide which samples to run by including the SRR ids in the names of the inputs to the rules <code>multiqc</code> and <code>generate_count_table</code>. This is a potential source of errors since it's easy to change in one place and forget to change in the other. As we've mentioned before, but not really used so far, Snakemake allows us to use Python code \"everywhere\". Let's therefore define a list of sample ids and put at the very top of the Snakefile, just before the rule <code>all</code>.</p> <pre><code>SAMPLES = [\"SRR935090\", \"SRR935091\", \"SRR935092\"]\n</code></pre> <p>Now use <code>expand()</code> in <code>multiqc</code> and <code>generate_count_table</code> to use <code>SAMPLES</code> for the sample ids. For the <code>multiqc</code> rule it could look like this:</p> <pre><code>input:\n    expand(\"intermediate/{sample_id}_fastqc.zip\", sample_id = SAMPLES)\n</code></pre> <p>See if you can update the <code>generate_count_table</code> rule in the same manner!</p> <p>Quick recap</p> <p>In this section we've learned:</p> <ul> <li>How to use the <code>expand()</code> expression to create a list with file names, inserting all provided wildcard values.</li> </ul>"},{"location":"pages/snakemake/snakemake-9-shadow-rules/","title":"Shadow rules","text":"<p>Take a look at the <code>index_genome</code> rule below:</p> <pre><code>rule index_genome:\n\"\"\"\n    Index a genome using Bowtie 2.\n    \"\"\"\n    output:\n        index = expand(\"intermediate/NCTC8325.{substr}.bt2\",\n           substr = [\"1\", \"2\", \"3\", \"4\", \"rev.1\", \"rev.2\"])\n    input:\n        \"data/raw_external/NCTC8325.fa.gz\"\n    log:\n        \"results/logs/index_genome/NCTC8325.log\"\n    shell:\n\"\"\"\n        # Bowtie2 cannot use .gz, so unzip to a temporary file first\n        gunzip -c {input} &gt; tempfile\n        bowtie2-build tempfile intermediate/NCTC8325 &gt;{log}\n\n        # Remove the temporary file\n        rm tempfile\n        \"\"\"\n</code></pre> <p>There is a temporary file here called <code>tempfile</code> which is the uncompressed version of the input, since Bowtie 2 cannot use compressed files. There are a number of drawbacks with having files that aren't explicitly part of the workflow as input/output files to rules:</p> <ul> <li>Snakemake cannot clean up these files if the job fails, as it would do for   normal output files.</li> <li>If several jobs are run in parallel there is a risk that they write to   <code>tempfile</code> at the same time. This can lead to very scary results.</li> <li>Sometimes we don't know the names of all the files that a program can   generate. It is, for example, not unusual that programs leave some kind of   error log behind if something goes wrong.</li> </ul> <p>All of these issues can be dealt with by using the <code>shadow</code> option for a rule. The shadow option results in that each execution of the rule is run in an isolated temporary directory (located in <code>.snakemake/shadow/</code> by default). There are a few options for <code>shadow</code> (for the full list of these options see the Snakemake docs). The most simple is <code>shadow: \"minimal\"</code>, which means that the rule is executed in an empty directory that the input files to the rule have been symlinked into. For the rule below, that means that the only file available would be <code>input.txt</code>. The shell commands would generate the files <code>some_other_junk_file</code> and <code>output.txt</code>. Lastly, Snakemake will move the output file (<code>output.txt</code>) to its \"real\" location and remove the whole shadow directory. We therefore never have to think about manually removing <code>some_other_junk_file</code>.</p> <pre><code>rule some_rule:\n    output:\n        \"output.txt\"\n    input:\n        \"input.txt\"\n    shadow: \"minimal\"\n    shell:\n\"\"\"\n        touch some_other_junk_file\n        cp {input} {output}\n        \"\"\"\n</code></pre> <p>Try this out for the rules where we have to \"manually\" deal with files that aren't tracked by Snakemake (<code>multiqc</code>, <code>index_genome</code>). Also remove the shell commands that remove temporary files from those rules, as they are no longer needed. Now rerun the workflow and validate that the temporary files don't show up in your working directory.</p> <p>Tip</p> <p>Some people use the shadow option for almost every rule and some never use it at all. One thing to keep in mind is that it leads to some extra file operations when the outputs are moved to their final location. This is no issue when the shadow directory is on the same disk as the output directory, but if you're running on a distributed file system and generate very many or very large files it might be worth considering other options (see e.g.   the <code>--shadow-prefix</code> flag).</p> <p>Quick recap</p> <p>In this section we've learned:</p> <ul> <li>How to use the shadow option to handle files that are not tracked by Snakemake.</li> </ul>"},{"location":"pages/snakemake/snakemake-installation/","title":"Setup Snakemake tutorial","text":""},{"location":"pages/snakemake/snakemake-installation/#setup-course-material","title":"Setup course material","text":"Follow this instructions only if you start the course at this stage! Otherwise skip this step! <pre><code>This tutorial depends on files from the course GitHub repo. Please follow these instructions \non how to set it up if you haven't done so already.  \nLet's create a directory and clone the course GitHub repo.\n\n```bash\nmkdir -p  ~/training-reproducible-research-area\ncd ~/training-reproducible-research-area\ngit clone https://github.com/SouthGreenPlatform/training_reproducible_research\n```\n</code></pre>"},{"location":"pages/snakemake/snakemake-installation/#setup-environment","title":"Setup environment","text":"<p>First let's create a dedicated folder for this tutorial:</p> <pre><code>mkdir -p  ~/training-reproducible-research-area/snakemake\ncd ~/training-reproducible-research-area/snakemake\ncp -r ~/training-reproducible-research-area/training_reproducible_research/tutorials/snakemake/* . </code></pre> <p>We will use Conda environments for the set up of this tutorial. So, now we will install all the tools via conda:</p> <pre><code>conda env create -f environment.yml -n snakemake-env\nconda activate snakemake-env\n</code></pre>"},{"location":"pages/take_down/take_down/","title":"Take down","text":"<p>Depending on which of the tutorials you have taken, there might be quite a lot of files stored on your computer. Here are instructions for how to remove them.</p> <p>All the tutorials depend on you cloning the <code>training-reproducible-research</code> GitHub repo. This can be removed like any other directory; via Finder, Explorer or <code>rm -rf training-reproducible-research</code>. Note that this will also delete the hidden directories <code>.git</code>, which contains the history of the repo, and <code>.snakemake</code>, which contains the history of any Snakemake runs.</p>"},{"location":"pages/take_down/take_down/#conda","title":"Conda","text":"<p>Several of the tutorials use Conda for installing packages. This amounts to about 2.6 GB if you've done all the tutorials. If you plan on using Conda in the future you can remove just the packages, or you can remove everything including Conda itself. Note that this is not needed if you've done the tutorials on Windows using Docker (see the section on Docker below instead).</p> <p>In order to remove all your Conda environments, you first need to list them:</p> <pre><code>conda env list\n</code></pre> <p>For each of the environments except \"base\" run the following:</p> <pre><code>conda remove -n envname --all\n</code></pre> <p>And, finally:</p> <pre><code>conda clean --all\n</code></pre> <p>If you also want to remove Conda itself (i.e. removing all traces of Conda), you need to check where Conda is installed. Look for the row \"base environment\".</p> <pre><code>conda info\n</code></pre> <p>This should say something like <code>/Users/&lt;user&gt;/miniconda3</code>. Then remove the entire Conda directory:</p> <pre><code>rm -rf /Users/&lt;user&gt;/miniconda3\n</code></pre> <p>Lastly, open your <code>~/.bashrc</code> file (or <code>~/.bash_profile</code> if on Mac) in a text editor and remove the path to Conda from PATH.</p>"},{"location":"pages/take_down/take_down/#snakemake","title":"Snakemake","text":"<p>Snakemake is installed via Conda and will be removed if you follow the instructions in the Conda section above. Note that Snakemake also generates a hidden <code>.snakemake</code> directory in the directory where it's run. You can remove this with the following:</p> <pre><code>rm -rf workshop-reproducible-research/snakemake/.snakemake\n</code></pre>"},{"location":"pages/take_down/take_down/#jupyter","title":"Jupyter","text":"<p>Jupyter is installed via Conda and will be removed if you follow the instructions in the Conda section above.</p>"},{"location":"pages/take_down/take_down/#docker","title":"Docker","text":"<p>If you've done the Docker tutorial or if you've been running Docker for Windows you have some cleaning up to do. Docker is infamous for quickly taking up huge amounts of space, and some maintenance is necessary every now and then. Here is how to uninstall Docker completely. For instructions for how to remove individual images or containers, see the Docker tutorial.</p>"},{"location":"pages/take_down/take_down/#macos","title":"macOS","text":"<p>Click the Docker icon in the menu bar (upper right part of the screen) and select \"Preferences\". In the upper right corner, you should find a little bug icon. Click on that icon and select \"Reset to factory defaults\". You may have to fill  in your password. Then select \"Uninstall\". Once it's done uninstalling, drag the  Docker app from Applications to Trash.</p>"},{"location":"pages/take_down/take_down/#linux","title":"Linux","text":"<p>If you've installed Docker with <code>apt-get</code>, uninstall it like this:</p> <pre><code>apt-get purge docker-ce\n</code></pre> <p>Images, containers, and volumes are not automatically removed. To delete all of them:</p> <pre><code>rm -rf /var/lib/docker\n</code></pre>"},{"location":"pages/take_down/take_down/#windows","title":"Windows","text":"<p>Uninstall Docker for Windows (on Windows 10) or Docker Toolbox (on Windows 7) via Control Panel &gt; Programs &gt; Programs and Features. Docker Toolbox will also have installed Oracle VM VirtualBox, so uninstall that as well if you're not using it for other purposes.</p>"},{"location":"pages/take_down/take_down/#singularity","title":"Singularity","text":"<p>Singularity images are files that can simply be deleted. Singularity also creates a hidden directory <code>.singularity</code> in your home directory that contains its cache, which you may delete.</p>"},{"location":"pages/take_down/take_down/#windows_1","title":"Windows","text":"<p>On Windows, you will additionally need to uninstall Git for Windows, VirtualBox, Vagrant and Vagrant Manager (see the  Singularity installation guide).</p>"}]}